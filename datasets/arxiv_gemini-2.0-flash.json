[
  {
    "id": 0,
    "prompt": "Properties of high-degree oscillation modes of the Sun observed with Hinode/SOT",
    "HWT": "Aims. With the Solar Optical Telescope on Hinode, we investigate the basic properties of high-degree solar oscillations observed at two levels in the solar atmosphere, in the G-band (formed in the photosphere) and in the Ca II H line (chromospheric emission). Methods. We analyzed the data by calculating the individual power spectra as well as the cross-spectral properties, i.e., coherence and phase shift. The observational properties are compared with a simple theoretical model, which includes the effects of correlated noise. Results. The results reveal significant frequency shifts between the Ca II H and G-band spectra, in particular above the acoustic cut-off frequency for pseudo-modes. The cross-spectrum phase shows peaks associated with the acoustic oscillation (p-mode) lines, and begins to increase with frequency around the acoustic cut-off. However, we find no phase shift for the (surface gravity wave) f-mode. The observed properties for the p-modes are qualitatively reproduced in a simple model with a correlated background if the correlated noise level in the Ca II H data is higher than in the G-band data. These results suggest that multi-wavelength observations of solar oscillations, in combination with the traditional intensity-velocity observations, may help to determine the level of the correlated background noise and to determine the type of wave excitation sources on the Sun.",
    "MGT": "High-degree solar oscillations, with their short wavelengths, offer a valuable probe of the Sun's near-surface layers. This study investigates the properties of these modes using high-resolution observations from the Solar Optical Telescope (SOT) onboard the Hinode satellite. We analyze Dopplergrams obtained from the Fe I 630.2 nm spectral line, focusing on a region near the solar disk center to minimize projection effects. A time-distance helioseismic analysis is performed to extract travel times of acoustic waves propagating between different points on the solar surface. These travel times are then used to infer the properties of the high-degree modes, including their frequencies, amplitudes, and damping rates.\n\nOur results reveal the presence of well-defined ridges in the k-ω diagram, indicating the existence of coherent high-degree oscillations. We observe a systematic shift in the mode frequencies compared to standard solar models, suggesting the need for refinements in the near-surface structure. Specifically, we find evidence for a higher sound speed in the uppermost layers of the convection zone than predicted by current models. Furthermore, we analyze the mode amplitudes and damping rates as a function of frequency and horizontal wavenumber. We find that the amplitudes decrease with increasing frequency, consistent with theoretical expectations. The damping rates, on the other hand, increase with both frequency and wavenumber, suggesting that the modes are more efficiently damped at smaller spatial scales and higher frequencies. These findings provide valuable constraints on the physical processes that govern the excitation and damping of solar oscillations in the near-surface layers, including turbulent convection and magnetic fields. The improved understanding of these processes is crucial for refining our models of the Sun's interior and its influence on the heliosphere.\n"
  },
  {
    "id": 1,
    "prompt": "Bifurcation and Secondary Bifurcation of Heavy Periodic Hydroelastic Travelling Waves",
    "HWT": "The paper deals with a problem of interaction between hydrodynamics and mechanics of nonlinear elastic bodies. The existence question for two-dimensional symmetric steady waves travelling on the surface of a deep ocean beneath a heavy elastic membrane is analyzed as a problem in bifurcation theory. The behaviour of the two-dimensional cross-section of the membrane is modelled as a thin (unshearable), heavy, hyperelastic Cosserat rod, following Antman's elasticity theory, and the fluid beneath is supposed to be in steady 2D irrotational motion under gravity. Assuming that gravity and the density of the undeformed membrane are prescribed, the free parameters of the problem are the speed of the wave and drift velocity of the membrane. The analysis relies upon a conformal formulation of the hydro-elastic problem developed in previous papers; the basic tool for the study of the bifurcation picture is the implicit function theorem, under some non-resonance assumptions. The most interesting part of the final result is the existence of a symmetry-breaking 'third sheet' of solutions, which bifurcates from primary sheets, and is a hydro-elastic analogue of the phenomenon known as 'Wilton ripples' in the surface tension case.",
    "MGT": "This study investigates the bifurcation and secondary bifurcation phenomena exhibited by heavy periodic hydroelastic traveling waves, focusing on the interplay between fluid dynamics and structural mechanics. A fully nonlinear numerical model, based on the finite element method and arbitrary Lagrangian-Eulerian formulation, is developed to simulate the fluid-structure interaction. The model accurately captures the complex free surface dynamics, structural deformation, and coupling effects inherent in these wave systems. Bifurcation diagrams are constructed by systematically varying key parameters, such as wave amplitude and structural stiffness, revealing the emergence of distinct wave patterns. Primary bifurcations from a trivial flat state lead to the formation of fundamental periodic waves. Beyond a critical threshold, secondary bifurcations are observed, giving rise to more complex wave forms characterized by period doubling, symmetry breaking, and the formation of higher-order harmonics. These secondary bifurcations significantly alter the wave’s energy dissipation characteristics and structural loading. The study provides detailed insights into the dynamics of these complex fluid-structure systems, demonstrating the importance of considering nonlinear effects and bifurcation phenomena in the design and analysis of offshore structures and floating platforms. Furthermore, the results highlight the potential for instability and the emergence of extreme wave events due to small perturbations in system parameters. The numerical framework developed offers a powerful tool for predicting and understanding the behavior of hydroelastic waves in various engineering applications.\n"
  },
  {
    "id": 2,
    "prompt": "Structure of the coadjoint orbits of Lie groups",
    "HWT": "We study the geometrical structure of the coadjoint orbits of an arbitrary complex or real Lie algebra ${\\mathfrak g}$ containing some ideal ${\\mathfrak n}$. It is shown that any coadjoint orbit in ${\\mathfrak g}^*$ is a bundle with the affine subspace of ${\\mathfrak g}^*$ as its fibre. This fibre is an isotropic submanifold of the orbit and is defined only by the coadjoint representations of the Lie algebras ${\\mathfrak g}$ and ${\\mathfrak n}$ on the dual space ${\\mathfrak n}^*$. The use of this fact and an application of methods of symplectic geometry give a new insight into the structure of coadjoint orbits and allow us to generalize results derived earlier in the case when ${\\mathfrak g}$ is a split extension using the Abelian ideal ${\\mathfrak n}$ (a semidirect product). As applications, a new proof of the formula for the index of Lie algebra and a necessary condition of integrality of a coadjoint orbit are obtained.",
    "MGT": "The structure of coadjoint orbits of Lie groups plays a fundamental role in representation theory, geometric quantization, and the study of integrable systems. This article provides a comprehensive overview of the geometric and algebraic properties of these orbits, focusing on their symplectic nature and their connection to the representation theory of Lie groups. We explore the Kirillov-Kostant-Souriau symplectic form, demonstrating its invariance under the coadjoint action and its significance in defining a Poisson structure on the dual of the Lie algebra. The article also investigates the relationship between coadjoint orbits and irreducible representations, highlighting the Kirillov orbit method, which proposes a correspondence between these objects. We examine the structure of coadjoint orbits for specific classes of Lie groups, including compact Lie groups, nilpotent Lie groups, and semisimple Lie groups, detailing the unique features and challenges associated with each case. Furthermore, we discuss the stratification of the dual of the Lie algebra into coadjoint orbits and the implications for understanding the representation theory of the group. Finally, we explore the role of coadjoint orbits in the geometric quantization program, illustrating how they provide a geometric framework for constructing quantum mechanical systems from classical ones and their role in completely integrable systems.\n"
  },
  {
    "id": 3,
    "prompt": "A manifold of possible physics-laws in a universe where the planck constant and speed of light parameters vary",
    "HWT": "I assume a universe whereby the speed of light and the planck constant are not constants but instead parameters that vary locally in time-and space. When describing motion, I am able to derive a modified path integral description at the quantum level, which offers a natural extension of quantum mechanics. At the microscopic level, this path integral intuitively describes a physics with many quantum realities thus leading to a novel concept of manifold of physics, which can be looked at as a novel action principle. This paradigm reflects the notion that the observed laws of physics on any given scale are determined by the underlying distribution of the fundamental parameters (i. e Quantum Mechanics is just one point on this manifold), thus leading to many possible physical-law based behaviors. By choosing a Gaussian distribution of the parameters, a quadratic action term appears in the path-integral, which in turns leads to a complex classical action (and by continuation a new description for inertia) at the classical level. In the accompanying manuscript the classical doublet equation of motion is applied to the Newtonian gravitation field, and a MOND-like, dark-energy-like, and pioneer-anomaly-like solutions are derived.",
    "MGT": "This study explores the theoretical implications of varying the Planck constant (ħ) and the speed of light (c) on the fundamental laws of physics within a hypothetical universe. While these constants are considered immutable within our current understanding of physics, this work investigates the ramifications of their variability on established physical laws and constants. We consider a manifold of possible values for ħ and c, examining how changes in these parameters influence phenomena such as quantum mechanics, electromagnetism, and gravity.\n\nOur analysis reveals that varying ħ significantly alters the scale at which quantum effects become prominent. An increase in ħ could potentially lead to macroscopic quantum phenomena, while a decrease might diminish quantum effects to near insignificance. Altering c, on the other hand, directly affects the relativistic framework, impacting the relationship between space and time, and influencing the energy-mass equivalence.\n\nFurthermore, we explore the interplay between these varying constants and other fundamental parameters, such as the gravitational constant (G) and the fine-structure constant (α). We investigate how changes in ħ and c might necessitate adjustments in these other constants to maintain a self-consistent physical framework. This investigation suggests the existence of constraints and interdependencies between fundamental constants, hinting at a deeper underlying structure governing the laws of physics. The implications of a universe governed by drastically different values of ħ and c are discussed, touching upon the potential for altered cosmological evolution, different forms of matter, and the possibility of life emerging under such conditions. This theoretical exploration provides valuable insights into the sensitivity of our universe to its fundamental constants and offers a broader perspective on the nature of physical laws.\n"
  },
  {
    "id": 4,
    "prompt": "Stochasticity in N-body Simulations of Disc Galaxies",
    "HWT": "We demonstrate that the chaotic nature of N-body systems can lead to macroscopic variations in the evolution of collisionless simulations containing rotationally supported discs. The unavoidable stochasticity that afflicts all simulations generally causes mild differences between the evolution of similar models but, in order to illustrate that this is not always true, we present a case that shows extreme bimodal divergence. The divergent behaviour occurs in two different types of code and is independent of all numerical parameters. We identify and give explicit illustrations of several sources of stochasticity, and also show that macroscopic variations in the evolution can originate from differences at the round-off error level. We obtain somewhat more consistent results from simulations in which the halo is set up with great care compared with those started from more approximate equilibria, but we have been unable to eliminate diverging behaviour entirely because the main sources of stochasticity are intrinsic to the disc. We show that the divergence is only temporary and that halo friction is merely delayed, for a substantial time in some cases. We argue that the delays are unlikely to arise in real galaxies, and that our results do not affect dynamical friction constraints on halo density. Stochastic variations in the evolution are inevitable in all simulations of disc-halo systems, irrespective of how they were created, although their effect is generally far less extreme than we find here. The possibility of divergent behaviour complicates comparison of results from different workers.",
    "MGT": "N-body simulations are a cornerstone of galaxy formation theory, providing insights into the dynamics and evolution of disc galaxies. However, these simulations are inherently subject to stochasticity arising from the discrete representation of collisionless matter. This paper investigates the impact of stochasticity on the evolution of simulated disc galaxies, focusing on the effects of varying particle number (N) and softening length (ε) on key galaxy properties. We perform a suite of simulations of isolated disc galaxies with varying N ranging from 10^5 to 10^7 particles and softening lengths ranging from 0.1 to 1 kpc. We analyze the resulting galaxy morphology, star formation history, and the development of spiral structure. Our results demonstrate that stochasticity can significantly influence the evolution of disc galaxies, particularly at lower particle numbers. We find that simulations with lower N exhibit more pronounced fluctuations in star formation rates, reflecting the increased discreteness noise. Furthermore, the morphology of the simulated galaxies is also affected, with lower N simulations showing a tendency towards the formation of more flocculent and less well-defined spiral arms. We quantify the impact of stochasticity on the radial distribution of stellar mass and find that low-N simulations deviate significantly from the expected smooth profiles. This deviation is attributed to the increased influence of individual particle interactions, leading to enhanced scattering and radial migration. The choice of softening length also plays a crucial role in mitigating stochastic effects. Smaller softening lengths can exacerbate the impact of close encounters, while excessively large softening lengths can suppress the formation of fine-scale structures. Our findings emphasize the importance of carefully considering the effects of stochasticity when interpreting the results of N-body simulations. We provide guidelines for selecting appropriate values of N and ε to minimize the influence of stochastic noise and ensure the reliability of simulation results. This work contributes to a better understanding of the limitations and uncertainties associated with N-body simulations of disc galaxies, and provides a framework for improving the accuracy and robustness of future simulations.\n"
  },
  {
    "id": 5,
    "prompt": "Cox rings, semigroups and automorphisms of affine algebraic varieties",
    "HWT": "We study the Cox realization of an affine variety, i.e., a canonical representation of a normal affine variety with finitely generated divisor class group as a quotient of a factorially graded affine variety by an action of the Neron-Severi quasitorus. The realization is described explicitly for the quotient space of a linear action of a finite group. A universal property of this realization is proved, and some results on the divisor theory of an abstract semigroup emerging in this context are given. We show that each automorphism of an affine variety can be lifted to an automorphism of the Cox ring normalizing the grading. It follows that the automorphism group of a non-degenerate affine toric variety of dimension $\\geq 2$ has infinite dimension. We obtain a wild automorphism of the three-dimensional quadratic cone that rises to Anick's automorphism of the polynomial algebra in four variables.",
    "MGT": "Cox rings offer a powerful tool for studying the geometry of algebraic varieties, particularly regarding their divisor class groups and automorphism groups. This article explores the intricate relationship between Cox rings, the semigroups arising from their defining data, and the automorphism groups of affine algebraic varieties. We focus on varieties whose Cox rings are finitely generated, allowing for a combinatorial description of their structure via associated semigroups. These semigroups, which encode information about effective divisors, facilitate the computation of automorphism groups. We present new methods for determining the structure of these groups, leveraging the interplay between the algebraic geometry of the variety and the combinatorial structure of the semigroup. We illustrate these methods through concrete examples, including varieties with torus actions and those arising from combinatorial data. Furthermore, we investigate the connection between the finite generation of the Cox ring and the structure of the automorphism group, providing insights into the geometric properties of varieties with well-behaved Cox rings.\n"
  },
  {
    "id": 6,
    "prompt": "The North American and Pelican Nebulae I. IRAC Observations",
    "HWT": "We present a 9 deg^2 map of the North American and Pelican Nebulae regions obtained in all four IRAC channels with the Spitzer Space Telescope. The resulting photometry is merged with that at JHKs from 2MASS and a more spatially limited $BVI$ survey from previous ground-based work. We use a mixture of color- color diagrams to select a minimally contaminated set of more than 1600 objects that we claim are young stellar objects (YSOs) associated with the star forming region. Because our selection technique uses IR excess as a requirement, our sample is strongly biased against inclusion of Class III YSOs. The distribution of IRAC spectral slopes for our YSOs indicates that most of these objects are Class II, with a peak towards steeper spectral slopes but a substantial contribution from a tail of flat spectrum and Class I type objects. By studying the small fraction of the sample that is optically visible, we infer a typical age of a few Myr for the low mass population. The young stars are clustered, with about a third of them located in eight clusters that are located within or near the LDN 935 dark cloud. Half of the YSOs are located in regions with surface densities higher than 1000 YSOs / deg^2. The Class I objects are more clustered than the Class II stars.",
    "MGT": "The North America and Pelican Nebulae (NGC 7000 and IC 5070) are two of the most prominent HII regions in the northern sky, ionized by the massive O star HD 199579. Located at a distance of approximately 550 pc, these nebulae offer a valuable opportunity to study the impact of massive stars on their surrounding interstellar medium. This study presents an analysis of new infrared observations obtained with the Infrared Array Camera (IRAC) on the Spitzer Space Telescope at 3.6, 4.5, 5.8, and 8.0 μm. These data provide a comprehensive view of the nebulae's morphology, dust distribution, and star formation activity.\n\nWe have produced high-resolution mosaics of the nebulae in all four IRAC bands, revealing intricate details of the ionized gas, photodissociation regions (PDRs), and embedded young stellar objects (YSOs). The IRAC images show a complex network of bright rims and dark lanes, tracing the interaction between the ionizing radiation from HD 199579 and the surrounding molecular cloud. We have identified and characterized a large population of YSOs within the nebulae, using color-color diagrams and spectral energy distributions (SEDs) to classify them according to their evolutionary stage.\n\nOur analysis reveals that the star formation activity is concentrated in the dense molecular clouds located behind the ionization front. We find evidence for triggered star formation, where the expansion of the HII region compresses the surrounding gas, leading to the collapse of pre-existing cores. The IRAC data also provide valuable information on the dust composition and distribution within the nebulae. We have mapped the spatial variations in the dust temperature and emissivity, which can be used to constrain models of dust processing and grain evolution. These observations provide a crucial foundation for understanding the complex interplay between massive stars, the interstellar medium, and star formation in the North America and Pelican Nebulae.\n"
  },
  {
    "id": 7,
    "prompt": "Nonlinear current-voltage behavior and electrically driven phase transition in charge frustrated LuFe2O4",
    "HWT": "Electric transport measurements of the charge frustrated LuFe2O4, in which the charge ordering (CO) and electronic ferroelectricity are found, reveal strong nonlinear electric conduction upon application of electrical field in both single crystalline and polycrystalline samples. The threshold electric fields (Et) in single crystalline LuFe2O4 are estimated respectively to be about 60V/cm and 10V/cm with E parallel and perpendicular to the c-axis direction. Experimental measurements also demonstrate that the I-V nonlinearity increases quickly with lowering temperature. Furthermore, our in-situ TEM investigations evidently reveal that the nonlinear I-V behavior is intrinsically in correlation with a current driven charge ordering insulator-metal transition, and the applied electrical field triggers a visible CO collapse recognizable as the fading of satellite spots of the CO modulations.",
    "MGT": "LuFe2O4, a charge-frustrated material, exhibits intriguing electrical properties due to the complex interplay of charge, spin, and lattice degrees of freedom. This study investigates the nonlinear current-voltage (I-V) characteristics and electrically driven phase transitions in LuFe2O4 single crystals. We observed a pronounced nonlinear I-V behavior at low temperatures, characterized by a threshold voltage above which the current increases dramatically. This threshold voltage decreases with increasing temperature. Further investigation reveals that the observed nonlinearity is associated with an electrically induced phase transition from a high-resistance to a low-resistance state. The transition is accompanied by changes in the dielectric properties and is attributed to the redistribution of charge carriers within the Fe bilayers, which effectively melts the charge order. The findings provide insights into the electrical transport mechanisms in charge-frustrated systems and highlight the potential for utilizing electrical fields to control the material's properties for electronic device applications.\n"
  },
  {
    "id": 8,
    "prompt": "Non-cooperative games for spreading code optimization, power control and receiver design in wireless data networks",
    "HWT": "This paper focuses on the issue of energy efficiency in wireless data networks through a game theoretic approach. The case considered is that in which each user is allowed to vary its transmit power, spreading code, and uplink receiver in order to maximize its own utility, which is here defined as the ratio of data throughput to transmit power. In particular, the case in which linear multiuser detectors are employed at the receiver is treated first, and, then, the more challenging case in which non-linear decision feedback multiuser receivers are adopted is addressed. It is shown that, for both receivers, the problem at hand of utility maximization can be regarded as a non-cooperative game, and it is proved that a unique Nash equilibrium point exists. Simulation results show that significant performance gains can be obtained through both non-linear processing and spreading code optimization; in particular, for systems with a number of users not larger than the processing gain, remarkable gains come from spreading code optimization, while, for overloaded systems, the largest gainscome from the use of non-linear processing. In every case, however, the non-cooperative games proposed here are shown to outperform competing alternatives.",
    "MGT": "This paper investigates the application of non-cooperative game theory to resource allocation problems in wireless data networks, specifically focusing on code optimization, power control, and receiver design. In the context of code optimization, we model the selection of spreading codes by users as a non-cooperative game where each user aims to minimize interference to other users while maximizing their own data rate. We propose a distributed algorithm based on iterative best response dynamics, proving its convergence to a Nash equilibrium under certain conditions. Simulation results demonstrate that the proposed code optimization scheme achieves significant improvements in network throughput compared to random code assignment.\n\nNext, we address the power control problem by formulating it as a non-cooperative power control game. Here, each user adjusts their transmit power to maximize their signal-to-interference-plus-noise ratio (SINR) while minimizing power consumption. We analyze the existence and uniqueness of Nash equilibria in this game, deriving conditions for convergence of iterative power control algorithms. Furthermore, we introduce a pricing mechanism to discourage excessive power usage and improve overall network efficiency. Simulations reveal that the proposed power control scheme effectively mitigates interference and enhances network capacity.\n\nFinally, we consider the receiver design problem, modeling the selection of receiver parameters, such as filter coefficients, as a non-cooperative game. Each receiver seeks to optimize its performance in terms of signal detection accuracy while minimizing the impact of interference from other users. We propose a distributed receiver design algorithm based on game-theoretic principles, demonstrating its effectiveness in improving receiver performance in challenging interference environments. Simulation results validate the benefits of the proposed approach in terms of reduced bit error rate and improved signal quality.\n"
  },
  {
    "id": 9,
    "prompt": "Galactic Small Scale Structure Revealed by the GALFA-HI Survey",
    "HWT": "The Galactic Arecibo L-band Feed Array HI (GALFA-HI) survey is mapping the entire Arecibo sky at 21-cm, over a velocity range of -700 to +700 km/s (LSR), at a velocity resolution of 0.18 km/s and an angular resolution of 3.5 arcmin. The unprecedented resolution and sensitivity of the GALFA-HI survey have resulted in the detection of many isolated, very compact HI clouds at low Galactic velocities which are distinctly separated from the HI disk emission. In the limited area of ~4600 deg$^2$ searched so far, we have detected 96 such compact clouds. The detected clouds are cold with kinetic temperature less than 300 K. Moreover, they are quite compact and faint, with median values of 5 arcmin in angular size, 0.75 K in peak brightness temperature, and $5\\times10^{18}$ cm$^{-2}$ in HI column density. From the modeling of spatial and velocity distributions of the whole compact cloud population, we find that the bulk of clouds are related to the Galactic disk, and are within a few kpc distance. We present properties of the compact clouds sample and discuss various possible scenarios for the origin of this clouds population and its role in the Galactic interstellar medium studies.",
    "MGT": "The Galactic Arecibo L-band Feed Array HI (GALFA-HI) survey provides an unprecedented view of the neutral hydrogen (HI) distribution in the Milky Way, revealing a wealth of small-scale structures. This study leverages the GALFA-HI data to investigate the properties and distribution of these structures, focusing on features with angular sizes ranging from a few arcminutes to approximately one degree. We employ a combination of visual inspection and automated feature extraction techniques to identify and characterize HI clouds, filaments, and shells across a significant portion of the northern sky. Our analysis reveals a complex hierarchy of structures, with larger features often exhibiting internal substructure. We examine the kinematic properties of these features, including their velocities, velocity dispersions, and velocity gradients, to understand their dynamics and potential origins. We find evidence for both coherent, dynamically-relaxed structures and more turbulent, actively-forming regions. Furthermore, we investigate the spatial distribution of these small-scale features, searching for correlations with known Galactic features such as spiral arms and molecular clouds. Our results suggest that a significant fraction of the Galactic HI resides in small-scale structures, highlighting the importance of high-resolution observations for understanding the multiphase interstellar medium. We discuss the implications of our findings for models of HI cloud formation, turbulence, and the overall lifecycle of gas in the Milky Way. The GALFA-HI survey provides a valuable dataset for future studies aimed at unraveling the mysteries of Galactic small-scale structure and its role in the evolution of the Galaxy.\n"
  },
  {
    "id": 10,
    "prompt": "Spin Bose-Metal phase in a spin-1/2 model with ring exchange on a two-leg triangular strip",
    "HWT": "Recent experiments on triangular lattice organic Mott insulators have found evidence for a 2D spin liquid in proximity to the metal-insulator transition. A Gutzwiller wavefunction study of the triangular lattice Heisenberg model with appropriate four-spin ring exchanges has found that the projected spinon Fermi sea state has a low variational energy. This wavefunction, together with a slave particle gauge theory, suggests that such spin liquid possesses spin correlations that are singular along surfaces in momentum space (\"Bose surfaces\"). Signatures of this state, which we refer to as a \"Spin Bose-Metal\" (SBM), are expected to be manifest in quasi-1D ladder systems: The discrete transverse momenta cut through the 2D Bose surface leading to a distinct pattern of 1D gapless modes. Here we search for a quasi-1D descendant of the triangular lattice SBM state by exploring the Heisenberg plus ring model on a two-leg strip (zigzag chain). Using DMRG, variational wavefunctions, and a Bosonization analysis, we map out the full phase diagram. Without ring exchange the model is equivalent to the J_1 - J_2 Heisenberg chain, and we find the expected Bethe-chain and dimerized phases. Remarkably, moderate ring exchange reveals a new gapless phase over a large swath of the phase diagram. Spin and dimer correlations possess particular singular wavevectors and allow us to identify this phase as the hoped for quasi-1D descendant SBM state. We derive a low energy theory and find three gapless modes and one Luttinger parameter controlling all power laws. Potential instabilities out of the zigzag SBM give rise to other interesting phases such as a period-3 VBS or a period-4 Chirality order, which we discover in the DMRG; we also find an interesting SBM state with partial ferromagnetism.",
    "MGT": "We investigate the ground state properties of a spin-1/2 model on a two-leg triangular strip, incorporating both nearest-neighbor Heisenberg exchange ($J_1$) and four-spin ring exchange ($J_4$). This model, relevant to materials with strong electronic correlations and geometrical frustration, is explored using a combination of Density Matrix Renormalization Group (DMRG) and exact diagonalization techniques. Our primary focus is to elucidate the role of the ring exchange in stabilizing exotic quantum spin liquid phases.\n\nIn the absence of ring exchange ($J_4 = 0$), the system exhibits a gapless spin-liquid phase with algebraic decay of spin correlations, characteristic of a Luttinger liquid. However, upon introducing a finite $J_4$, we observe a transition into a distinct phase, which we identify as a spin Bose-Metal (SBM). This phase is characterized by gapless spinon excitations and power-law decaying spin correlations, similar to a Luttinger liquid, but with crucial differences in the correlation exponents and entanglement properties.\n\nThrough detailed analysis of the spin structure factor, we find evidence for incommensurate correlations, indicating a tendency towards spiral magnetic order. Furthermore, the entanglement entropy exhibits a logarithmic scaling with the subsystem size, consistent with gapless behavior. Crucially, the central charge extracted from the entanglement entropy differs significantly from that of a conventional Luttinger liquid, suggesting a more complex low-energy effective theory.\n\nWe propose that the SBM phase arises from the condensation of bosonic spinon pairs, induced by the ring exchange. This condensation leads to the formation of a gapless excitation spectrum and alters the nature of spin correlations. We further analyze the single-particle spinon gap and find that it remains gapless within the SBM phase, consistent with the presence of fractionalized excitations.\n\nOur results highlight the importance of ring exchange interactions in stabilizing exotic quantum spin liquid phases in frustrated spin systems. The spin Bose-Metal phase identified in this study provides a promising platform for exploring novel quantum phenomena and may be relevant to understanding the behavior of certain frustrated magnets. Finally, we discuss the implications of our findings for the search for SBM phases in real materials and propose future directions for research.\n"
  },
  {
    "id": 11,
    "prompt": "Assisted Common Information with Applications to Secure Two-Party Computation",
    "HWT": "Secure multi-party computation is a central problem in modern cryptography. An important sub-class of this are problems of the following form: Alice and Bob desire to produce sample(s) of a pair of jointly distributed random variables. Each party must learn nothing more about the other party's output than what its own output reveals. To aid in this, they have available a set up - correlated random variables whose distribution is different from the desired distribution - as well as unlimited noiseless communication. In this paper we present an upperbound on how efficiently a given set up can be used to produce samples from a desired distribution. The key tool we develop is a generalization of the concept of common information of two dependent random variables [Gacs-Korner, 1973]. Our generalization - a three-dimensional region - remedies some of the limitations of the original definition which captured only a limited form of dependence. It also includes as a special case Wyner's common information [Wyner, 1975]. To derive the cryptographic bounds, we rely on a monotonicity property of this region: the region of the \"views\" of Alice and Bob engaged in any protocol can only monotonically expand and not shrink. Thus, by comparing the regions for the target random variables and the given random variables, we obtain our upperbound.",
    "MGT": "This paper introduces the concept of assisted common information (ACI), a novel relaxation of common information, and explores its applications in secure two-party computation (2PC). Traditional common information requires two parties, Alice and Bob, to learn a shared random variable. ACI relaxes this requirement by allowing Alice to learn a public, possibly correlated, random variable, while Bob learns a related private random variable. The key constraint is that Bob's variable, when conditioned on Alice's, reveals the desired common information. We formalize this notion and provide information-theoretic bounds on the achievable rates of ACI.\n\nOur primary contribution is demonstrating how ACI can significantly enhance the efficiency of 2PC protocols. We present a new semi-honest 2PC protocol for general functions based on garbled circuits. By leveraging ACI, we reduce the communication complexity associated with oblivious transfer (OT) extension, a crucial component of garbled circuit evaluation. Specifically, we show that ACI enables us to amortize the cost of seed OTs over multiple garbled circuit evaluations, leading to substantial communication savings, particularly for large circuits.\n\nWe analyze the security of our protocol under the semi-honest adversary model and provide a rigorous proof demonstrating its security against malicious parties. Furthermore, we present experimental results showcasing the practical benefits of our ACI-based 2PC protocol. These results demonstrate a significant reduction in communication overhead compared to existing garbled circuit implementations, especially for computationally intensive functions. We also explore extensions of ACI to multi-party computation and discuss potential applications in other cryptographic primitives. Finally, we highlight the limitations of our approach and outline directions for future research, including investigating the applicability of ACI to malicious security settings and exploring alternative constructions of ACI for different types of correlations.\n"
  },
  {
    "id": 12,
    "prompt": "Quantum fluctuations in the transverse Ising spin glass model: A field theory of random quantum spin systems",
    "HWT": "We develop a mean-field theory for random quantum spin systems using the spin coherent state path integral representation. After the model is reduced to the mean field one-body Hamiltonian, the integral is analyzed with the aid of several methods such as the semiclassical method and the gauge transformation. As an application we consider the Sherrington-Kirkpatrick model in a transverse field. Using the Landau expansion and its improved versions, we give a detailed analysis of the imaginary-time dependence of the order parameters. Integrating out the quantum part of the order parameters, we obtain the effective renormalized free energy written in terms of the classically defined order parameters. Our method allows us to obtain the spin glass-paramagnetic phase transition point $\\Gamma/J\\sim 1.62$ at T=0.",
    "MGT": "We present a field-theoretical treatment of the transverse Ising spin glass model, focusing on the role of quantum fluctuations in shaping its low-temperature behavior. Employing the replica method and Parisi's replica symmetry breaking scheme, we derive an effective action that captures the interplay between disorder, quantum tunneling, and spin correlations. The model is analyzed using a variational approach, allowing us to estimate the critical temperature and the spin glass order parameter. We find that quantum fluctuations significantly suppress the spin glass transition temperature compared to the classical Ising spin glass. Our analysis reveals a rich phase diagram with a quantum spin glass phase characterized by a non-trivial distribution of local fields. Furthermore, we investigate the influence of the transverse field on the distribution of the spin glass order parameter, uncovering deviations from the classical Gaussian behavior. The results provide insights into the nature of quantum spin glasses and their potential applications in quantum information processing.\n"
  },
  {
    "id": 13,
    "prompt": "The Kadomtsev-Petviashvili II Equation on the Half-Plane",
    "HWT": "The KPII equation is an integrable nonlinear PDE in 2+1 dimensions (two spatial and one temporal), which arises in several physical circumstances, including fluid mechanics where it describes waves in shallow water. It provides a multidimensional generalisation of the renowned KdV equation. In this work, we employ a novel approach recently introduced by one of the authors in connection with the Davey-Stewartson equation \\cite{FDS2009}, in order to analyse the initial-boundary value problem for the KPII equation formulated on the half-plane. The analysis makes crucial use of the so-called d-bar formalism, as well as of the so-called global relation. A novel feature of boundary as opposed to initial-value problems in 2+1 is that the d-bar formalism now involves a function in the complex plane which is discontinuous across the real axis.",
    "MGT": "This article investigates the Kadomtsev-Petviashvili II (KPII) equation posed on the half-plane, a domain relevant to modeling wave propagation in physical systems with boundaries. We focus on constructing solutions that satisfy specific boundary conditions, exploring both Dirichlet and Neumann type conditions. The analysis employs the unified transform method, also known as the Fokas method, a powerful technique for solving boundary value problems for integrable nonlinear partial differential equations. The method involves formulating a global relation that couples the unknown boundary values with the given initial and boundary data. We derive explicit integral representations for the solution in terms of these boundary values. The novelty lies in the application of this method to the KPII equation on the half-plane, which presents unique challenges compared to its application on the whole plane. We discuss the spectral analysis required for the implementation of the unified transform and provide insights into the characterization of the unknown boundary spectral functions. Furthermore, we explore the long-time asymptotics of the solution, revealing how the boundary influences the wave behavior.\n"
  },
  {
    "id": 14,
    "prompt": "Two-loop renormalization of scalar and pseudoscalar fermion bilinears on the lattice",
    "HWT": "We compute the two-loop renormalization functions, in the RI $^\\prime$ scheme, of local bilinear quark operators $\\bar{\\psi}\\Gamma\\psi$, where $\\Gamma$ denotes the Scalar and Pseudoscalar Dirac matrices, in the lattice formulation of QCD. We consider both the flavor non-singlet and singlet operators; the latter, in the scalar case, leads directly to the two-loop fermion mass renormalization, $Z_m$. As a prerequisite for the above, we also compute the quark field renormalization, $Z_{\\psi}$, up to two loops. We use the clover action for fermions and the Wilson action for gluons. Our results are given as a polynomial in $c_{SW}$, in terms of both the renormalized and bare coupling constant, in the renormalized Feynman gauge. We also confirm the 1-loop renormalization functions, for generic gauge. Finally, we present our results in the $\\bar{MS}$ scheme, for easier comparison with calculations in the continuum. The corresponding results, for fermions in an arbitrary representation, are included in an Appendix.",
    "MGT": "We present a comprehensive two-loop lattice perturbation theory calculation of the renormalization factors for scalar and pseudoscalar fermion bilinear operators, employing Wilson fermions. Our computation utilizes an automated setup involving symbolic manipulation software to handle the complexities of multi-loop diagrams on the lattice. We calculate the required renormalization factors in the RI'-MOM scheme, a non-perturbative renormalization scheme commonly used in lattice QCD. Our results are obtained for general values of the lattice spacing and Wilson parameter, allowing for application to a wide range of lattice discretizations. We analyze the convergence properties of the perturbative expansion and quantify the impact of lattice artifacts on the renormalization factors. Furthermore, we investigate the mixing between scalar and pseudoscalar operators, induced by the explicit breaking of chiral symmetry with Wilson fermions, and determine the corresponding mixing coefficients. Our findings provide crucial input for precision calculations of hadron matrix elements relevant for flavor physics and searches for new physics. The presented results are essential for obtaining accurate physical results from lattice QCD simulations involving scalar and pseudoscalar densities. We provide numerical estimates for specific lattice parameters and compare our results to existing one-loop calculations, highlighting the importance of including two-loop corrections.\n"
  },
  {
    "id": 15,
    "prompt": "The stubborn problem is stubborn no more (a polynomial algorithm for 3-compatible colouring and the stubborn list partition problem)",
    "HWT": "One of the driving problems in the CSP area is the Dichotomy Conjecture, formulated in 1993 by Feder and Vardi [STOC'93], stating that for any fixed relational structure G the Constraint Satisfaction Problem CSP(G) is either NP--complete or polynomial time solvable. A large amount of research has gone into checking various specific cases of this conjecture. One such variant which attracted a lot of attention in the recent years is the LIST MATRIX PARTITION problem. In 2004 Cameron et al. [SODA'04] classified almost all LIST MATRIX PARTITION variants for matrices of size at most four. The only case which resisted the classification became known as the STUBBORN PROBLEM. In this paper we show a result which enables us to finish the classification - thus solving a problem which resisted attacks for the last six years. Our approach is based on a combinatorial problem known to be at least as hard as the STUBBORN PROBLEM - the 3-COMPATIBLE COLOURING problem. In this problem we are given a complete graph with each edge assigned one of 3 possible colours and we want to assign one of those 3 colours to each vertex in such a way that no edge has the same colour as both of its endpoints. The tractability of the 3-COMPATIBLE COLOURING problem has been open for several years and the best known algorithm prior to this paper is due to Feder et al. [SODA'05] - a quasipolynomial algorithm with a n^O(log n / log log n) time complexity. In this paper we present a polynomial-time algorithm for the 3-COMPATIBLE COLOURING problem and consequently we prove a dichotomy for the k-COMPATIBLE COLOURING problem.",
    "MGT": "The 3-Compatible Colouring problem, a variant of graph colouring with constraints on vertex neighbourhoods, has remained a challenging puzzle in algorithmic graph theory. Given a graph *G* and a set of colours, the problem asks whether there exists a colouring of *G* such that for every vertex *v*, at least three neighbours of *v* have colours different from the colour of *v*. Despite its seemingly simple formulation, the problem's computational complexity has been open, with existing approaches primarily focusing on approximation algorithms and heuristics. This paper presents a polynomial-time algorithm that resolves the complexity of 3-Compatible Colouring, demonstrating that it belongs to the class P.\n\nOur approach hinges on a novel reduction to a carefully constructed instance of 2-SAT. By encoding the compatibility constraints within the clauses of the 2-SAT formula, we are able to capture the essence of the 3-Compatible Colouring problem while leveraging the polynomial-time solvability of 2-SAT. The reduction involves a sophisticated analysis of the local structure of the graph and the interplay between vertex colour assignments and neighbourhood constraints. We demonstrate the correctness of our reduction, proving that a satisfying assignment to the 2-SAT formula directly corresponds to a valid 3-compatible colouring of the original graph, and vice versa.\n\nFurthermore, we extend our results to address the related Stubborn List Partition Problem. In this problem, we are given a graph *G* and a list of allowed colour sets for each vertex. The goal is to determine if there exists a partition of the vertex set such that each vertex is assigned a colour from its allowed list, and the resulting colouring satisfies a specific stubbornness condition. We show that the Stubborn List Partition Problem can also be solved in polynomial time using a similar reduction technique, building upon the insights gained from our solution to the 3-Compatible Colouring problem. This establishes a broader class of colouring problems solvable in polynomial time.\n\nThe presented algorithms offer a significant advancement in our understanding of graph colouring problems with neighbourhood constraints. By providing polynomial-time solutions for both 3-Compatible Colouring and the Stubborn List Partition Problem, we contribute to the development of efficient algorithms for real-world applications in areas such as resource allocation, scheduling, and network design. Our work opens up new avenues for research in graph algorithms and complexity theory, suggesting that other seemingly intractable colouring problems may also be amenable to polynomial-time solutions through clever reductions and algorithmic techniques.\n"
  },
  {
    "id": 16,
    "prompt": "SAM Lectures on Extremal Black Holes in d=4 Extended Supergravity",
    "HWT": "We report on recent results in the study of extremal black hole attractors in N=2, d=4 ungauged Maxwell-Einstein supergravities. For homogeneous symmetric scalar manifolds, the three general classes of attractor solutions with non-vanishing Bekenstein-Hawking entropy are discussed. They correspond to three (inequivalent) classes of orbits of the charge vector, which sits in the relevant symplectic representation R_{V} of the U-duality group. Other than the 1/2-BPS one, there are two other distinct non-BPS classes of charge orbits, one of which has vanishing central charge. The complete classification of the U-duality orbits, as well as of the moduli spaces of non-BPS attractors (spanned by the scalars which are not stabilized at the black hole event horizon), is also reviewed. Finally, we consider the analogous classification for N>2-extended, d=4 ungauged supergravities, in which also the 1/N-BPS attractors yield a related moduli space.",
    "MGT": "This article presents a detailed analysis of extremal black hole solutions within the framework of four-dimensional extended supergravity. We focus on the application of Sen's Attractor Mechanism (SAM) to determine the near-horizon geometry and physical properties of these black holes. Specifically, we explore the role of scalar fields in the attractor flow and their stabilization at the horizon, driven by the minimization of an effective potential derived from the supergravity Lagrangian. Our investigation encompasses various N = 2, 4, and 8 supergravity theories, examining how the degree of supersymmetry influences the structure of the attractor equations and the resulting black hole solutions. We derive explicit solutions for several representative cases, highlighting the interplay between electric and magnetic charges, scalar field configurations, and the resulting black hole entropy. Furthermore, we analyze the connection between the attractor mechanism and the underlying U-duality symmetries of the supergravity theory, demonstrating how these symmetries can be used to generate new black hole solutions from known ones. The results provide insights into the microscopic origin of black hole entropy and the role of supergravity in string theory compactifications.\n"
  },
  {
    "id": 17,
    "prompt": "Enhancing extraordinary transmission of light through a metallic nano slit with a nano cavity antenna",
    "HWT": "The extraordinary transmission of light through a nano slit in a metal film is enhanced by introducing a nano cavity antenna formed by a nearby metallic nano-strip over the slit opening. For a fixed wavelength, the width of the metallic nano-strip should be chosen to make the horizontal metal-insulator-metal waveguide of finite length resonant as a Fabry-Perot cavity. When such a cavity antenna is used to enhance the transmission through a non-resonant nano slit, the slit should be opened at a position with maximal magnetic field in the horizontal resonant cavity. It is shown that an optimized cavity antenna can enhance greatly the transmission of light through a non-resonant nano slit (by about 20 times) or a resonant nano slit (by 124%). The transmission spectrum of the nano slit can also be tuned by adjusting the width of the metallic nano-strip. Such a transmission enhancement with a nano cavity antenna is studied for the first time and the physical mechanism is explained.",
    "MGT": "This study investigates the enhancement of extraordinary optical transmission (EOT) through a single metallic nano-slit by integrating a nano-cavity antenna (NCA) adjacent to the slit. The proposed structure leverages the localized surface plasmon resonance (LSPR) of the NCA to efficiently couple incident light into the nano-slit, thereby amplifying the transmitted optical power. Finite-difference time-domain (FDTD) simulations demonstrate a significant increase in the transmission efficiency compared to a bare nano-slit, with a resonant peak red-shifted due to the NCA's influence. Parametric studies are conducted to optimize the NCA's dimensions and its position relative to the nano-slit, revealing a strong dependence of the EOT enhancement on these parameters. The underlying mechanism is attributed to the near-field coupling between the NCA and the nano-slit, resulting in a concentrated electromagnetic field within the slit. Furthermore, the impact of varying the metal material and surrounding dielectric environment is explored. The findings suggest potential applications in nanoscale optical devices, including sensors, waveguides, and light sources, where efficient light manipulation at the subwavelength scale is crucial. The proposed structure offers a promising pathway for achieving high-performance plasmonic devices with enhanced light-matter interaction.\n"
  },
  {
    "id": 18,
    "prompt": "A universal exponential factor in the dimensional crossover from graphene to graphite",
    "HWT": "A universal exponential factor, $\\gamma_{c}=\\pi/2$, is disclosed for the dimensional crossover of few-layer graphene (FLG) from two-dimensional graphene to three-dimensional graphite. $\\gamma_{c}$ is found by analyzing available experimental data on different properties of FLG with varying thickness. A theoretical study on the phonon spectrum of the vertical acoustic mode in FLG is carried out to further check this exponential factor $\\gamma_{c}$. Interestingly, the same exponential factor appears in the dimensional crossover of the phonon mode. It turns out that the exponential factor $\\gamma_{c}$ is related to the homogeneous Helmholtz-like molal equation in the mass transfer with a first order chemical reaction. The finding should provide valuable information for experimentalists and theorists in the future investigation on thickness dependent properties of FLG.",
    "MGT": "The electronic properties of layered materials like graphene evolve significantly as the number of layers increases, eventually approaching those of graphite. This dimensional crossover is crucial for understanding and tailoring the behavior of these materials in diverse applications. We present a theoretical and computational investigation of the electronic structure of multilayer graphene, focusing on the evolution of the density of states near the Fermi level as a function of the number of layers. Our analysis reveals a universal exponential factor governing the interlayer coupling strength, which dictates the transition from the two-dimensional characteristics of graphene to the three-dimensional behavior of graphite. This exponential factor is directly related to the hopping parameter between adjacent layers and the interlayer spacing. Furthermore, we demonstrate that this universal scaling behavior is independent of the stacking order and the presence of external electric fields. Our findings provide a fundamental understanding of the dimensional crossover in layered materials and offer valuable insights for designing novel electronic devices based on graphene heterostructures.\n"
  },
  {
    "id": 19,
    "prompt": "Shrinkage regression for multivariate inference with missing data, and an application to portfolio balancing",
    "HWT": "Portfolio balancing requires estimates of covariance between asset returns. Returns data have histories which greatly vary in length, since assets begin public trading at different times. This can lead to a huge amount of missing data--too much for the conventional imputation-based approach. Fortunately, a well-known factorization of the MVN likelihood under the prevailing historical missingness pattern leads to a simple algorithm of OLS regressions that is much more reliable. When there are more assets than returns, however, OLS becomes unstable. Gramacy, et al. (2008), showed how classical shrinkage regression may be used instead, thus extending the state of the art to much bigger asset collections, with further accuracy and interpretation advantages. In this paper, we detail a fully Bayesian hierarchical formulation that extends the framework further by allowing for heavy-tailed errors, relaxing the historical missingness assumption, and accounting for estimation risk. We illustrate how this approach compares favorably to the classical one using synthetic data and an investment exercise with real returns. An accompanying R package is on CRAN.",
    "MGT": "This article introduces a novel shrinkage regression approach tailored for multivariate inference in the presence of missing data, with a specific application to portfolio balancing. The methodology leverages the power of regularization techniques to mitigate the adverse effects of missing values and high dimensionality, common challenges encountered in financial datasets. We propose a composite estimator that combines ridge regression with imputation strategies, effectively shrinking parameter estimates towards a more stable and generalizable solution. The imputation step employs a k-nearest neighbors approach, dynamically adapting to the local data structure and minimizing imputation bias. The ridge regression component then stabilizes the estimation process, particularly when dealing with highly correlated assets.\n\nThe proposed method is evaluated through extensive simulation studies, demonstrating its superior performance compared to traditional complete-case analysis and single imputation methods in terms of bias, variance, and prediction accuracy. Furthermore, we apply the methodology to a real-world portfolio balancing problem, using historical stock market data with inherent missing values. The results indicate that the shrinkage regression approach leads to portfolios with improved risk-adjusted returns and reduced turnover compared to portfolios constructed using conventional methods. This highlights the practical relevance and potential benefits of the proposed methodology for financial decision-making in incomplete data environments. The framework is flexible and can be extended to other multivariate settings beyond portfolio balancing.\n"
  },
  {
    "id": 20,
    "prompt": "Galaxy Satellites and the Weak Equivalence Principle",
    "HWT": "Numerical simulations of the effect of a long-range scalar interaction (LRSI) acting only on nonbaryonic dark matter, with strength comparable to gravity, show patterns of disruption of satellites that can agree with what is seen in the Milky Way. This includes the symmetric Sagittarius stellar stream. The exception presented here to the Kesden and Kamionkowski demonstration that an LRSI tends to produce distinctly asymmetric streams follows if the LRSI is strong enough to separate the stars from the dark matter before tidal disruption of the stellar component, and if stars dominate the mass in the luminous part of the satellite. It requires that the Sgr galaxy now contains little dark matter, which may be consistent with the Sgr stellar velocity dispersion, for in the simulation the dispersion at pericenter exceeds virial. We present other examples of simulations in which a strong LRSI produces satellites with large mass-to-light ratio, as in Draco, or free streams of stars, which might be compared to \"orphan\" streams.",
    "MGT": "The weak equivalence principle (WEP), a cornerstone of general relativity, posits the universality of free fall, asserting that all objects accelerate equally in a gravitational field regardless of their mass or composition. While extensively tested on Earth and within the solar system, the WEP remains largely unexplored on galactic scales. This study investigates the dynamics of satellite galaxies orbiting Milky Way-like host galaxies in cosmological simulations to constrain potential violations of the WEP. By analyzing the radial acceleration relation (RAR) of these satellites, we search for deviations from Newtonian gravity that correlate with satellite properties, such as stellar mass, metallicity, and dark matter halo concentration. Our analysis reveals no statistically significant evidence for WEP violations within the current observational uncertainties. However, we identify a subtle trend suggesting a possible correlation between satellite stellar mass and RAR residuals, warranting further investigation with larger, higher-resolution simulations and improved observational data. We discuss the implications of our findings for modified Newtonian dynamics (MOND) and other alternative gravity theories, highlighting the need for more precise measurements of satellite galaxy kinematics and compositions to definitively test the WEP on galactic scales.\n"
  },
  {
    "id": 21,
    "prompt": "Thermal and magnetic properties of a low-temperature antiferromagnet Ce$_4$Pt$_{12}$Sn$_{25}$",
    "HWT": "We report specific heat ($C$) and magnetization ($M$) of single crystalline Ce$_4$Pt$_{12}$Sn$_{25}$ at temperature down to $\\sim$50mK and in fields up to 3T. $C/T$ exhibits a sharp anomaly at 180mK, with a large $\\Delta C/T\\sim$30J/molK$^2$-Ce, which, together with the corresponding cusp-like magnetization anomaly, indicates an antiferromagnetic (AFM) ground state with a N\\'eel temperature $T_N$=180mK. Numerical calculations based on a Heisenberg model reproduce both zero-field $C$ and $M$ data, thus placing Ce$_4$Pt$_{12}$Sn$_{25}$ in the weak exchange coupling $J<J_c$ limit of the Doniach diagram, with a very small Kondo scale $T_K\\ll T_N$. Magnetic field suppresses the AFM state at $H^*\\approx$0.7T, much more effectively than expected from the Heisenberg model, indicating additional effects possibly due to frustration or residual Kondo screening.",
    "MGT": "We report on the thermal and magnetic properties of the heavy-fermion antiferromagnet Ce$_4$Pt$_{12}$Sn$_{25}$ single crystals. Measurements of magnetic susceptibility, specific heat, and electrical resistivity reveal antiferromagnetic (AFM) order at $T_N \\approx 1.9$ K. The magnetic susceptibility exhibits Curie-Weiss behavior at high temperatures, with an effective magnetic moment close to the free-ion value of Ce$^{3+}$. A sharp peak in the specific heat at $T_N$ confirms the bulk nature of the AFM transition. The Sommerfeld coefficient, $\\gamma$, is estimated to be 480 mJ/mol K$^2$, indicating moderately enhanced quasiparticle masses. The magnetic entropy released at $T_N$ is significantly reduced compared to $4R\\ln{2}$, suggesting Kondo screening effects. Electrical resistivity shows a Kondo-like logarithmic increase with decreasing temperature above $T_N$, followed by a sharp drop below $T_N$, consistent with the opening of a gap on part of the Fermi surface. Magnetoresistance measurements exhibit a negative response at low temperatures, which could be attributed to the suppression of spin fluctuations by the magnetic field. The upper critical field is estimated to be approximately 1.2 T. Our findings suggest that Ce$_4$Pt$_{12}$Sn$_{25}$ is a moderately heavy-fermion antiferromagnet with Kondo interactions playing a significant role in shaping its low-temperature properties.\n"
  },
  {
    "id": 22,
    "prompt": "Profiles of emission lines generated by rings orbiting braneworld Kerr black holes",
    "HWT": "In the framework of the braneworld models, rotating black holes can be described by the Kerr metric with a tidal charge representing the influence of the non-local gravitational (tidal) effects of the bulk space Weyl tensor onto the black hole spacetime. We study the influence of the tidal charge onto profiled spectral lines generated by radiating tori orbiting in vicinity of a rotating black hole. We show that with lowering the negative tidal charge of the black hole, the profiled line becomes to be flatter and wider keeping their standard character with flux stronger at the blue edge of the profiled line. The extension of the line grows with radius falling and inclination angle growing. With growing inclination angle a small hump appears in the profiled lines due to the strong lensing effect of photons coming from regions behind the black hole. For positive tidal charge ($b>0$) and high inclination angles two small humps appear in the profiled lines close to the red and blue edge of the lines due to the strong lensing effect. We can conclude that for all values of $b$, the strongest effect on the profiled lines shape (extension) is caused by the changes of the inclination angle.",
    "MGT": "This study investigates the properties of emission lines originating from geodesic rings orbiting braneworld Kerr black holes. Braneworld models propose the existence of extra spatial dimensions, potentially altering the gravitational field near compact objects. We analyze the spectral features of these emission lines, focusing on how the braneworld parameter, which quantifies the deviation from standard Kerr spacetime, influences the line profiles. Utilizing a ray-tracing approach, we simulate the propagation of photons emitted from a geometrically thin, Keplerian accretion disk orbiting the black hole. The observed flux is then computed by integrating the specific intensity over the image plane, taking into account relativistic effects such as gravitational redshift, Doppler boosting, and light bending.\n\nOur results demonstrate that the braneworld parameter significantly affects the shape and position of the emission lines. Increasing the braneworld parameter leads to a more compact black hole shadow and a shift in the emission line peaks towards higher frequencies, attributable to the stronger gravitational field. We observe that the characteristic double-horned profile, typical of accretion disks around Kerr black holes, is modified by the presence of the braneworld parameter, revealing distinct features that could potentially be used to observationally distinguish between braneworld and standard Kerr black holes. Furthermore, we explore the impact of the black hole's spin on the emission line profiles, finding that higher spin values amplify the effects of the braneworld parameter. This work provides valuable insights into the potential observational signatures of braneworld black holes and offers a framework for testing alternative theories of gravity using astrophysical observations of accretion disks.\n"
  },
  {
    "id": 23,
    "prompt": "Eta Carinae and Nebulae Around Massive Stars: Similarities to Planetary Nebulae?",
    "HWT": "I discuss some observational properties of aspherical nebulae around massive stars, and conclusions inferred for how they may have formed. Whether or not these ideas are applicable to the shaping of planetary nebulae is uncertain, but the observed similarities between some PNe and bipolar nebulae around massive stars is compelling. In the well-observed case of Eta Carinae, several lines of observational evidence point to a scenario where the shape of its bipolar nebula resulted from an intrinsically bipolar explosive ejection event rather than an interacting winds scenario occurring after ejection from teh star. A similar conclusion has been inferred for some planetary nebulae. I also briefly mention bipolar nebulae around some other massive stars, such as the progenitor of SN 1987A and related blue supergiants.",
    "MGT": "Eta Carinae, a massive interacting binary system surrounded by the Homunculus Nebula, exhibits striking morphological and kinematic similarities to planetary nebulae (PNe). This raises questions about the evolutionary connections between evolved massive stars and the progenitors of PNe. We explore these similarities by comparing the shaping mechanisms operating in both types of nebulae. Focusing on Eta Carinae, we analyze high-resolution spectroscopic and imaging data, revealing bipolar outflows, equatorial density enhancements, and evidence for interacting stellar winds. We then compare these features to those observed in PNe, particularly those known to be shaped by binary interaction. Our analysis suggests that similar physical processes, such as common envelope evolution or interacting stellar winds within binary systems, may play a crucial role in shaping both Eta Carinae and PNe. This comparison offers insight into the late stages of stellar evolution and the potential for massive stars to contribute to the population of bipolar nebulae previously thought to be solely associated with lower-mass stars.\n"
  },
  {
    "id": 24,
    "prompt": "Sum-Capacity and the Unique Separability of the Parallel Gaussian MAC-Z-BC Network",
    "HWT": "It is known that the capacity of parallel (e.g., multi-carrier) Gaussian point-to-point, multiple access and broadcast channels can be achieved by separate encoding for each subchannel (carrier) subject to a power allocation across carriers. Recent results have shown that parallel interference channels are not separable, i.e., joint coding is needed to achieve capacity in general. This work studies the separability, from a sum-capacity perspective, of single hop Gaussian interference networks with independent messages and arbitrary number of transmitters and receivers. The main result is that the only network that is always (for all values of channel coefficients) separable from a sum-capacity perspective is the MAC-Z-BC network, i.e., a network where a MAC component and a BC component are linked by a Z component. The sum capacity of this network is explicitly characterized.",
    "MGT": "This paper investigates the parallel Gaussian MAC-Z-BC network, a communication system combining multiple access, Z-channel, and broadcast functionalities. We derive the sum-capacity of this network under the assumption of unique separability, meaning that the optimal encoding and decoding strategies for each sub-network (MAC, Z-channel, BC) can be independently designed without sacrificing overall performance. This separability condition simplifies the complex joint optimization problem, allowing us to leverage existing capacity results for the individual components. Specifically, we demonstrate that the sum-capacity is achieved by time-sharing between different operational modes, each corresponding to a specific configuration of active users and desired message flows. Furthermore, we provide a closed-form expression for the sum-capacity as a function of the channel parameters (noise variances and link gains) and analyze the conditions under which the unique separability assumption holds. Simulation results validate our theoretical findings and illustrate the impact of various channel parameters on the achievable sum-capacity. Finally, we discuss the implications of our results for practical network design and future research directions.\n"
  },
  {
    "id": 25,
    "prompt": "Advances in Data Combination, Analysis and Collection for System Reliability Assessment",
    "HWT": "The systems that statisticians are asked to assess, such as nuclear weapons, infrastructure networks, supercomputer codes and munitions, have become increasingly complex. It is often costly to conduct full system tests. As such, we present a review of methodology that has been proposed for addressing system reliability with limited full system testing. The first approaches presented in this paper are concerned with the combination of multiple sources of information to assess the reliability of a single component. The second general set of methodology addresses the combination of multiple levels of data to determine system reliability. We then present developments for complex systems beyond traditional series/parallel representations through the use of Bayesian networks and flowgraph models. We also include methodological contributions to resource allocation considerations for system relability assessment. We illustrate each method with applications primarily encountered at Los Alamos National Laboratory.",
    "MGT": "System reliability assessment is crucial for ensuring the dependability and safety of complex engineering systems. This paper explores recent advances in data combination, analysis, and collection techniques that enhance the accuracy and efficiency of reliability evaluations. We investigate novel methods for integrating diverse data sources, including historical failure data, expert opinions, and sensor measurements, using Bayesian approaches and machine learning algorithms. Furthermore, we examine advanced statistical analysis techniques, such as copula-based modeling and stochastic simulation, to capture dependencies and uncertainties in system components. The study also addresses innovative data collection strategies, including active learning and adaptive testing, to minimize data acquisition costs while maximizing information gain. We demonstrate the effectiveness of these advances through case studies on power grids and manufacturing systems, highlighting their potential to improve reliability predictions, optimize maintenance schedules, and enhance system design. The results contribute to a more comprehensive and informed approach to system reliability assessment, leading to safer and more resilient engineering systems.\n"
  },
  {
    "id": 26,
    "prompt": "A celestial gamma-ray foreground due to the albedo of small solar system bodies and a remote probe of the interstellar cosmic ray spectrum",
    "HWT": "We calculate the gamma-ray albedo flux from cosmic-ray (CR) interactions with the solid rock and ice in Main Belt asteroids (MBAs), Jovian and Neptunian Trojan asteroids, and Kuiper Belt objects (KBOs) using the Moon as a template. We show that the gamma-ray albedo for the Main Belt, Trojans, and Kuiper Belt strongly depends on the small-body size distribution of each system. Based on an analysis of the Energetic Gamma Ray Experiment Telescope (EGRET) data we infer that the diffuse emission from the MBAs, Trojans, and KBOs has an integrated flux of less than ~6x10^{-6} cm^{-2} s^{-1} (100-500 MeV), which corresponds to ~12 times the Lunar albedo, and may be detectable by the forthcoming Gamma Ray Large Area Space Telescope (GLAST). If detected by GLAST, it can provide unique direct information about the number of small bodies in each system that is difficult to assess by any other method. Additionally, the KBO albedo flux can be used to probe the spectrum of CR nuclei at close-to-interstellar conditions. The orbits of MBAs, Trojans, and KBOs are distributed near the ecliptic, which passes through the Galactic center and high Galactic latitudes. Therefore, the asteroid gamma-ray albedo has to be taken into account when analyzing weak gamma-ray sources close to the ecliptic, especially near the Galactic center and for signals at high Galactic latitudes, such as the extragalactic gamma-ray emission. The asteroid albedo spectrum also exhibits a 511 keV line due to secondary positrons annihilating in the rock. This may be an important and previously unrecognized celestial foreground for the INTErnational Gamma-Ray Astrophysics Laboratory (INTEGRAL) observations of the Galactic 511 keV line emission including the direction of the Galactic center.",
    "MGT": "The interstellar cosmic ray (CR) spectrum, particularly at low energies, remains poorly constrained due to solar modulation effects. This work investigates a novel source of celestial gamma-ray emission arising from the interaction of interstellar CRs with small solar system bodies (SSSBs), such as asteroids and Kuiper Belt Objects (KBOs). These interactions generate a diffuse gamma-ray albedo, potentially detectable by current and future gamma-ray telescopes and offering a unique probe of the local interstellar CR spectrum. We present a detailed model of this gamma-ray foreground, considering the spatial distribution of SSSBs, their size-frequency distribution, and the physics of CR interactions with rocky and icy surfaces. The model incorporates updated asteroid and KBO population models derived from observational constraints and dynamical simulations. We calculate the expected gamma-ray flux and spectral characteristics of the SSSB albedo across the sky, focusing on the inner Solar System and the ecliptic plane, regions with the highest SSSB density. Our simulations utilize GEANT4 to accurately simulate CR interactions with representative SSSB compositions, accounting for hadronic cascades, electromagnetic showers, and subsequent gamma-ray production via processes like neutral pion decay, bremsstrahlung, and inverse Compton scattering. We explore the sensitivity of the predicted gamma-ray flux to variations in the low-energy interstellar CR spectrum, testing several widely used CR propagation models and examining the impact of uncertainties in the local interstellar radiation field. We find that the SSSB albedo generates a potentially significant gamma-ray foreground, particularly at energies below 1 GeV, with a distinct spatial morphology correlated with the distribution of SSSBs. The predicted flux is comparable to, or even exceeds, existing estimates of diffuse gamma-ray emission from other sources in the inner Solar System, such as solar flares and the Earth’s atmosphere. Finally, we discuss the prospects for detecting this signal with current gamma-ray telescopes, such as Fermi-LAT, and future instruments with improved sensitivity at low energies. We explore strategies for disentangling the SSSB albedo from other diffuse gamma-ray components and assessing the potential to constrain the local interstellar CR spectrum at energies inaccessible by direct measurements. This work highlights the importance of considering SSSBs as a significant contributor to the celestial gamma-ray background and offers a new avenue for probing the fundamental properties of cosmic rays in our local galactic environment.\n"
  },
  {
    "id": 27,
    "prompt": "Cosmological MHD simulation of a cooling flow cluster",
    "HWT": "Various observations of magnetic fields in the Intra-Cluster Medium (ICM), most of the time restricted to cluster cores, point towards field strength of the order of a few microG (synchrotron radiation from radio relics and radio halos, inverse Compton radiation in X-rays and Faraday rotation measure of polarised background sources). Both the origin and the spatial structure of galaxy clusters magnetic fields are still under debate. In particular, the radial profile of the magnetic field, from the core of clusters to their outskirts, is of great importance for cosmic rays propagation within the Cosmic Web. In this letter, we highlight the importance of cooling processes in amplifying the magnetic field in the core of galaxy clusters up to one order of magnitude above the typical amplification obtained for a pure adiabatic evolution. We have performed a \"zoom'' cosmological simulation of a 3 keV cluster, including dark matter and gas dynamics, atomic cooling, UV heating and star formation using the newly developed MHD solver in the AMR code RAMSES. Magnetic field amplification proceeds mainly through gravitational contraction. Shearing motions due to turbulence provide additional amplification in the outskirts of the cluster, while magnetic reconnection during mergers causes magnetic field dissipation in the core. Cooling processes have a strong impact on the magnetic field structure in the cluster. First, due to the sharp rise of the gas density in the centre, gravitational amplification is significantly amplified, when compared to the non--radiative run. Second, due to cooling processes, shearing motions are much stronger in the core than in the adiabatic case, leading to additional field amplification and no significant magnetic reconnection.",
    "MGT": "Galaxy clusters, the largest gravitationally bound structures in the universe, are filled with hot, X-ray emitting plasma. Observations reveal that the central regions of many clusters exhibit a cooling flow, where the radiative cooling time is shorter than the Hubble time. This should lead to significant cooling and star formation, but observations show that star formation rates are much lower than predicted. This discrepancy, known as the \"cooling flow problem,\" suggests the presence of a heating mechanism that counteracts radiative cooling. Active Galactic Nuclei (AGN) feedback, powered by supermassive black holes at the cluster center, is the leading candidate for this heating source.\n\nWe present results from a cosmological magneto-hydrodynamic (MHD) simulation of a massive galaxy cluster, focusing on the interplay between radiative cooling, AGN feedback, and magnetic fields. The simulation was performed using the Enzo code, incorporating adaptive mesh refinement to achieve high resolution in the cluster core. Our simulation includes a detailed model for AGN feedback, where energy and momentum are injected into the surrounding gas via jets launched from the central black hole. The model also incorporates a realistic treatment of radiative cooling, star formation, and supernova feedback.\n\nWe find that AGN feedback is effective at suppressing the cooling flow, preventing the catastrophic cooling and star formation predicted by purely radiative models. The AGN jets inflate bubbles of hot, low-density plasma that buoyantly rise through the cluster atmosphere, displacing the cooler, denser gas. Magnetic fields play a crucial role in shaping the AGN outflow and mediating the interaction between the jet and the surrounding gas. The magnetic fields also suppress thermal conduction and regulate the mixing of hot and cold gas, influencing the stability of the cooling flow. Our results demonstrate that the combination of AGN feedback and magnetic fields can self-consistently regulate the cooling flow and maintain the observed properties of galaxy clusters. The simulation highlights the complex interplay between various physical processes in shaping the evolution of galaxy clusters.\n"
  },
  {
    "id": 28,
    "prompt": "Probing the dusty environment of the Seyfert 1 nucleus in NGC 3783 with MIDI/VLTI interferometry",
    "HWT": "We present mid-IR spectro-interferometry of the Seyfert type 1 nucleus of NGC 3783. The dusty circumnuclear environment is spatially resolved and the wavelength dependence of the compact emission is discussed. The observations were carried out with the MIDI instrument at the Very Large Telescope Interferometer in the N-band. Spectra and visibilities were derived with a spectral resolution of 30 in the wavelength range from 8 to 13 micron. For the interpretation we developed a simple dusty disk model with small and variable covering factor. At baselines of 65 and 69 m, visibilities in the range of 0.4 to 0.7 were measured. The N-band spectra show a monotonic increase of the measured flux with wavelength with no apparent silicate feature around 10 micron. We find that the mid-IR emission from the nucleus can be reproduced by an extended dust disk or torus with a small covering factor of the radiating dust clouds. Our mid-IR observations of NGC 3783 are consistent with a clumpy circumnuclear dust environment. The interpretation in terms of a dusty torus with low covering factor supports a clumpy version of the unified scheme for AGN. The inferred sizes and luminosities are in good agreement with dust reverberation sizes and bolometric luminosities from optical and X-ray observations.",
    "MGT": "We present a study of the inner dusty environment of the Seyfert 1 galaxy NGC 3783 using mid-infrared (MIR) interferometric observations obtained with the MIDI instrument at the Very Large Telescope Interferometer (VLTI). Our observations spatially resolve the MIR emission at 8-13 μm, allowing us to probe the geometry and temperature distribution of the circumnuclear dust. We find evidence for a compact, elongated structure with a projected size of approximately 1.5 pc along the major axis and a position angle consistent with the orientation of the galaxy's radio jet. The visibility data are best fit by a geometrical model consisting of a Gaussian component and an extended disk component. The Gaussian component, representing the hottest dust closest to the active galactic nucleus (AGN), has a FWHM of 0.7 pc. The extended disk component, contributing a significant fraction of the total flux, suggests a cooler, more diffuse dust distribution at larger distances. The inferred temperature of the hot dust component is approximately 400 K, consistent with dust sublimation models. Our results support a scenario where the MIR emission originates from a clumpy, torus-like structure composed of dust clouds that are heated by the AGN. The spatial resolution achieved with VLTI/MIDI allows us to constrain the size and geometry of the inner regions of this dusty torus, providing valuable insights into the AGN unification scheme and the role of dust in obscuring and reprocessing the AGN emission. Further analysis reveals potential evidence for deviations from axial symmetry, possibly related to the influence of the radio jet or asymmetries in the accretion disk.\n"
  },
  {
    "id": 29,
    "prompt": "On a reduction procedure for Horn inequalities in finite von Neumann algebras",
    "HWT": "We consider the analogues of the Horn inequalities in finite von Neumann algebras, which concern the possible spectral distributions of sums $a+b$ of self--adjoint elements $a$ and $b$ in a finite von Neumann algebra. It is an open question whether all of these Horn inequalities must hold in all finite von Neumann algebras, and this is related to Connes' embedding problem. For each choice of integers $1\\le r\\le n$, there is a set $T^n_r$ of Horn triples, and the Horn inequalities are in one-to-one correspondence with $\\cup_{1\\le r\\le n}T^n_r$. We consider a property P$_n$, analogous to one introduced by Therianos and Thompson in the case of matrices, amounting to the existence of projections having certain properties relative to arbitrary flags, which guarantees that a given Horn inequality holds in all finite von Neumann algebras. It is an open question whether all Horn triples in $T^n_r$ have property P$_n$. Certain triples in $T^n_r$ can be reduced to triples in $T^{n-1}_r$ by an operation we call {\\em TT--reduction}. We show that property P$_n$ holds for the original triple if property P$_{n-1}$ holds for the reduced one. We then characterize the TT--irreducible Horn triples in $T^n_3$, for arbitrary $n$, and for those LR--minimal ones (namely, those having Littlewood--Richardson coefficient equal to 1), we perform a construction of projections with respect to flags in arbitrary von Neumann algebras in order to prove property P$_n$ for them. This shows that all LR--minimal triples in $\\cup_{n\\ge3}T^n_3$ have property P$_n$, and so that the corresponding Horn inequalities hold in all finite von Neumann algebras.",
    "MGT": "This paper introduces a novel reduction procedure for establishing Horn inequalities in the context of finite von Neumann algebras. Horn inequalities, originally formulated for eigenvalues of sums of Hermitian matrices, have found significant extensions in various operator algebraic settings, including tracial inequalities for sums of self-adjoint operators in finite von Neumann algebras. The core challenge in verifying these inequalities lies in the combinatorial complexity associated with identifying relevant triples of partitions satisfying specific dominance relations. Our reduction procedure aims to simplify this complexity by systematically reducing the problem to a smaller set of \"irreducible\" or \"atomic\" inequalities.\n\nThe procedure leverages the interplay between representation theory of symmetric groups and the structure of partitions. We demonstrate that any Horn inequality can be decomposed into a linear combination of simpler inequalities, effectively reducing the verification problem to checking these fundamental building blocks. This decomposition relies on a careful analysis of the Littlewood-Richardson coefficients and their connection to the multiplicities of irreducible representations in tensor products. By identifying redundant or linearly dependent inequalities, we construct a minimal set of generating inequalities that are sufficient to verify the entire family of Horn inequalities.\n\nFurthermore, we establish a link between our reduction procedure and the theory of Schubert calculus on Grassmannians. This connection provides a geometric interpretation of the reduction process, revealing that the simplification of Horn inequalities corresponds to geometric operations on Schubert varieties. This geometric perspective allows us to utilize tools from algebraic geometry to further refine the reduction procedure and identify potential simplifications.\n\nThe effectiveness of our reduction procedure is illustrated through several concrete examples. We explicitly compute the reduced set of inequalities for specific classes of partitions, demonstrating a significant reduction in the number of inequalities that need to be verified. These examples include cases relevant to the study of operator algebras and quantum information theory. The results presented in this paper contribute to a deeper understanding of the structure of Horn inequalities in finite von Neumann algebras and provide a powerful tool for verifying these inequalities in practice. The reduction procedure not only simplifies the verification process but also sheds light on the underlying combinatorial and geometric structure of these inequalities, opening avenues for further research in this area.\n"
  },
  {
    "id": 30,
    "prompt": "Thermonuclear Reflect AB-Reactor",
    "HWT": "The author offers a new kind of thermonuclear reflect reactor. The remarkable feature of this new reactor is a three net AB reflector, which confines the high temperature plasma. The plasma loses part of its energy when it contacts with the net but this loss can be compensated by an additional permanent plasma heating. When the plasma is rarefied (has a small density), the heat flow to the AB reflector is not large and the temperature in the triple reflector net is lower than 2000 - 3000 K. This offered AB-reactor has significantly less power then the currently contemplated power reactors with magnetic or inertial confinement (hundreds-thousands of kW, not millions of kW). But it is enough for many vehicles and ships and particularly valuable for tunnelers, subs and space apparatus, where air to burn chemical fuel is at a premium or simply not available. The author has made a number of innovations in this reactor, researched its theory, developed methods of computation, made a sample computation of typical project. The main point of preference for the offered reactor is its likely cheapness as a power source. Key words: Micro-thermonuclear reactor, Multi-reflex AB-thermonuclear reactor, Self-magnetic AB-thermonuclear reactor, aerospace thermonuclear engine.",
    "MGT": "The Thermonuclear Reflect AB-Reactor (TRAB-Reactor) presents a novel approach to controlled thermonuclear fusion, utilizing a unique magneto-inertial confinement scheme combined with advanced fuel composition. This paper details the theoretical framework, design considerations, and preliminary simulation results for the TRAB-Reactor, which leverages a high-density deuterium-tritium (DT) fuel mixture augmented with trace amounts of advanced fuels, specifically Astatine-211 (²¹¹At) and Boron-11 (¹¹B). The addition of these elements aims to enhance energy amplification through secondary reactions and improve overall plasma stability.\n\nThe core concept involves the pulsed compression of the fuel target within a reflective magnetic nozzle configuration. This nozzle is engineered to minimize particle losses and maximize energy confinement, effectively reflecting charged particles back into the reaction zone. The magnetic field is dynamically tailored to achieve optimal plasma parameters, including ion temperature, density, and confinement time, necessary for sustained thermonuclear burn.\n\nComputational modeling, using a multi-physics code coupling magneto-hydrodynamics with particle transport, demonstrates the potential for achieving significant energy gain. Simulation results indicate that the inclusion of ²¹¹At leads to enhanced alpha particle heating, while ¹¹B contributes to neutron-lean fusion pathways, reducing radioactive waste production. Parametric studies explore the sensitivity of reactor performance to key variables such as initial fuel density, magnetic field strength, and pulse shaping. Challenges associated with achieving stable plasma confinement and mitigating radiative losses are also addressed. The TRAB-Reactor concept offers a promising avenue for compact and efficient fusion energy production, paving the way for future experimental validation.\n"
  },
  {
    "id": 31,
    "prompt": "Vacuum static compactified wormholes in eight-dimensional Lovelock theory",
    "HWT": "In this paper new exact solutions in eight dimensional Lovelock theory will be presented. These solutions are vacuum static wormhole, black hole and generalized Bertotti-Robinson space-times with nontrivial torsion. All the solutions have a cross product structure of the type $M_{5}\\times \\Sigma_{3} $ where $M_{5}$ is a five dimensional manifold and $\\Sigma_{3}$ a compact constant curvature manifold. The wormhole is the first example of a smooth vacuum static Lovelock wormhole which is neither Chern-Simons nor Born-Infeld. It will be also discussed how the presence of torsion affects the \"navigableness\" of the wormhole for scalar and spinning particles. It will be shown that the wormhole with torsion may act as \"geometrical filter\": a very large torsion may \"increase the traversability\" for scalars while acting as a \"polarizator\" on spinning particles. This may have interesting phenomenological consequences.",
    "MGT": "We investigate the existence of static, spherically symmetric wormhole solutions in eight-dimensional Lovelock gravity, focusing on vacuum configurations. The Lovelock Lagrangian, a natural generalization of Einstein gravity to higher dimensions, includes higher-order curvature terms that can support traversable wormholes without exotic matter. We present a detailed analysis of the field equations, derived from the Lovelock action, and explore the conditions necessary for the existence of wormhole geometries. We focus on solutions where the Lovelock coefficients are chosen such that the effective energy-momentum tensor vanishes, leading to vacuum wormholes. Through a combination of analytical and numerical methods, we demonstrate the existence of such solutions, characterized by a non-trivial radial profile of the metric components. These solutions exhibit a throat connecting two asymptotically flat regions. Furthermore, we investigate the properties of these wormholes, including their size and the behavior of tidal forces. The stability of these vacuum wormhole solutions is also discussed. Our findings provide insights into the possibility of traversable wormholes within the framework of higher-dimensional gravity theories, offering a potential avenue for exploring exotic spacetime geometries.\n"
  },
  {
    "id": 32,
    "prompt": "Arithmetic of N=8 Black Holes",
    "HWT": "The microscopic formula for the degeneracies of 1/8 BPS black holes in type II string theory compactified on a six dimensional torus can be expressed as a sum of several terms. One of the terms is a function of the Cremmer-Julia invariant and gives the leading contribution to the entropy in the large charge limit. The other terms, which give exponentially subleading contribution, depend not only on the Cremmer-Julia invariant, but also on the arithmetic properties of the charges, and in fact exist only when the charges satisfy special arithmetic properties. We identify the origin of these terms in the macroscopic formula for the black hole entropy, based on quantum entropy function, as the contribution from non-trivial saddle point(s) in the path integral of string theory over the near horizon geometry. These saddle points exist only when the charge vectors satisfy the arithmetic properties required for the corresponding term in the microscopic formula to exist. Furthermore the leading contribution from these saddle points in the large charge limit agrees with the leading asymptotic behaviour of the corresponding term in the degeneracy formula.",
    "MGT": "The arithmetic properties of N=8 supersymmetric black holes are explored, focusing on the interplay between their entropy, charges, and underlying U-duality symmetries. These black holes, arising as solutions to supergravity theories in diverse dimensions, exhibit intriguing connections to number theory, particularly through the representation theory of exceptional groups. We examine the quantization conditions imposed on the black hole charges and their impact on the entropy formula, which is classically a continuous function of the charges but becomes quantized in the quantum theory. The entropy, computed using both macroscopic and microscopic methods, is investigated in relation to automorphic forms and modularity. Specifically, we delve into the role of quartic invariants associated with the E7(7) U-duality group in four dimensions, demonstrating how these invariants govern the black hole entropy and its corrections. The analysis extends to higher-derivative corrections to the effective action and their influence on the entropy function. Furthermore, we discuss the connections between the black hole charge lattice and the underlying string theory compactification, highlighting the role of elliptic genera and topological invariants in characterizing the microstates. The study emphasizes the rich mathematical structure inherent in these black hole solutions, providing insights into the quantum nature of gravity and its relation to fundamental physics.\n"
  },
  {
    "id": 33,
    "prompt": "Magnetic moments of $^{33}$Mg in time-odd relativistic mean field approach",
    "HWT": "The configuration-fixed deformation constrained relativistic mean field approach with time-odd component has been applied to investigate the ground-state properties of $^{33}$Mg with effective interaction PK1. The ground state of $^{33}$Mg has been found to be prolate deformed, $\\beta_2=0.23$, with the odd neutron in $1/2[330]$ orbital and the energy -251.85 MeV which is close to the data -252.06 MeV. The magnetic moment $- 0.9134 \\mu_\\mathrm{N}$ is obtained with the effective electromagnetic current which well reproduces the data $- 0.7456 \\mu_\\mathrm{N}$ self-consistently without introducing any parameter. The energy splittings of time reversal conjugate states, the neutron current, the energy contribution from the nuclear magnetic potential, and the effect of core polarization are discussed in detail.",
    "MGT": "The magnetic moment of the exotic nucleus $^{33}$Mg is investigated within the framework of the time-odd relativistic mean-field (TORMF) approach. This study employs the density-dependent point-coupling (DD-PC1) functional to describe the effective nucleon-nucleon interaction. The inclusion of time-odd mean fields, crucial for accurately describing rotating and deformed nuclei, is found to significantly impact the calculated magnetic moment. Specifically, the contribution of the time-odd meson fields modifies the single-particle wave functions and occupation probabilities near the Fermi surface, leading to a substantial quenching of the magnetic moment. Our results, obtained without any adjustable parameters, are compared with available experimental data and previous theoretical calculations based on non-relativistic models. The TORMF approach provides a robust description of the nuclear structure of $^{33}$Mg, highlighting the importance of relativistic effects and the self-consistent treatment of time-odd fields in understanding the magnetic properties of nuclei far from stability. This work demonstrates the predictive power of relativistic mean-field theory for exploring the structure of exotic nuclei.\n"
  },
  {
    "id": 34,
    "prompt": "Counting p'-characters in finite reductive groups",
    "HWT": "This article is concerned with the relative McKay conjecture for finite reductive groups. Let G be a connected reductive group defined over the finite field F_q of characteristic p>0 with corresponding Frobenius map F. We prove that if the F-coinvariants of the component group of the center of G has prime order and if p is a good prime for G, then the relative McKay conjecture holds for G at the prime p. In particular, this conjecture is true for G^F in defining characteristic for G a simple and simply-connected group of type B_n, C_n, E_6 and E_7. Our main tools are the theory of Gelfand-Graev characters for connected reductive groups with disconnected center developed by Digne-Lehrer-Michel and the theory of cuspidal Levi subgroups. We also explicitly compute the number of semisimple classes of G^F for any simple algebraic group G.",
    "MGT": "Let $G$ be a finite reductive group with connected center and $p$ a prime not dividing the order of the Weyl group of $G$. We investigate the number of irreducible complex characters of $G$ whose degree is not divisible by a fixed prime $p'$. We derive a formula expressing this number in terms of Lusztig's almost characters, leading to a character-theoretic interpretation of certain geometric quantities associated with $G$. Specifically, we show that the number of irreducible characters of $G$ of $p'$-degree can be expressed as a sum involving the values of almost characters at unipotent elements. Furthermore, we provide explicit bounds for the number of such characters, demonstrating that it grows polynomially with the order of the underlying finite field. Our results extend previous work on the number of characters of $p$-defect zero and offer new insights into the representation theory of finite reductive groups in non-describing characteristic. We provide examples illustrating our results for specific groups and primes.\n"
  },
  {
    "id": 35,
    "prompt": "Semiclassical dynamics of quasi-one-dimensional, attractive Bose-Einstein condensates",
    "HWT": "The strongly interacting regime for attractive Bose-Einstein condensates (BECs) tightly confined in an extended cylindrical trap is studied. For appropriately prepared, non-collapsing BECs, the ensuing dynamics are found to be governed by the one-dimensional focusing Nonlinear Schr\\\"odinger equation (NLS) in the semiclassical (small dispersion) regime. In spite of the modulational instability of this regime, some mathematically rigorous results on the strong asymptotics of the semiclassical limiting solutions were obtained recently. Using these results, \"implosion-like\" and \"explosion-like\" events are predicted whereby an initial hump focuses into a sharp spike which then expands into rapid oscillations. Seemingly related behavior has been observed in three-dimensional experiments and models, where a BEC with a sufficient number of atoms undergoes collapse. The dynamical regimes studied here, however, are not predicted to undergo collapse. Instead, distinct, ordered structures, appearing after the \"implosion\", yield interesting new observables that may be experimentally accessible.",
    "MGT": "We investigate the semiclassical dynamics of quasi-one-dimensional Bose-Einstein condensates (BECs) with attractive interactions. Our analysis focuses on regimes where the number of atoms is sufficiently large, allowing for a mean-field description based on the Gross-Pitaevskii equation (GPE). We derive effective equations of motion for collective variables characterizing the BEC's shape and position, employing a variational ansatz for the condensate wavefunction. These equations capture the essential dynamics of the system, including breathing modes and center-of-mass oscillations. We explore the stability properties of the BEC, identifying critical conditions for collapse based on the interplay between attractive interactions and kinetic energy. Furthermore, we analyze the effects of external potentials, such as harmonic traps, on the condensate's dynamics. Our results reveal the emergence of nonlinear phenomena, including self-trapping and soliton-like behavior. We compare our semiclassical predictions with numerical simulations of the GPE, demonstrating good agreement in various parameter regimes. This work provides insights into the complex dynamics of attractive BECs and offers a valuable tool for understanding and predicting their behavior in experiments. The study also highlights the limitations of the semiclassical approach and points towards future directions for research, including the incorporation of quantum fluctuations.\n"
  },
  {
    "id": 36,
    "prompt": "How to prevent type-flaw attacks on security protocols under algebraic properties",
    "HWT": "Type-flaw attacks upon security protocols wherein agents are led to misinterpret message types have been reported frequently in the literature. Preventing them is crucial for protocol security and verification. Heather et al. proved that tagging every message field with it's type prevents all type-flaw attacks under a free message algebra and perfect encryption system. In this paper, we prove that type-flaw attacks can be prevented with the same technique even under the ACUN algebraic properties of XOR which is commonly used in \"real-world\" protocols such as SSL 3.0. Our proof method is general and can be easily extended to other monoidal operators that possess properties such as Inverse and Idempotence as well. We also discuss how tagging could be used to prevent type-flaw attacks under other properties such as associativity of pairing, commutative encryption, prefix property and homomorphic encryption.",
    "MGT": "Type-flaw attacks exploit vulnerabilities in security protocols arising from the mishandling of data types, particularly when algebraic properties are involved. This paper investigates methods to prevent such attacks by focusing on formal verification techniques that incorporate algebraic reasoning. We present a novel approach that combines type theory with equational logic to rigorously model and analyze protocols susceptible to type confusion. Our methodology involves defining a type system enriched with algebraic properties and developing a verification algorithm that checks for type consistency and algebraic validity throughout protocol execution. We demonstrate the effectiveness of our approach by applying it to several well-known security protocols exhibiting algebraic properties, such as Diffie-Hellman and elliptic curve-based protocols. The results show that our technique can effectively identify and prevent type-flaw attacks that traditional type systems might miss. Furthermore, we discuss the limitations of our approach and outline potential directions for future research, including extending the type system to handle more complex algebraic structures and developing automated tools for protocol verification.\n"
  },
  {
    "id": 37,
    "prompt": "A dynamical classification of the range of pair interactions",
    "HWT": "We formalize a classification of pair interactions based on the convergence properties of the {\\it forces} acting on particles as a function of system size. We do so by considering the behavior of the probability distribution function (PDF) P(F) of the force field F in a particle distribution in the limit that the size of the system is taken to infinity at constant particle density, i.e., in the \"usual\" thermodynamic limit. For a pair interaction potential V(r) with V(r) \\rightarrow \\infty) \\sim 1/r^a defining a {\\it bounded} pair force, we show that P(F) converges continuously to a well-defined and rapidly decreasing PDF if and only if the {\\it pair force} is absolutely integrable, i.e., for a > d-1, where d is the spatial dimension. We refer to this case as {\\it dynamically short-range}, because the dominant contribution to the force on a typical particle in this limit arises from particles in a finite neighborhood around it. For the {\\it dynamically long-range} case, i.e., a \\leq d-1, on the other hand, the dominant contribution to the force comes from the mean field due to the bulk, which becomes undefined in this limit. We discuss also how, for a \\leq d-1 (and notably, for the case of gravity, a=d-2) P(F) may, in some cases, be defined in a weaker sense. This involves a regularization of the force summation which is generalization of the procedure employed to define gravitational forces in an infinite static homogeneous universe. We explain that the relevant classification in this context is, however, that which divides pair forces with a > d-2 (or a < d-2), for which the PDF of the {\\it difference in forces} is defined (or not defined) in the infinite system limit, without any regularization. In the former case dynamics can, as for the (marginal) case of gravity, be defined consistently in an infinite uniform system.",
    "MGT": "Understanding the behavior of many-body systems relies heavily on the nature of the interactions between constituent particles. This article presents a novel dynamical classification of pair interactions based on the long-time behavior of the relative motion of two interacting particles. We move beyond the traditional classification based solely on the potential energy function itself, and instead focus on the observable trajectories and asymptotic properties of the system. Our classification scheme identifies distinct dynamical regimes characterized by qualitatively different types of motion, such as scattering, capture, orbiting, and complex oscillatory behavior.\n\nWe begin by establishing a theoretical framework based on Hamiltonian mechanics, deriving equations of motion for the relative coordinate and conjugate momentum. We then perform a comprehensive numerical exploration of a wide range of potential energy functions, encompassing both attractive and repulsive interactions, as well as potentials with varying degrees of smoothness and long-range behavior. The numerical simulations are designed to systematically map out the parameter space of initial conditions and interaction parameters, allowing us to identify the boundaries between different dynamical regimes.\n\nThe key to our classification lies in analyzing the asymptotic behavior of the relative coordinate. We introduce several quantitative measures, such as the scattering angle, the capture cross-section, and the oscillation frequency, to characterize the different types of motion. These measures are then used to construct a phase diagram that delineates the boundaries between the different dynamical regimes. We find that the topology of this phase diagram is highly sensitive to the shape of the potential energy function, revealing subtle but important distinctions between seemingly similar interactions.\n\nFurthermore, we demonstrate the applicability of our classification scheme to a variety of physical systems, including atomic collisions, gravitational interactions, and plasma physics. By mapping the interaction potentials in these systems onto our dynamical classification, we gain new insights into the underlying mechanisms that govern their behavior. Our findings suggest that the dynamical classification provides a more complete and nuanced understanding of pair interactions than traditional potential-based classifications, paving the way for more accurate and efficient modeling of many-body systems. We conclude by discussing the limitations of our approach and outlining future directions for research, including the extension of our classification scheme to include three-body and many-body interactions, as well as the development of analytical tools for predicting the dynamical behavior of complex systems.\n"
  },
  {
    "id": 38,
    "prompt": "Heavy water around the L1448-mm protostar",
    "HWT": "Context: L1448-mm is the prototype of a low-mass Class 0 protostar driving a high-velocity jet. Given its bright H2O spectra observed with ISO, L1448-mm is an ideal laboratory to observe heavy water (HDO) emission. Aims: Our aim is to image the HDO emission in the protostar surroundings, the possible occurrence of HDO emission also investigating off L1448-mm, towards the molecular outflow. Methods: We carried out observations of L1448-mm in the HDO(1_10-1_11) line at 80.6 GHz, an excellent tracer of HDO column density, with the IRAM Plateau de Bure Interferometer. Results: We image for the first time HDO emission around L1448-mm. The HDO structure reveals a main clump at velocities close to the ambient one towards the the continuum peak that is caused by the dust heated by the protostar. In addition, the HDO map shows tentative weaker emission at about 2000 AU from the protostar towards the south, which is possibly associated with the walls of the outflow cavity opened by the protostellar wind. Conclusions: Using an LVG code, modelling the density and temperature profile of the hot-corino, and adopting a gas temperature of 100 K and a density of 1.5 10^8 cm^-3, we derive a beam diluted HDO column density of about 7 10^13 cm^-2, corresponding to a HDO abundance of about 4 10^-7. In addition, the present map supports the scenario where HDO can be efficiently produced in shocked regions and not uniquely in hot corinos heated by the newly born star.",
    "MGT": "The protostellar system L1448-mm provides a crucial laboratory for understanding the initial stages of star formation and the chemical processes occurring in cold, dense environments. Water, a fundamental molecule for life as we know it, plays a significant role in these processes, acting as a coolant, a reactant, and a tracer of physical conditions. This study presents new high-resolution observations of heavy water (HDO) emission towards L1448-mm, obtained with the Atacama Large Millimeter/submillimeter Array (ALMA). We detect HDO emission associated with the protostellar envelope and outflow, revealing the spatial distribution and kinematics of deuterated water in this system.\n\nOur analysis focuses on the HDO 3(1,2)-2(2,1) transition at 225.897 GHz, tracing relatively warm gas within the inner regions of the envelope. By comparing the observed HDO emission with radiative transfer models, we constrain the abundance of HDO relative to H2O, finding a significantly enhanced D/H ratio compared to the local interstellar medium. This enhancement suggests efficient deuteration pathways occurring on grain surfaces at low temperatures. Furthermore, we observe distinct velocity components in the HDO spectra, indicating the presence of both infalling material and outflowing gas. The spatial distribution of these components provides insights into the dynamics of the protostellar system and the role of the outflow in transporting deuterated species.\n\nThe high D/H ratio in water observed towards L1448-mm has important implications for the origin of water on planets. The enhanced deuteration suggests that protostellar envelopes can be a significant source of heavy water, which could be incorporated into planet-forming disks and ultimately delivered to planetary surfaces. Our findings contribute to a better understanding of the chemical evolution of water during the early stages of star formation and its potential role in the emergence of habitable environments. Future observations of other deuterated water isotopologues and chemical modeling are needed to further refine our understanding of the deuteration pathways and the physical conditions in the L1448-mm protostellar system.\n"
  },
  {
    "id": 39,
    "prompt": "Towards truly simultaneous PIXE and RBS analysis of layered objects in cultural heritage",
    "HWT": "For a long time, RBS and PIXE techniques have been used in the field of cultural heritage. Although the complementarity of both techniques has long been acknowledged, its full potential has not been yet developed due to the lack of general purpose software tools for analysing the data from both techniques in a coherent way. In this work we provide an example of how the recent addition of PIXE to the set of techniques supported by the DataFurnace code can significantly change this situation. We present a case in which a non homogeneous sample (an oxidized metal from a photographic plate -heliography- made by Niepce in 1827) is analysed using RBS and PIXE in a straightforward and powerful way that can only be performed with a code that treats both techniques simultaneously as a part of one single and coherent analysis. The optimization capabilities of DataFurnace, allowed us to obtain the composition profiles for these samples in a very simple way.",
    "MGT": "Particle Induced X-ray Emission (PIXE) and Rutherford Backscattering Spectrometry (RBS) are powerful ion beam analysis (IBA) techniques widely used in cultural heritage research for non-destructive elemental analysis and depth profiling of layered objects. While traditionally performed sequentially, a truly simultaneous approach offers significant advantages in terms of beam time efficiency, reduced beam-induced damage, and improved data consistency. This article explores the development and optimization of a simultaneous PIXE/RBS setup for the characterization of layered cultural heritage materials. The experimental setup, utilizing a silicon drift detector (SDD) for PIXE and a silicon surface barrier detector (SSBD) for RBS, is described in detail. We present a comprehensive methodology for spectral data processing, including background subtraction, peak fitting, and quantitative analysis for both techniques. The accuracy and reliability of the simultaneous PIXE/RBS approach are validated through the analysis of well-characterized standard reference materials. Furthermore, we demonstrate the application of this method to the study of a historical glass fragment, revealing its elemental composition and depth profile, providing valuable insights into its manufacturing technology and degradation processes. The results highlight the potential of simultaneous PIXE/RBS as a valuable tool for the comprehensive and efficient characterization of layered objects in cultural heritage.\n"
  },
  {
    "id": 40,
    "prompt": "Dynamics of the entanglement between two oscillators in the same environment",
    "HWT": "We provide a complete characterization of the evolution of entanglement between two oscillators coupled to a common environment. For initial Gaussian states we identify three phases with different qualitative long time behavior: There is a phase where entanglement undergoes a sudden death (SD). Another phase (SDR) is characterized by an infinite sequence of events of sudden death and revival of entanglement. In the third phase (NSD) there is no sudden death of entanglement, which persist for long time. The phase diagram is described and analytic expressions for the boundary between phases are obtained. Numerical simulations show the accuracy of the analytic expressions. These results are applicable to a large variety of non--Markovian environments. The case of non--resonant oscillators is also numerically investigated.",
    "MGT": "This paper investigates the dynamics of entanglement between two coupled harmonic oscillators interacting with a common environment. We model the environment as a bath of independent harmonic oscillators and employ the Born-Markov approximation to derive a master equation governing the system's evolution.  The influence of various parameters, including the coupling strength between the oscillators, the temperature of the environment, and the spectral density of the bath, on the entanglement dynamics is analyzed. We quantify entanglement using the logarithmic negativity, exploring its behavior under different conditions.  Our results demonstrate that entanglement can be generated and sustained, albeit transiently, even in a high-temperature environment.  The coupling strength between the oscillators plays a crucial role in preserving entanglement against decoherence induced by the environment.  Furthermore, we observe that a structured spectral density of the bath can lead to non-Markovian effects, potentially enhancing entanglement robustness. The findings offer insights into controlling and manipulating entanglement in open quantum systems, relevant to quantum information processing and quantum metrology.\n"
  },
  {
    "id": 41,
    "prompt": "Lie's Reduction Method and Differential Galois Theory in the Complex Analytic Context",
    "HWT": "This paper is dedicated to the differential Galois theory in the complex analytic context for Lie-Vessiot systems. Those are the natural generaliza- tion of linear systems, and the more general class of differential equations adimitting superposition laws, as recently stated in [5]. A Lie-Vessiot sys- tem is automatically translated into a equation in a Lie group that we call automorphic system. Reciprocally an automorphic system induces a hierarchy of Lie-Vessiot systems. In this work we study the global analytic aspects of a classical method of reduction of differential equations, due to S. Lie. We propose an differential Galois theory for automorphic systems, and explore the relationship between integrability in terms of Galois the- ory and the Lie's reduction method. Finally we explore the algebra of Lie symmetries of a general automorphic system.",
    "MGT": "This article explores the intersection of Lie's reduction method and differential Galois theory within the complex analytic setting. We extend the classical Lie's reduction method for ordinary differential equations (ODEs) to encompass complex analytic ODEs, focusing on leveraging Lie symmetries to simplify and solve such equations. We then investigate the differential Galois group associated with these reduced equations, examining how the symmetry reduction process affects the structure and properties of the Galois group. Specifically, we demonstrate how the presence of Lie symmetries leads to a reduction in the complexity of the Galois group, potentially enabling the determination of closed-form solutions. Furthermore, we analyze the relationship between the Lie algebra of symmetries and the algebraic structure of the differential Galois group. Through concrete examples of complex analytic ODEs admitting Lie symmetries, we illustrate the practical application of our findings, showcasing how the combination of Lie's reduction method and differential Galois theory provides a powerful approach for analyzing and solving complex analytic ODEs.\n"
  },
  {
    "id": 42,
    "prompt": "Generating technique for $U(1)^3 5D$ supergravity",
    "HWT": "We develop generating technique for solutions of $U(1)^3 5D$ supergravity via dimensional reduction to three dimensions. This theory, which recently attracted attention in connection with black rings, can be viewed as consistent truncation of the $T^6$ compactification of the eleven-dimensional supergravity. Its further reduction to three dimensions accompanied by dualisation of the vector fields leads to 3D gravity coupled sigma model on the homogeneous space $SO(4,4)/SO(4)\\times SO(4)$ or $SO(4,4)/SO(2,2)\\times SO(2,2)$ depending on the signature of the three-space. We construct a $8\\times 8$ matrix representation of these cosets in terms of lower-dimensional blocks. Using it we express solution generating transformations in terms of the potentials and identify those preserving asymptotic conditions relevant to black holes and black rings. As an application, we derive the doubly rotating black hole solution with three independent charges. A suitable contraction of the above cosets is used to construct a new representation of the coset $G_{2(2)}/(SL(2, R)\\times SL(2, R))$ relevant for minimal five-dimensional supergravity.",
    "MGT": "We present a novel generating technique for constructing solutions of five-dimensional (5D) supergravity with three $U(1)$ gauge fields, focusing on asymptotically flat, dyonic black objects. Our method leverages the underlying pseudo-harmonic formalism and extends existing solution-generating techniques applicable to simpler 5D supergravity theories. Specifically, we employ a series of solution-generating transformations based on spectral flows and Harrison transformations, adapted to the more complex structure of the three-vector supergravity. These transformations act on a seed solution, systematically introducing electric and magnetic charges associated with the three $U(1)$ gauge fields, while preserving the supersymmetry. The technique is algorithmic and relies on solving a set of linear equations, making it amenable to computer algebra systems. We demonstrate the efficacy of our approach by constructing new, explicit solutions representing dyonic black holes and black rings carrying three independent electric and magnetic charges. These solutions exhibit non-trivial interactions between the different gauge fields and provide insights into the landscape of supersymmetric solutions in 5D supergravity. We analyze the physical properties of these solutions, including their mass, angular momentum, and charges, and verify that they satisfy the expected BPS bounds. Furthermore, we discuss the potential for extending this technique to generate even more complex solutions with different topologies and charge configurations, as well as its implications for understanding the microscopic entropy of black holes in string theory.\n"
  },
  {
    "id": 43,
    "prompt": "Detailed Spectral Analysis of the Type Ib Supernova 1999dn. Paper I: Hydrogen-free Models",
    "HWT": "We present spectral fits to five epochs of the typical Type Ib supernova 1999dn using the generalized, non-LTE, stellar atmospheres code PHOENIX. Our goal is threefold: to determine basic physical properties of the supernova ejecta, such as velocity, temperature, and density gradients; to reproduce He I absorption lines by invoking non-thermal excitation; and, to investigate possible spectral signatures of hydrogen, especially a feature around 6200 Angstrom, which has been attributed to high velocity $H_\\alpha$. Our models assume an atmosphere with uniform composition devoid of any hydrogen. Our model spectra fit the observed spectra well, successfully reproducing most of the features, including the prominent He I absorptions. The most plausible alternative to $H_\\alpha$ as the source of the 6200 Angstrom feature is a blend of Fe II and Si II lines, which can be made stronger to fit the observed feature better by increasing the metallicity of the ejecta. High-metallicity models fit well at early epochs, but not as well as solar-metallicity models after maximum light. While this blend of metal lines is a reasonable explanation of the source of the 6200 Angstrom feature, it is still important to investigate hydrogen as the source; therefore, a second paper will present models that include a thin shell of hydrogen around the main composition structure.",
    "MGT": "Type Ib supernovae (SNe Ib) are characterized by the lack of hydrogen lines in their optical spectra, yet they exhibit strong helium features. Understanding the physical processes that lead to this spectral morphology is crucial for accurately modeling their progenitors and explosion mechanisms. This paper presents a detailed spectral analysis of the Type Ib SN 1999dn, focusing on constructing hydrogen-free models to reproduce its observed spectral evolution.\n\nWe utilize a time-dependent, non-local thermodynamic equilibrium (non-LTE) radiative transfer code to synthesize synthetic spectra. Our models incorporate detailed atomic data for key elements, including He, O, C, Mg, Si, Ca, and Fe, with a particular emphasis on accurately treating helium line formation. We explore a range of density structures, elemental abundances, and excitation/ionization mechanisms to identify the parameters that best reproduce the observed spectra of SN 1999dn at various epochs.\n\nOur analysis reveals that the early-time spectra are dominated by strong He I lines, which require relatively high helium abundances in the outer layers of the ejecta. We find that models with a sharp transition between helium-rich and oxygen-rich zones provide a better fit to the observed line profiles compared to models with a gradual abundance gradient. Moreover, we investigate the influence of different excitation mechanisms, including collisional excitation and recombination, on the helium line strengths.\n\nAt later epochs, the spectra exhibit the emergence of heavier elements, such as O I, Mg II, and Ca II. Our models suggest that these elements are located in deeper layers of the ejecta, consistent with a stratified abundance distribution. We explore the impact of varying the mixing of these elements on the spectral features and find that some degree of mixing is necessary to reproduce the observed line strengths. Finally, we discuss the implications of our findings for understanding the progenitor system and explosion scenario of SN 1999dn, and consider the limitations of hydrogen-free models in fully capturing the observed spectral diversity of SNe Ib.\n"
  },
  {
    "id": 44,
    "prompt": "Evidence for Evolution Among Primordial Disks in the 5 Myr Old Upper Scorpius OB Association",
    "HWT": "Moderate-resolution, near-infrared spectra between 0.8 and 5.2 microns were obtained for 12 late-type (K0-M3) disk-bearing members of the ~5 Myr old Upper Scorpius OB association using SpeX on the NASA Infrared Telescope Facility. For most sources, continuum excess emission first becomes apparent between ~2.2 and 4.5 microns and is consistent with that produced by single-temperature blackbodies having characteristic temperatures ranging from ~500 to 1300 K. The near-infrared spectra for 5 of 12 Upper Scorpius sources exhibit Pa-gamma, Pa-beta and Br-gamma emission, indicators of disk accretion. Using a correlation between Pa-beta and Br-gamma emission line luminosity and accretion luminosity, mass accretion rates (Mdot) are derived for these sources that range from Mdot = 3.5 X 10^{-10} to 1.5 X 10^{-8} MSun per yr. Merging the SpeX observations with Spitzer Space Telescope mid-infrared (5.4-37.0 micron) spectroscopy and 24 and 70 micron broadband photometry, the observed spectral energy distributions are compared with those predicted by two-dimensional, radiative transfer accretion disk models. Of the 9 Upper Scorpius sources examined in this analysis, 3 exhibit spectral energy distributions that are most consistent with models having inner disk radii that substantially exceed their respective dust sublimation radii. The remaining Upper Scorpius members possess spectral energy distributions that either show significant dispersion among predicted inner disk radii or are best described by models having inner disk rims coincident with the dust sublimation radius.",
    "MGT": "Circumstellar disks around young stars are the sites of planet formation, and their evolution is crucial for understanding the diversity of exoplanetary systems. This study presents new ALMA observations of millimeter-wave continuum emission from protoplanetary disks in the Upper Scorpius OB association, a coeval population with an estimated age of 5 Myr. We analyze a sample of 130 disks, combining our ALMA data with existing optical and infrared photometry, to investigate disk properties and their dependence on stellar mass and environment. We find a strong correlation between millimeter luminosity, a proxy for disk mass, and stellar mass, confirming previous findings. However, the scatter in this relationship is significant, suggesting that other factors, such as photoevaporation from nearby massive stars, play a role in disk evolution. We observe a clear trend of decreasing disk mass with increasing stellar age within Upper Scorpius, indicating ongoing disk dispersal. Furthermore, we find evidence for a decline in the disk fraction compared to younger star-forming regions, supporting the notion that planet formation is well underway in this association. By comparing the observed disk properties with theoretical models of disk evolution, we constrain the timescales for dust growth and planetesimal formation. Our results suggest that the inner regions of protoplanetary disks in Upper Scorpius have likely undergone significant evolution, potentially leading to the formation of planetary systems. The study underscores the importance of studying disk evolution in coeval populations to disentangle the effects of age, stellar mass, and environment on planet formation.\n"
  },
  {
    "id": 45,
    "prompt": "VLT and GTC observations of SDSS J0123+00: a type 2 quasar triggered in a galaxy encounter?",
    "HWT": "We present long-slit spectroscopy, continuum and [OIII]5007 imaging data obtained with the Very Large Telescope and the Gran Telescopio Canarias of the type 2 quasar SDSS J0123+00 at z=0.399. The quasar lies in a complex, gas-rich environment. It appears to be physically connected by a tidal bridge to another galaxy at a projected distance of ~100 kpc, which suggests this is an interacting system. Ionized gas is detected to a distance of at least ~133 kpc from the nucleus. The nebula has a total extension of ~180 kpc. This is one of the largest ionized nebulae ever detected associated with an active galaxy. Based on the environmental properties, we propose that the origin of the nebula is tidal debris from a galactic encounter, which could as well be the triggering mechanism of the nuclear activity. SDSS J0123+00 demonstrates that giant, luminous ionized nebulae can exist associated with type 2 quasars of low radio luminosities, contrary to expectations based on type 1 quasar studies.",
    "MGT": "We present optical spectroscopic and imaging observations of the type 2 quasar SDSS J0123+00, obtained with the Very Large Telescope (VLT) and the Gran Telescopio Canarias (GTC). Our VLT/FORS2 long-slit spectroscopy reveals extended emission line regions (EELRs) reaching up to ~15 kpc from the nucleus, exhibiting complex kinematics. Velocity maps show disturbed gas motions, with significant velocity shear and offsets suggestive of tidal interactions. The GTC/HiPERCAM imaging unveils a disturbed morphology, with tidal tails and bridges indicative of an ongoing or recent galaxy merger. We also detect a secondary nucleus at a projected distance of ~5 kpc, further supporting the merger scenario. Photoionization models applied to the EELR spectra suggest that the gas is primarily ionized by the active galactic nucleus (AGN), with possible contributions from shocks in the outermost regions. The gas metallicities inferred from strong-line diagnostics are near-solar. These findings suggest that the quasar activity in SDSS J0123+00 has likely been triggered by a galaxy merger, which has funneled gas towards the central supermassive black hole. The observed EELRs trace the outflowing and tidally stripped gas, providing valuable insights into the AGN feedback processes and the interplay between galaxy mergers and black hole accretion. The dual nucleus system indicates a late-stage merger, making this system an ideal laboratory for studying the triggering mechanisms of AGN in galaxy mergers.\n"
  },
  {
    "id": 46,
    "prompt": "Reasoning in Abella about Structural Operational Semantics Specifications",
    "HWT": "The approach to reasoning about structural operational semantics style specifications supported by the Abella system is discussed. This approach uses lambda tree syntax to treat object language binding and encodes binding related properties in generic judgments. Further, object language specifications are embedded directly into the reasoning framework through recursive definitions. The treatment of binding via generic judgments implicitly enforces distinctness and atomicity in the names used for bound variables. These properties must, however, be made explicit in reasoning tasks. This objective can be achieved by allowing recursive definitions to also specify generic properties of atomic predicates. The utility of these various logical features in the Abella system is demonstrated through actual reasoning tasks. Brief comparisons with a few other logic based approaches are also made.",
    "MGT": "This paper presents a formalization of Structural Operational Semantics (SOS) within the Abella interactive theorem prover, leveraging Abella's support for reasoning about binding and computation in object languages. We introduce a novel approach to encoding SOS rules, representing judgments as inductive definitions within Abella's meta-logic. This encoding enables direct reasoning about the derivability of judgments and the operational behavior of programs. We demonstrate the effectiveness of our approach through several case studies, including the formal verification of type soundness for a simple imperative language and the establishment of properties of program equivalence. The formalization supports reasoning about both terminating and non-terminating computations.  Moreover, we show how Abella's features for reasoning about open terms are crucial for proving properties that hold for all contexts. This work contributes to the growing body of research on formalizing programming language semantics, offering a practical and expressive framework for verifying properties of language implementations directly from their operational specifications. We argue that Abella's meta-logic offers a powerful tool for reasoning about the dynamic semantics of programming languages.\n"
  },
  {
    "id": 47,
    "prompt": "Post-launch performance of the Fermi Large Area Telescope",
    "HWT": "The Large Area Telescope (LAT) on-board the Fermi Gamma-ray Space Telescope started nominal operations on August 13, 2008, after about 60 days of instrument checkout and commissioning and is currently performing an all-sky gamma-ray survey from 30 MeV to above 300 GeV with unprecedented sensitivity and angular resolution. The LAT pre-launch response was tuned using Monte Carlo simulations and test beam data from a campaign necessarily limited in scope. This suggested a conservative approach in dealing with systematics that affect the reconstruction analysis of the first months of data taking. The first major update of the instrument performance based on flight data is now being completed. Not only are the LAT calibrations now based on flight data, but also the ground event reconstruction has been updated to accommodate on-orbit calibrations, and response was carefully verified using real data from celestial sources. In this contribution we describe the current best knowledge of the instrument, and our plans towards releasing public response functions to support data release in year 2.",
    "MGT": "The Fermi Large Area Telescope (LAT) has revolutionized our understanding of the gamma-ray sky since its launch in 2008. This article presents a comprehensive overview of the LAT's post-launch performance, focusing on key aspects such as its calibration, data processing, and long-term stability. We detail the improvements made to the instrument response functions (IRFs) over time, which are crucial for accurate source localization and spectral analysis. The evolution of the data processing pipeline is discussed, highlighting enhancements in background rejection and event reconstruction. We analyze the LAT's sensitivity to various gamma-ray sources, including pulsars, active galactic nuclei (AGNs), and diffuse emission, demonstrating its capability to probe both galactic and extragalactic phenomena. Furthermore, we address the challenges encountered during the mission, such as radiation damage and detector degradation, and outline the mitigation strategies employed to maintain optimal performance. The impact of these performance improvements on astrophysical discoveries is illustrated through specific examples, showcasing the LAT's contributions to our understanding of cosmic-ray acceleration, dark matter searches, and the high-energy universe. Finally, we discuss the future prospects for the LAT, including ongoing efforts to refine its calibration and data analysis techniques, ensuring its continued scientific productivity in the coming years.\n"
  },
  {
    "id": 48,
    "prompt": "The GASP-WEBT monitoring of 3C 454.3 during the 2008 optical-to-radio and gamma-ray outburst",
    "HWT": "Since 2001, the radio quasar 3C 454.3 has undergone a period of high optical activity, culminating in the brightest optical state ever observed, during the 2004-2005 outburst. The Whole Earth Blazar Telescope (WEBT) consortium has carried out several multifrequency campaigns to follow the source behaviour. The GLAST-AGILE Support Program (GASP) was born from the WEBT to provide long-term continuous optical-to-radio monitoring of a sample of gamma-loud blazars, during the operation of the AGILE and GLAST (now known as Fermi GST) gamma-ray satellites. The main aim is to shed light on the mechanisms producing the high-energy radiation, through correlation analysis with the low-energy emission. Thus, since 2008 the monitoring task on 3C 454.3 passed from the WEBT to the GASP, while both AGILE and Fermi detected strong gamma-ray emission from the source. We present the main results obtained by the GASP at optical, mm, and radio frequencies in the 2008-2009 season, and compare them with the WEBT results from previous years. An optical outburst was observed to peak in mid July 2008, when Fermi detected the brightest gamma-ray levels. A contemporaneous mm outburst maintained its brightness for a longer time, until the cm emission also reached the maximum levels. The behaviour compared in the three bands suggests that the variable relative brightness of the different-frequency outbursts may be due to the changing orientation of a curved inhomogeneous jet. The optical light curve is very well sampled during the entire season, which is also well covered by the various AGILE and Fermi observing periods. The relevant cross-correlation studies will be very important in constraining high-energy emission models.",
    "MGT": "We present the results of an extensive multi-frequency monitoring campaign of the blazar 3C 454.3 during its dramatic 2008 outburst, combining optical, near-infrared, and radio observations from the GASP-WEBT consortium with gamma-ray data from the Fermi-LAT. This period witnessed unprecedented activity across the electromagnetic spectrum, offering a unique opportunity to probe the emission mechanisms and physical conditions within the relativistic jet of this powerful active galactic nucleus. Our observations reveal a complex interplay between the various wavebands, with significant variability observed on timescales ranging from hours to months. The optical R-band light curve exhibits multiple peaks, with the most prominent coinciding with a major gamma-ray flare.\n\nWe performed detailed spectral energy distribution (SED) modeling, employing both a one-zone synchrotron self-Compton (SSC) model and an external Compton (EC) model to interpret the observed emission. The SSC model requires extreme parameters, such as a very high Doppler factor and a compact emission region, to adequately reproduce the observed SEDs, particularly during the peak of the outburst. The EC model, which incorporates seed photons from external sources like the broad-line region or the dusty torus, provides a more natural explanation for the high-energy emission.\n\nCross-correlation analysis between the optical and gamma-ray light curves reveals a strong correlation, with the gamma-ray emission lagging the optical emission by a few days. This delay suggests that the gamma-ray emission region is located further down the jet than the optical emission region, potentially due to adiabatic cooling or particle acceleration processes. Furthermore, we find evidence for spectral hardening during the outburst, indicating an increase in the maximum electron energy within the jet. Our findings support the scenario where the 2008 outburst was triggered by a shock propagating down the jet, accelerating particles and boosting the emission across all frequencies. The comprehensive dataset presented here provides valuable insights into the complex physics governing the emission processes in blazars and highlights the importance of multi-wavelength monitoring campaigns for understanding these extreme objects.\n"
  },
  {
    "id": 49,
    "prompt": "Right sneutrinos and the signals of a stable stop at the Large Hadron Collider",
    "HWT": "We investigate charged tracks signals of a supersymmetric scenario, where the lighter stop is the next-to-lightest supersymmetric particle (NLSP). It is found that such an NLSP is stable on the scale of the detector at the LHC if one has a right-chiral sneutrino as the lightest supersymmetric particle (LSP). After identifying some benchmark points in the parameter space of a supergravity scenario with non-universal scalar masses, we study a few specific classes of signals, namely, stop pair production and gluino pair production followed by each decaying into a stop and a top. It is shown that proper kinematic cuts remove the backgrounds in each case, and, while a few months' worth of data is sufficient to have copious events in the first case, one may require 300 $fb^{-1}$ for the other. One can also aspire to reconstruct the gluino mass, using the `visible' stable NLSP tracks.",
    "MGT": "We explore the phenomenology of a stable stop squark in the context of the Minimal Supersymmetric Standard Model extended with right-handed neutrinos (MRSSM). The presence of right-handed neutrinos provides a natural candidate for the lightest supersymmetric particle, the right sneutrino, which can be lighter than the lightest stop. We investigate the scenario where the stop is the next-to-lightest supersymmetric particle and decays predominantly to a right sneutrino and a top quark. This leads to distinctive signatures at the Large Hadron Collider (LHC).\n\nThe stable stop hadronizes, forming R-hadrons which can be slow-moving and highly ionizing. We analyze the constraints from existing searches for heavy stable charged particles and determine the allowed parameter space for the stop and right sneutrino masses. We further study the potential for discovering such a scenario through disappearing track searches, where the R-hadron decays within the detector, producing a track that abruptly ends. We find that the LHC has the potential to probe a significant portion of the parameter space, especially with increased luminosity. Our results highlight the importance of considering the MRSSM when searching for supersymmetry at the LHC, as the presence of right-handed neutrinos can significantly alter the expected signatures.\n"
  },
  {
    "id": 50,
    "prompt": "The Dynamics of Dense Cores in the Perseus Molecular Cloud II: The Relationship Between Dense Cores and the Cloud",
    "HWT": "We utilize the extensive datasets available for the Perseus molecular cloud to analyze the relationship between the kinematics of small-scale dense cores and the larger structures in which they are embedded. The kinematic measures presented here can be used in conjunction with those discussed in our previous work as strong observational constraints that numerical simulations (or analytic models) of star formation should match. We find that dense cores have small motions with respect to the 13CO gas, about one third of the 13CO velocity dispersion along the same line of sight. Within each extinction region, the core-to-core velocity dispersion is about half of the total (13CO) velocity dispersion seen in the region. Large-scale velocity gradients account for roughly half of the total velocity dispersion in each region, similar to what is predicted from large-scale turbulent modes following a power spectrum of P(k) ~ k^{-4}.",
    "MGT": "We present an investigation into the relationship between dense cores and the larger-scale structure of the Perseus Molecular Cloud, using data from the COMPLETE survey, supplemented with *Herschel* data. We analyze the spatial distribution of dense cores relative to the cloud's filamentary network, finding a strong correlation between core locations and filament spines. The vast majority of cores are found within or near filaments, suggesting that filaments play a crucial role in core formation. We explore the mass distribution of cores within different regions of the cloud, observing variations that hint at environmental influences on core properties. Specifically, cores located in denser, more active regions of Perseus tend to be more massive. We calculate the core formation efficiency (CFE) across the cloud, finding that the CFE is higher in regions with more prominent filamentary structures. This suggests that the gravitational focusing of material along filaments enhances core formation. We investigate the kinematic properties of the cores, observing a trend of decreasing velocity dispersion with increasing core density. This supports the idea that cores become more gravitationally bound as they evolve. Our findings provide further support for the role of filaments in shaping the dynamics and evolution of dense cores within molecular clouds.\n"
  },
  {
    "id": 51,
    "prompt": "Towards a Maximal Mass Model",
    "HWT": "We investigate the possibility to construct a generalization of the Standard Model, which we call the Maximal Mass Model because it contains a limiting mass $M$ for its fundamental constituents. The parameter $M$ is considered as a new universal physical constant of Nature and therefore is called the fundamental mass. It is introduced in a purely geometrical way, like the velocity of light as a maximal velocity in the special relativity. If one chooses the Euclidean formulation of quantum field theory, the adequate realization of the limiting mass hypothesis is reduced to the choice of the de Sitter geometry as the geometry of the 4-momentum space. All fields, defined in de Sitter p-space in configurational space obey five dimensional Klein-Gordon type equation with fundamental mass $M$ as a mass parameter. The role of dynamical field variables is played by the Cauchy initial conditions given at $x_5 = 0$, guarantying the locality and gauge invariance principles. The corresponding to the geometrical requirements formulation of the theory of scalar, vector and spinor fields is considered in some detail. On a simple example it is demonstrated that the spontaneously symmetry breaking mechanism leads to renormalization of the fundamental mass $M$. A new geometrical concept of the chirality of the fermion fields is introduced. It would be responsible for new measurable effects at high energies $E \\geq M$. Interaction terms of a new type, due to the existence of the Higgs boson are revealed. The most intriguing prediction of the new approach is the possible existence of exotic fermions with no analogues in the SM, which may be candidate for dark matter constituents.",
    "MGT": "The pursuit of understanding the universe's composition and structure has led to various cosmological models, each attempting to reconcile observational data with theoretical frameworks. This article proposes a novel cosmological model, termed the \"Maximal Mass Model\" (MMM), predicated on the hypothesis that the universe possesses a finite, maximal mass limit. This limit arises from a confluence of gravitational self-interaction, quantum mechanical considerations, and the observed accelerated expansion of the universe. Unlike standard Lambda-CDM cosmology, which relies heavily on dark matter and dark energy to explain observed phenomena, the MMM posits that these enigmatic components may be emergent properties arising from the universe approaching its maximal mass capacity.\n\nThe core premise of the MMM involves a modification of Einstein's field equations to incorporate a mass-dependent cosmological term. This term, initially negligible at early times when the universe's mass was significantly below the maximal limit, becomes increasingly dominant as the universe evolves and its mass approaches this theoretical bound. This increasing dominance naturally leads to an accelerated expansion, mimicking the effects attributed to dark energy. Furthermore, the gravitational self-interaction within a system approaching its maximal mass limit can lead to complex dynamics that manifest as the observed effects of dark matter, influencing galactic rotation curves and the formation of large-scale structures.\n\nThe article explores the theoretical underpinnings of the MMM, deriving the modified field equations and examining their implications for various cosmological observables. We analyze the model's predictions for the cosmic microwave background (CMB) anisotropies, the baryon acoustic oscillations (BAO), and the Hubble constant, comparing them with existing observational data. The analysis suggests that the MMM can provide a viable alternative to the Lambda-CDM model, potentially resolving some of the tensions and inconsistencies inherent in the standard cosmological paradigm. Furthermore, the MMM offers testable predictions, particularly concerning the evolution of the Hubble constant and the distribution of matter at extremely large scales, which can be probed by future cosmological surveys. Finally, we discuss the philosophical implications of a universe with a maximal mass limit, touching upon the nature of infinity and the ultimate fate of the cosmos.\n"
  },
  {
    "id": 52,
    "prompt": "3-He in the Milky Way Interstellar Medium: Ionization Structure",
    "HWT": "The cosmic abundance of the 3-He isotope has important implications for many fields of astrophysics. We are using the 8.665 GHz hyperfine transition of 3-He+ to determine the 3-He/H abundance in Milky Way HII regions and planetary nebulae. This is one in a series of papers in which we discuss issues involved in deriving accurate 3-He/H abundance ratios from the available measurements. Here we describe the ionization correction we use to convert the 3-He+/H+ abundance, y3+, to the 3-He/H abundance, y3. In principle the nebular ionization structure can significantly influence the y3 derived for individual sources. We find that in general there is insufficient information available to make a detailed ionization correction. Here we make a simple correction and assess its validity. The correction is based on radio recombination line measurements of H+ and 4-He+, together with simple core-halo source models. We use these models to establish criteria that allow us to identify sources that can be accurately corrected for ionization and those that cannot. We argue that this effect cannot be very large for most of the sources in our observational sample. For a wide range of models of nebular ionization structure we find that the ionization correction factor varies from 1 to 1.8. Although large corrections are possible, there would have to be a conspiracy between the density and ionization structure for us to underestimate the ionization correction by a substantial amount.",
    "MGT": "The distribution and ionization state of 3-He in the Milky Way's interstellar medium (ISM) offers valuable insights into Galactic chemical evolution, stellar nucleosynthesis, and ISM physics.  3-He, primarily produced in Big Bang nucleosynthesis and low-mass stars, is susceptible to destruction in stellar interiors depending on stellar mass and mixing processes. Its observed abundance is therefore a sensitive probe of stellar evolution models and the degree of ISM mixing. This study presents a comprehensive model of 3-He ionization within various ISM phases, considering photoionization by the interstellar radiation field (ISRF), collisional ionization, and charge exchange reactions with hydrogen and helium ions. We employ a state-of-the-art 3D model of the Milky Way, incorporating realistic distributions of stars, gas, and dust to accurately calculate the ISRF throughout the Galaxy.  We explore the ionization balance of 3-He in warm neutral medium (WNM), cold neutral medium (CNM), and warm ionized medium (WIM) conditions, accounting for variations in density, temperature, and metallicity. Our results indicate that 3-He is predominantly singly ionized (3-He+) in the WNM and WIM, while it remains largely neutral in the CNM due to the increased recombination rate at lower temperatures and shielding from the ISRF.  We find a significant dependence of the 3-He+/3-He ratio on the local ISRF intensity and gas density, with higher ionization fractions in regions with intense radiation fields and lower densities. Furthermore, we investigate the impact of charge exchange reactions on the ionization balance, demonstrating their importance in maintaining ionization equilibrium, especially in regions where the abundance of ionized hydrogen is significant.  The predicted distribution of 3-He+ provides crucial information for planning and interpreting future observations of 3-He+ hyperfine structure lines, which can be used to directly measure the 3-He abundance in different regions of the Galaxy, thus providing constraints on Galactic chemical evolution models. We also discuss the implications of our findings for understanding the primordial 3-He abundance and the contribution of low-mass stars to the Galactic 3-He budget.\n"
  },
  {
    "id": 53,
    "prompt": "A Hybrid Mechanism Forming a 2:1 Librating-Circulating Resonant Configuration in the Planetary System",
    "HWT": "A diversity of resonance configurations may be formed under different migration of two giant planets. And the researchers show that the HD 128311 and HD 73526 planetary systems are involved in a 2:1 mean motion resonance but not in apsidal corotation, because one of the resonance argument circulates over the dynamical evolution. In this paper, we investigate potential mechanisms to form the 2:1 librating-circulating resonance configuration. In the late stage of planetary formation, scattering or colliding among planetesimals and planetary embryos can frequently occur. Hence, in our model, we consider a planetary configuration of two giants together with few terrestrial planets. We find that both colliding or scattering events at very early stage of dynamical evolution can influence the configurations trapped into resonance. A planet-planet scattering of a moderate terrestrial planet, or multiple scattering of smaller planets in a crowded planetary system can change the resonant configuration. In addition, collision or merging can alter the masses and location of the giant planets, which also play an important role in shaping the resonant configuration during the dynamical evolution. In this sense, the librating-circulating resonance configuration is more likely to form by a hybrid mechanism of scattering and collision.",
    "MGT": "The formation of planetary systems frequently results in mean-motion resonances (MMRs) between planets, influencing their long-term dynamical evolution. This study investigates a hybrid mechanism leading to a 2:1 MMR, specifically a configuration where one planet librates within the resonance while the other circulates. The proposed mechanism combines aspects of convergent migration and in-situ formation. Initially, a planetary embryo forms in-situ at a location exterior to a pre-existing inner planet. Subsequently, the embryo undergoes inward migration due to interactions with a protoplanetary disk. As the embryo migrates, it approaches the 2:1 MMR with the inner planet.\n\nCrucially, the migration rate and the embryo's mass play vital roles. If the migration is sufficiently slow, the embryo can be captured into the 2:1 MMR. However, instead of both planets librating, the system transitions into a hybrid state. This occurs when the embryo's mass is relatively small compared to the inner planet. The inner planet's eccentricity is excited by the resonance, leading to a libration of its resonant argument. Simultaneously, the outer, migrating planet experiences weaker resonant forcing, allowing its resonant argument to circulate. N-body simulations confirm the viability of this hybrid mechanism, demonstrating the formation of stable 2:1 MMR configurations with one librating and one circulating planet. The resulting planetary system exhibits a unique dynamical architecture, potentially observable through transit timing variations or radial velocity measurements. This hybrid mechanism offers a novel pathway for the formation of diverse planetary system architectures.\n"
  },
  {
    "id": 54,
    "prompt": "Decidability properties for fragments of CHR",
    "HWT": "We study the decidability of termination for two CHR dialects which, similarly to the Datalog like languages, are defined by using a signature which does not allow function symbols (of arity >0). Both languages allow the use of the = built-in in the body of rules, thus are built on a host language that supports unification. However each imposes one further restriction. The first CHR dialect allows only range-restricted rules, that is, it does not allow the use of variables in the body or in the guard of a rule if they do not appear in the head. We show that the existence of an infinite computation is decidable for this dialect. The second dialect instead limits the number of atoms in the head of rules to one. We prove that in this case, the existence of a terminating computation is decidable. These results show that both dialects are strictly less expressive than Turing Machines. It is worth noting that the language (without function symbols) without these restrictions is as expressive as Turing Machines.",
    "MGT": "Constraint Handling Rules (CHR) is a powerful rule-based language for declarative constraint programming. A central question in the study of CHR is the decidability of its fundamental properties, such as termination and confluence, which guarantee predictable and reliable program behavior. This article investigates the decidability landscape for fragments of CHR characterized by syntactic restrictions on the rules. We focus on prominent restrictions such as range-restrictedness, where variables in the rule's head must appear in the body, and head-acyclicity, which prohibits cycles in the head constraints of the rules.\n\nWe establish novel decidability and undecidability results for termination and confluence of these fragments. Specifically, we demonstrate that termination is decidable for a fragment of range-restricted CHR with linear head constraints. However, we also prove that extending this fragment with even simple non-linear constraints leads to undecidability of termination. Furthermore, we examine the confluence property and show that it remains undecidable for several restricted CHR fragments, even under strong assumptions on the constraint theory. These negative results highlight the inherent complexity of analyzing CHR programs and underscore the need for more sophisticated analysis techniques. Our findings contribute to a deeper understanding of the boundaries between decidable and undecidable properties in CHR, providing valuable insights for the design of more predictable and verifiable constraint solvers.\n"
  },
  {
    "id": 55,
    "prompt": "Temperature and fluence dependence of ultrafast phase separation dynamics in Pr0.6Ca0.4MnO3 thin films",
    "HWT": "Temperature and fluence dependence of the transient photoinduced reflectivity and the magnetooptical Kerr angle was measured in two Pr0.6Ca0.4MnO3 thin films subject to tensile and compressive substrate-induced strain. A photoinduced transient ferromagnetic metallic (TFM) phase is found to form below ~60K and ~40K in the substrate-strained and substrate-compressed film, respectively. From the hysteresis loops a difference in the TFM cluster sizes and amount of photomodulation is observed at low temperatures and low excitation fluences in the films with different strain. Surprisingly, the characteristic timescale for the TFM phase photomodulation is virtually strain independent. At high excitation fluences, the cluster sizes and amount of photomodulation are independent on the substrate-induced strain.",
    "MGT": "We investigate the ultrafast dynamics of photoinduced phase separation in Pr0.6Ca0.4MnO3 thin films using femtosecond time-resolved optical spectroscopy. The material exhibits a photoinduced insulator-to-metal transition associated with the melting of the charge-ordered insulating phase and the formation of metallic ferromagnetic nanoregions. We explore the temperature and fluence dependence of this process, revealing a complex interplay between the initial lattice temperature and the excitation density. At lower temperatures, the phase separation dynamics are significantly faster, indicating a reduced energy barrier for the transition. Higher fluences lead to a saturation of the metallic volume fraction and a slowing down of the recovery dynamics. The observed behavior is interpreted within a framework of nucleation and growth of metallic domains, influenced by the initial charge-ordered state and the energy deposited by the laser pulse. Our results provide insights into the fundamental mechanisms governing ultrafast phase transitions in strongly correlated electron systems.\n"
  },
  {
    "id": 56,
    "prompt": "Confinement of electrons in size modulated silicon nanowires",
    "HWT": "Based on first-principles calculations we showed that superlattices of periodically repeated junctions of hydrogen saturated silicon nanowire segments having different lengths and diameters form multiple quantum well structures. The band gap of the superlattice is modulated in real space as its diameter does and results in a band gap in momentum space which is different from constituent nanowires. Specific electronic states can be confined in either narrow or wide regions of superlattice. The type of the band lineup and hence the offsets of valence and conduction bands depend on the orientation of the superlattice as well as on the diameters of the constituent segments. Effects of the SiH vacancy and substitutional impurities on the electronic and magnetic properties have been investigated by carrying out spin-polarized calculations. Substitutional impurities with localized states near band edges can make modulation doping possible. Stability of the superlattice structure was examined by ab initio molecular dynamics calculations at high temperatures.",
    "MGT": "Silicon nanowires (SiNWs) have garnered significant attention due to their potential applications in nanoscale electronics and optoelectronics. This study investigates the electronic confinement effects in size-modulated SiNWs, focusing on the impact of periodic diameter variations on the electron energy levels and wavefunctions. We employ a three-dimensional finite element method to solve the Schrödinger equation for electrons confined within SiNWs with sinusoidal diameter modulations. Our results demonstrate that the modulated nanowire diameter induces the formation of quantum dots along the wire, leading to the quantization of electron energy levels. The energy gap and the spatial localization of the electronic states are strongly dependent on the modulation amplitude and period. Specifically, increasing the modulation amplitude enhances the confinement, resulting in larger energy gaps and more localized wavefunctions. Conversely, increasing the modulation period reduces the confinement effects. These findings provide valuable insights into the design and optimization of SiNW-based quantum devices with tailored electronic properties. The ability to control the electronic structure through size modulation opens up new avenues for creating novel nanoscale electronic devices.\n"
  },
  {
    "id": 57,
    "prompt": "Constraining the LRG Halo Occupation Distribution using Counts-in-Cylinders",
    "HWT": "The low number density of the Sloan Digital Sky Survey (SDSS) Luminous Red Galaxies (LRGs) suggests that LRGs occupying the same dark matter halo can be separated from pairs occupying distinct dark matter halos with high fidelity. We present a new technique, Counts-in-Cylinders (CiC), to constrain the parameters of the satellite contribution to the LRG Halo-Occupation Distribution (HOD). For a fiber collision-corrected SDSS spectroscopic LRG subsample at 0.16 < z < 0.36, we find the CiC multiplicity function is fit by a halo model where the average number of satellites in a halo of mass M is <Nsat(M)> = ((M - Mcut)/M1)^alpha with Mcut = 5.0 +1.5/-1.3 (+2.9/-2.6) X 10^13 Msun, M1 = 4.95 +0.37/-0.26 (+0.79/-0.53) X 10^14 Msun, and alpha = 1.035 +0.10/-0.17 (+0.24/-0.31) at the 68% and 95% confidence levels using a WMAP3 cosmology and z=0.2 halo catalog. Our method tightly constrains the fraction of LRGs that are satellite galaxies, 6.36 +0.38/-0.39, and the combination Mcut/10^{14} Msun + alpha = 1.53 +0.08/-0.09 at the 95% confidence level. We also find that mocks based on a halo catalog produced by a spherical overdensity (SO) finder reproduce both the measured CiC multiplicity function and the projected correlation function, while mocks based on a Friends-of-Friends (FoF) halo catalog has a deficit of close pairs at ~1 Mpc/h separations. Because the CiC method relies on higher order statistics of close pairs, it is robust to the choice of halo finder. In a companion paper we will apply this technique to optimize Finger-of-God (FOG) compression to eliminate the 1-halo contribution to the LRG power spectrum.",
    "MGT": "We present an analysis of the halo occupation distribution (HOD) of luminous red galaxies (LRGs) using a counts-in-cylinders (CIC) approach applied to the Sloan Digital Sky Survey (SDSS) Data Release 7 (DR7) LRG sample. CIC provides a robust and efficient method for measuring galaxy clustering on small scales, which is particularly sensitive to the details of the HOD. Unlike traditional two-point correlation function analyses, CIC directly probes the number of galaxies residing within cylindrical volumes, allowing us to better constrain the satellite galaxy fraction and the scatter in the relationship between halo mass and galaxy luminosity.\n\nOur analysis involves constructing cylindrical volumes around randomly selected points within the survey volume and measuring the number of LRGs contained within each cylinder. By varying the cylinder radius and length, we are able to probe different halo scales and galaxy densities. We compare our measurements to predictions from a suite of HOD models, which are populated within dark matter halos extracted from a large N-body simulation. We explore a range of HOD parameterizations, including variations in the central and satellite galaxy occupation functions, the halo mass-luminosity relation, and the radial distribution of satellite galaxies.\n\nWe find that the CIC measurements provide strong constraints on the mean halo occupation function, particularly at the low-mass end. Our results indicate a relatively sharp cutoff in the probability of a halo hosting a central LRG as a function of halo mass. Furthermore, we find evidence for a non-negligible satellite fraction, with approximately 10-20% of LRGs residing as satellites within more massive halos. The radial distribution of satellite galaxies appears to be well-described by an NFW profile with a concentration parameter similar to that of the host halo. We also investigate the impact of assembly bias on our results and find that it has a relatively small effect on the derived HOD parameters. Our findings provide valuable insights into the relationship between LRGs and their host halos, contributing to our understanding of galaxy formation and evolution within the large-scale structure of the universe.\n"
  },
  {
    "id": 58,
    "prompt": "Three-Dimensional Simulations of Mixing Instabilities in Supernova Explosions",
    "HWT": "We present the first three-dimensional (3D) simulations of the large-scale mixing that takes place in the shock-heated stellar layers ejected in the explosion of a 15.5 solar-mass blue supergiant star. The outgoing supernova shock is followed from its launch by neutrino heating until it breaks out from the stellar surface more than two hours after the core collapse. Violent convective overturn in the post-shock layer causes the explosion to start with significant asphericity, which triggers the growth of Rayleigh-Taylor (RT) instabilities at the composition interfaces of the exploding star. Deep inward mixing of hydrogen (H) is found as well as fast-moving, metal-rich clumps penetrating with high velocities far into the H-envelope of the star as observed, e.g., in the case of SN 1987A. Also individual clumps containing a sizeable fraction of the ejected iron-group elements (up to several 0.001 solar masses) are obtained in some models. The metal core of the progenitor is partially turned over with Ni-dominated fingers overtaking oxygen-rich bullets and both Ni and O moving well ahead of the material from the carbon layer. Comparing with corresponding 2D (axially symmetric) calculations, we determine the growth of the RT fingers to be faster, the deceleration of the dense metal-carrying clumps in the He and H layers to be reduced, the asymptotic clump velocities in the H-shell to be higher (up to ~4500 km/s for the considered progenitor and an explosion energy of 10^{51} ergs, instead of <2000 km/s in 2D), and the outward radial mixing of heavy elements and inward mixing of hydrogen to be more efficient in 3D than in 2D. We present a simple argument that explains these results as a consequence of the different action of drag forces on moving objects in the two geometries. (abridged)",
    "MGT": "Type Ia supernovae (SNe Ia) are critical cosmological distance indicators and play a key role in the chemical evolution of galaxies. However, the precise mechanism triggering these explosions remains a subject of ongoing research. One prevailing model involves the deflagration-to-detonation transition (DDT) in a Chandrasekhar-mass white dwarf. Hydrodynamic instabilities, particularly the Rayleigh-Taylor (RT) and Kelvin-Helmholtz (KH) instabilities, are believed to be crucial in shaping the explosion dynamics and influencing the flame propagation, potentially facilitating the DDT. These instabilities arise from density gradients and velocity shear at the interface between the expanding hot ashes and the unburnt fuel.\n\nThis study presents high-resolution three-dimensional (3D) simulations of mixing instabilities in the context of SN Ia explosions, focusing on the interaction between the flame and the surrounding material. We employ a massively parallel hydrodynamics code to model the evolution of these instabilities, incorporating detailed nuclear reaction networks to accurately capture the energy release and composition changes. Our simulations investigate the influence of various parameters, including the initial flame geometry, density stratification, and the presence of pre-existing turbulence, on the development of RT and KH instabilities.\n\nWe find that the RT instability rapidly develops at the flame front, leading to the formation of large-scale mushroom-shaped structures. These structures entrain unburnt fuel into the hot ashes, enhancing mixing and promoting further burning. The KH instability, arising from velocity shear between the expanding flame and the surrounding material, further contributes to the mixing process, creating smaller-scale turbulent eddies. The interplay between these instabilities significantly increases the surface area of the flame, accelerating the burning rate.\n\nOur simulations reveal a complex interplay between the initial conditions and the subsequent development of the instabilities. Specifically, we observe that the presence of pre-existing turbulence can significantly enhance the growth rate of both RT and KH instabilities, leading to a more vigorous mixing process. Furthermore, the initial flame geometry plays a crucial role in determining the overall morphology of the explosion. We analyze the resulting elemental distribution and compare our findings with observational data of SNe Ia remnants. The results highlight the importance of 3D simulations in accurately capturing the complex hydrodynamical processes that govern SN Ia explosions and provide valuable insights into the conditions required for a DDT to occur. These simulations contribute to a better understanding of the explosion mechanism and help refine the use of SNe Ia as cosmological probes.\n"
  },
  {
    "id": 59,
    "prompt": "Bayesian orthogonal component analysis for sparse representation",
    "HWT": "This paper addresses the problem of identifying a lower dimensional space where observed data can be sparsely represented. This under-complete dictionary learning task can be formulated as a blind separation problem of sparse sources linearly mixed with an unknown orthogonal mixing matrix. This issue is formulated in a Bayesian framework. First, the unknown sparse sources are modeled as Bernoulli-Gaussian processes. To promote sparsity, a weighted mixture of an atom at zero and a Gaussian distribution is proposed as prior distribution for the unobserved sources. A non-informative prior distribution defined on an appropriate Stiefel manifold is elected for the mixing matrix. The Bayesian inference on the unknown parameters is conducted using a Markov chain Monte Carlo (MCMC) method. A partially collapsed Gibbs sampler is designed to generate samples asymptotically distributed according to the joint posterior distribution of the unknown model parameters and hyperparameters. These samples are then used to approximate the joint maximum a posteriori estimator of the sources and mixing matrix. Simulations conducted on synthetic data are reported to illustrate the performance of the method for recovering sparse representations. An application to sparse coding on under-complete dictionary is finally investigated.",
    "MGT": "Orthogonal Component Analysis (OCA) is a dimensionality reduction technique that seeks to decompose data into uncorrelated components. However, traditional OCA methods often struggle with high-dimensional data and may not effectively capture sparse structures. To address these limitations, we propose a novel Bayesian Orthogonal Component Analysis (BOCA) framework tailored for sparse representation. Our approach leverages Bayesian inference to estimate the orthogonal components and their corresponding loadings, incorporating sparsity-inducing priors on the loadings to promote parsimonious representations. Specifically, we employ a hierarchical prior structure consisting of a global shrinkage parameter and local variance parameters, enabling adaptive shrinkage of irrelevant features. We develop an efficient Markov Chain Monte Carlo (MCMC) algorithm for posterior inference, allowing us to estimate the model parameters and quantify uncertainty. Furthermore, we introduce a model selection criterion based on the marginal likelihood to determine the optimal number of components. Through extensive simulations and real-world data experiments, we demonstrate that BOCA outperforms existing OCA methods and other sparse dimensionality reduction techniques in terms of reconstruction error, feature selection accuracy, and interpretability. The proposed framework provides a flexible and robust approach for extracting meaningful orthogonal components from high-dimensional, sparse data, with applications in various fields such as image processing, bioinformatics, and text analysis.\n"
  },
  {
    "id": 60,
    "prompt": "An analysis of the nucleon spectrum from lattice partially-quenched QCD",
    "HWT": "The chiral extrapolation of the nucleon mass, M_n, is investigated using data coming from 2-flavour partially-quenched lattice simulations. A large sample of lattice results from the CP-PACS Collaboration is analysed using the leading one-loop corrections, with explicit corrections for finite lattice spacing artifacts. The extrapolation is studied using finite range regularised chiral perturbation theory. The analysis also provides a quantitative estimate of the leading finite volume corrections. It is found that the discretisation, finite-volume and partial quenching effects can all be very well described in this framework, producing an extrapolated value of M_n in agreement with experiment. Furthermore, determinations of the low energy constants of the nucleon mass's chiral expansion are in agreement with previous methods, but with significantly reduced errors. This procedure is also compared with extrapolations based on polynomial forms, where the results are less encouraging.",
    "MGT": "We present an analysis of the nucleon spectrum using lattice QCD calculations performed in the partially-quenched approximation. Our study employs an improved staggered fermion action for both sea and valence quarks, allowing us to explore a range of quark masses and lattice spacings. We focus on extracting the masses of the nucleon and its parity partner, the $N^*(1535)$, by fitting correlation functions computed from appropriate interpolating operators. The dependence of these masses on the sea and valence quark masses is carefully examined, and chiral extrapolations are performed to estimate the physical nucleon and $N^*(1535)$ masses. Finite volume effects are investigated and accounted for in our analysis. We compare our results with experimental data and other lattice QCD calculations, highlighting the strengths and limitations of the partially-quenched approximation. Furthermore, we discuss the implications of our findings for understanding the nature of the $N^*(1535)$ resonance and the role of chiral symmetry in the nucleon spectrum. This work contributes to the ongoing effort to precisely determine the properties of baryons from first-principles calculations.\n"
  },
  {
    "id": 61,
    "prompt": "Single-photon cooling at the limit of trap dynamics: Maxwell's Demon near maximum efficiency",
    "HWT": "We demonstrate a general and efficient informational cooling technique for atoms which is an experimental realization of a one-dimensional Maxwell's Demon. The technique transfers atoms from a magnetic trap into an optical trap via a single spontaneous Raman transition which is discriminatively driven near each atom's classical turning point. In this way, nearly all of the atomic ensemble's kinetic energy in one dimension is removed. We develop a simple analytical model to predict the efficiency of transfer between the traps and provide evidence that the performance is limited only by particle dynamics in the magnetic trap. Transfer efficiencies up to 2.2% are reported. We show that efficiency can be traded for phase-space compression, and we report compression up to a factor of 350. Our results represent a 15-fold improvement over our previous demonstration of the cooling technique.",
    "MGT": "Single-photon cooling offers a promising route to manipulating and controlling the motion of trapped particles at the quantum level. Here, we investigate the theoretical limits of single-photon cooling efficiency, considering realistic constraints imposed by trap dynamics. We develop a theoretical framework that treats the cooling process as a Maxwell's Demon operating on the trapped particle's motional state. This framework allows us to analyze the interplay between photon absorption, spontaneous emission, and the inherent limitations of the trap potential. We demonstrate that the cooling efficiency is fundamentally bounded by the rate at which information can be extracted about the particle's state, dictated by the single-photon absorption probability and the trap oscillation frequency. Our analysis reveals that achieving near-maximum Maxwell's Demon efficiency requires carefully tailoring the laser parameters and trap characteristics to optimize the information gain rate. Furthermore, we identify key experimental parameters that limit the achievable cooling rate in practical implementations, paving the way for future advancements in single-photon cooling techniques.\n"
  },
  {
    "id": 62,
    "prompt": "Evidence for primordial mass segregation in globular clusters",
    "HWT": "We have studied the dissolution of initially mass segregated and unsegregated star clusters due to two-body relaxation in external tidal fields, using Aarseth's collisional N-body code NBODY4 on GRAPE6 special-purpose computers. When extrapolating results of initially not mass segregated models to globular clusters, we obtain a correlation between the time until destruction and the slope of the mass function, in the sense that globular clusters which are closer to dissolution are more strongly depleted in low-mass stars. This correlation fits observed mass functions of most globular clusters. The mass functions of several globular clusters are however more strongly depleted in low-mass stars than suggested by these models. Such strongly depleted mass functions can be explained if globular clusters started initially mass segregated. Primordial mass segregation also explains the correlation between the slope of the stellar mass function and the cluster concentration which was recently discovered by De Marchi et al. (2007). In this case, it is possible that all globular clusters started with a mass function similar to that seen in young open clusters in the present-day universe, at least for stars below m=0.8 Msun. This argues for a near universality of the mass function for different star formation environments and metallicities in the range -2 < [Fe/H] < 0. We finally describe a novel algorithm which can initialise stationary mass segregated clusters with arbitrary density profile and amount of mass segregation.",
    "MGT": "Globular clusters (GCs), dense stellar systems found in the halos of galaxies, have long been considered paradigms of single-age, single-metallicity stellar populations, dynamically evolved over billions of years. While dynamical processes like mass segregation are expected to shape their present-day structure, the question of whether GCs formed with a degree of primordial mass segregation remains a topic of intense debate. This study presents compelling evidence supporting the presence of primordial mass segregation in a sample of Galactic GCs. We analyzed deep, high-resolution photometric data from the Hubble Space Telescope, focusing on the radial distribution of stars across a range of stellar masses, derived from their positions on the color-magnitude diagram. We developed a novel statistical approach, employing a mass function-weighted radial density profile analysis, to quantify the degree of mass segregation in each cluster.\n\nOur findings reveal a statistically significant trend of more massive stars being more centrally concentrated than less massive stars in several GCs, even after accounting for the effects of dynamical relaxation. By comparing the observed degree of mass segregation with predictions from N-body simulations of GCs evolving from initially unsegregated states, we demonstrate that the observed segregation levels cannot be solely attributed to dynamical evolution within the clusters' lifetimes. Furthermore, we explore correlations between the degree of primordial mass segregation and other cluster properties, such as age, metallicity, and central density, finding suggestive relationships that may offer clues to the formation mechanisms of GCs. These results strongly suggest that some GCs formed with a pre-existing degree of mass segregation, potentially reflecting the conditions in the star-forming environments from which they originated. This has significant implications for our understanding of GC formation and the early evolution of stellar populations in dense environments.\n"
  },
  {
    "id": 63,
    "prompt": "Synchronization and entrainment of coupled circadian oscillators",
    "HWT": "Circadian rhythms in mammals are controlled by the neurons located in the suprachiasmatic nucleus of the hypothalamus. In physiological conditions, the system of neurons is very efficiently entrained by the 24-hour light-dark cycle. Most of the studies carried out so far emphasize the crucial role of the periodicity imposed by the light dark cycle in neuronal synchronization. Nevertheless, heterogeneity as a natural and permanent ingredient of these cellular interactions is seemingly to play a major role in these biochemical processes. In this paper we use a model that considers the neurons of the suprachiasmatic nucleus as chemically-coupled modified Goodwin oscillators, and introduce non-negligible heterogeneity in the periods of all neurons in the form of quenched noise. The system response to the light-dark cycle periodicity is studied as a function of the interneuronal coupling strength, external forcing amplitude and neuronal heterogeneity. Our results indicate that the right amount of heterogeneity helps the extended system to respond globally in a more coherent way to the external forcing. Our proposed mechanism for neuronal synchronization under external periodic forcing is based on heterogeneity-induced oscillators death, damped oscillators being more entrainable by the external forcing than the self-oscillating neurons with different periods.",
    "MGT": "Circadian oscillators, ubiquitous biological timekeepers, exhibit self-sustained rhythms and are crucial for coordinating physiological processes with the external environment. This article investigates the synchronization and entrainment dynamics of coupled circadian oscillators, focusing on the mechanisms that govern their collective behavior. We develop a mathematical model representing a network of interconnected oscillators, each possessing intrinsic period and amplitude characteristics. The model incorporates various coupling schemes, including diffusive and pulsatile interactions, to mimic different biological scenarios. We analyze the system's response to external periodic forcing, mimicking light-dark cycles, and explore the conditions for stable entrainment.\n\nOur findings reveal that the strength and nature of coupling significantly influence the synchronization properties of the oscillator network. Weak coupling promotes desynchronization or phase dispersion, while strong coupling facilitates robust synchronization. Furthermore, the topology of the network plays a critical role, with global coupling leading to faster synchronization compared to local coupling. The entrainment analysis demonstrates that the ability of the oscillators to lock onto the external forcing signal depends on the forcing amplitude, frequency, and the intrinsic period of the oscillators. Resonance phenomena are observed when the forcing frequency is close to the natural frequency of the oscillators. These results provide insights into the mechanisms underlying circadian rhythm regulation and have implications for understanding and treating circadian-related disorders.\n"
  },
  {
    "id": 64,
    "prompt": "Coherent Cherenkov radio pulses from hadronic showers up to EeV energies",
    "HWT": "The Cherenkov radio pulse emitted by hadronic showers in ice is calculated for showers of energies in the EeV range. This is obtained with three dimensional simulations of both shower development and the coherent radio pulse emitted as the excess charge develops in the shower. A Monte Carlo, ZHAireS, has been developed for this purpose combining the high energy hadronic interaction capabilities of AIRES, and the dense media propagation capabilities of TIERRAS, with the precise low energy tracking and specific algorithms developed to calculate the radio emission in ZHS. A thinning technique is implemented and optimized to allow the simulation of radio pulses induced by showers up to 10 EeV in ice. The code is validated comparing the results for electromagnetic and hadronic showers to those obtained with GEANT4 and ZHS codes. The contribution to the pulse of other shower particles in addition to electrons and positrons, mainly pions and muons, is found to be below 1%. The characteristics of hadronic showers and the corresponding Cherenkov frequency spectra are compared with those from purely electromagnetic showers. The dependence of the spectra on shower energy and high-energy hadronic model is addressed and parameterizations for the radio emission in hadronic showers in ice are given for practical applications.",
    "MGT": "Ultra-high-energy cosmic rays (UHECRs) interacting in dense media, such as ice or rock, produce extensive hadronic showers. These showers emit coherent Cherenkov radiation in the radio frequency range, providing a promising avenue for UHECR detection. This work presents a comprehensive simulation study of the properties of these coherent Cherenkov pulses, focusing on shower energies ranging from 10^17 eV to 10^19 eV (EeV). We employ a detailed Monte Carlo simulation framework incorporating Geant4 to model the shower development and the ZHS algorithm to calculate the radio emission. Our simulations investigate the pulse amplitude, pulse shape, polarization, and spatial distribution as a function of shower energy, depth, and viewing angle. We explore the influence of the refractive index profile of the medium on the propagation and characteristics of the radio signals. We find that the peak amplitude scales approximately linearly with shower energy and that the pulse shape exhibits characteristic features dependent on the shower geometry. The polarization analysis reveals a strong correlation with the shower axis, enabling directional reconstruction. Furthermore, we present an analysis of the uncertainties associated with the simulation parameters, including the hadronic interaction models and the dielectric properties of the medium. The study provides valuable insights for the design and optimization of future radio detectors aimed at probing the UHECR energy spectrum and composition, and it contributes to the understanding of the fundamental physics underlying the emission mechanism. Finally, we discuss the implications of our results for current and planned radio detection experiments, highlighting the potential for improving their sensitivity and angular resolution.\n"
  },
  {
    "id": 65,
    "prompt": "Angular Momentum Transport in Protoplanetary and Black-Hole Accretion Disks: The Role of Parasitic Modes in the Saturation of MHD Turbulence",
    "HWT": "The magnetorotational instability (MRI) is considered a key process for driving efficient angular momentum transport in astrophysical disks. Understanding its non-linear saturation constitutes a fundamental problem in modern accretion disk theory. The large dynamical range in physical conditions in accretion disks makes it challenging to address this problem only with numerical simulations. We analyze the concept that (secondary) parasitic instabilities are responsible for the saturation of the MRI. Our approach enables us to explore dissipative regimes that are relevant to astrophysical and laboratory conditions that lie beyond the regime accessible to current numerical simulations. We calculate the spectrum and physical structure of parasitic modes that feed off the fastest, exact (primary) MRI mode when its amplitude is such that the fastest parasitic mode grows as fast as the MRI. We argue that this \"saturation\" amplitude provides an estimate of the magnetic field that can be generated by the MRI before the secondary instabilities suppress its growth significantly. Recent works suggest that the saturation amplitude of the MRI depends mainly on the magnetic Prandtl number. Our results suggest that, as long as viscous effects do not dominate the fluid dynamics, the saturation level of the MRI depends only on the Elsasser number $\\Lambda_\\eta$. We calculate the ratio between the stress and the magnetic energy density, $\\alpha_{\\rm sat}\\beta_{\\rm sat}$, associated with the primary MRI mode. We find that for $\\Lambda_\\eta >1$ Kelvin-Helmholtz modes are responsible for saturation and $\\alpha_{\\rm sat}\\beta_{\\rm sat} = 0.4$, while for $\\Lambda_\\eta < 1$ tearing modes prevail and $\\alpha_{\\rm sat}\\beta_{\\rm sat} \\simeq 0.5 \\, \\Lambda_\\eta$. Several features of MRI simulations in accretion disks surrounding young stars and compact objects can be interpreted in terms of our findings.",
    "MGT": "Magnetohydrodynamic (MHD) turbulence is widely considered the primary driver of angular momentum transport in protoplanetary and black-hole accretion disks, enabling accretion onto the central object. While the magnetorotational instability (MRI) is a robust source of this turbulence, the precise mechanisms governing its saturation and the resulting angular momentum transport efficiency remain areas of active research. Numerical simulations consistently show that MHD turbulence driven by the MRI saturates at levels far exceeding predictions based on simple mixing-length arguments. This discrepancy suggests the presence of secondary instabilities and non-linear interactions that significantly influence the turbulent cascade and ultimately limit the MRI's effectiveness.\n\nThis paper investigates the role of parasitic instabilities, specifically the elliptical instability (EI) and the subcritical baroclinic instability (SBI), in the saturation of MHD turbulence in accretion disks. We present a series of high-resolution, three-dimensional, non-ideal MHD simulations using the Athena++ code, focusing on shearing box models that capture the local dynamics of the disk. Our simulations incorporate explicit resistivity and viscosity, allowing us to explore the impact of non-ideal effects on the development and saturation of turbulence. We systematically vary the magnetic field strength and the Reynolds and magnetic Reynolds numbers to assess the parametric dependence of the turbulent stress.\n\nOur results demonstrate that parasitic instabilities, triggered by the large-scale coherent structures formed by the MRI, play a crucial role in disrupting these structures and enhancing the turbulent cascade. The EI and SBI act as efficient pathways for transferring energy from the large-scale MRI modes to smaller scales, leading to a more isotropic and homogeneous turbulent state. We find that the saturation level of MHD turbulence is significantly reduced in the presence of these parasitic instabilities, compared to simulations where their growth is suppressed. Furthermore, the angular momentum transport efficiency, quantified by the α parameter, is also reduced, indicating that these secondary instabilities limit the overall accretion rate. We observe a strong correlation between the strength of the parasitic instabilities and the dissipation rates of kinetic and magnetic energy, suggesting that these instabilities contribute significantly to the heating of the disk. Finally, we discuss the implications of our findings for the thermal structure and evolution of protoplanetary and black-hole accretion disks, highlighting the importance of considering parasitic instabilities in future models of disk accretion.\n"
  },
  {
    "id": 66,
    "prompt": "Rebuttal to \"Comment by V. M. Krasnov on 'Counterintuitive consequence of heating in strongly-driven intrinsic junctions of Bi2Sr2CaCu2O8+d Mesas' \"",
    "HWT": "In our article [1], we found that with increasing dissipation there is a clear, systematic shift and sharpening of the conductance peak along with the disappearance of the higher-bias dip/hump features (DHF), for a stack of intrinsic Josephson junctions (IJJs) of intercalated Bi2Sr2CaCu2O8+{\\delta} (Bi2212). Our work agrees with Zhu et al [2] on unintercalated, pristine Bi2212, as both studies show the same systematic changes with dissipation. The broader peaks found with reduced dissipation [1,2] are consistent with broad peaks in the density-of-states (DOS) found among scanning tunneling spectroscopy [3] (STS), mechanical contact tunneling [4] (MCT) and inferred from angle (momentum) resolved photoemission spectroscopy [5] (ARPES); results that could not be ignored. Thus, sharp peaks are extrinsic and cannot correspond to the superconducting DOS. We suggested that the commonality of the sharp peaks in our conductance data, which is demonstrably shown to be heating-dominated, and the peaks of previous intrinsic tunneling spectroscopy (ITS) data implies that these ITS reports might need reinterpretation.",
    "MGT": "This article presents a rebuttal to the comment by V. M. Krasnov regarding our previous publication, \"Counterintuitive consequence of heating in strongly-driven intrinsic junctions of Bi2Sr2CaCu2O8+d Mesas.\" Krasnov's comment raises concerns about the interpretation of our experimental data, specifically questioning the validity of attributing the observed voltage suppression at high driving currents to self-heating effects within the Bi2Sr2CaCu2O8+d mesas. Krasnov proposes an alternative explanation based on non-equilibrium quasiparticle dynamics.\n\nOur rebuttal addresses these concerns by providing a detailed analysis of the experimental evidence supporting our initial interpretation. We re-examine the temperature dependence of the IV characteristics, the power dissipation analysis, and the consistency of our findings with established thermal models for similar mesa structures. Furthermore, we present additional experimental data, including spatially resolved Raman spectroscopy measurements, which directly probe the temperature distribution within the mesas under high current injection. These measurements provide strong evidence for significant self-heating, confirming our original assertion. We demonstrate that while non-equilibrium quasiparticle dynamics may contribute to the observed phenomena, they cannot fully account for the magnitude and behavior of the voltage suppression. Finally, we argue that Krasnov's proposed alternative explanation fails to adequately address several key aspects of our experimental observations, particularly the observed spatial homogeneity of the voltage suppression at high currents. We conclude that self-heating remains the dominant mechanism responsible for the counterintuitive behavior reported in our original publication.\n"
  },
  {
    "id": 67,
    "prompt": "Temperature dependent sound velocity in hydrodynamic equations for relativistic heavy-ion collisions",
    "HWT": "We analyze the effects of different forms of the sound-velocity function cs(T) on the hydrodynamic evolution of matter formed in the central region of relativistic heavy-ion collisions. At high temperatures (above the critical temperature Tc) the sound velocity is calculated from the recent lattice simulations of QCD, while in the low temperature region it is obtained from the hadron gas model. In the intermediate region we use different interpolations characterized by the values of the sound velocity at the local maximum (at T = 0.4 Tc) and local minimum (at T = Tc). In all considered cases the temperature dependent sound velocity functions yield the entropy density, which is consistent with the lattice QCD simulations at high temperature. Our calculations show that the presence of a distinct minimum of the sound velocity leads to a very long (about 20 fm/c) evolution time of the system, which is not compatible with the recent estimates based on the HBT interferometry. Hence, we conclude that the hydrodynamic description is favored in the case where the cross-over phase transition renders the smooth sound velocity function with a possible shallow minimum at Tc.",
    "MGT": "The hydrodynamic description of relativistic heavy-ion collisions relies on an equation of state (EoS) relating thermodynamic quantities. The speed of sound, directly derived from the EoS, plays a crucial role in the evolution of the hydrodynamic fields and influences the final observables. This study investigates the impact of a temperature-dependent speed of sound, $c_s(T)$, on the hydrodynamic evolution of the quark-gluon plasma (QGP) created in these collisions. We employ a (3+1)-dimensional viscous hydrodynamic code, VISH2+1, coupled to the UrQMD hadronic afterburner, to simulate the space-time evolution of the QGP. We explore several parametrizations of $c_s(T)$ motivated by lattice QCD calculations and phenomenological models, focusing on variations around the critical temperature, $T_c$. Our results demonstrate that the temperature dependence of the speed of sound significantly affects the radial and elliptic flow ($v_2$) of produced hadrons. A softer EoS near $T_c$, corresponding to a lower $c_s$, generally leads to larger $v_2$ values. We analyze the sensitivity of different hadron species to the chosen $c_s(T)$ and explore correlations between the speed of sound and other transport coefficients, such as the shear viscosity to entropy density ratio ($\\eta/s$). Furthermore, we compare our simulations to experimental data from RHIC and LHC, providing constraints on the temperature dependence of the speed of sound and its implications for the properties of the QGP. This work highlights the importance of accurately modeling the EoS and its temperature dependence for a comprehensive understanding of the dynamics of relativistic heavy-ion collisions.\n"
  },
  {
    "id": 68,
    "prompt": "Energy spectra of cosmic-ray nuclei at high energies",
    "HWT": "We present new measurements of the energy spectra of cosmic-ray (CR) nuclei from the second flight of the balloon-borne experiment Cosmic Ray Energetics And Mass (CREAM). The instrument included different particle detectors to provide redundant charge identification and measure the energy of CRs up to several hundred TeV. The measured individual energy spectra of C, O, Ne, Mg, Si, and Fe are presented up to $\\sim 10^{14}$ eV. The spectral shape looks nearly the same for these primary elements and it can be fitted to an $E^{-2.66 \\pm 0.04}$ power law in energy. Moreover, a new measurement of the absolute intensity of nitrogen in the 100-800 GeV/$n$ energy range with smaller errors than previous observations, clearly indicates a hardening of the spectrum at high energy. The relative abundance of N/O at the top of the atmosphere is measured to be $0.080 \\pm 0.025 $(stat. )$ \\pm 0.025 $(sys. ) at $\\sim $800 GeV/$n$, in good agreement with a recent result from the first CREAM flight.",
    "MGT": "The energy spectra of cosmic-ray nuclei, particularly protons and helium, are fundamental to understanding the origin, acceleration, and propagation of cosmic rays. This study presents a detailed analysis of the energy spectra of these nuclei at high energies, utilizing data from the Alpha Magnetic Spectrometer (AMS-02) experiment on the International Space Station. We examine the energy range from approximately 50 GeV to several TeV, focusing on the precise measurement of the spectral indices and their variations with energy. The analysis incorporates advanced techniques for particle identification and energy reconstruction, minimizing systematic uncertainties. Our results reveal a hardening of the proton and helium spectra at energies above a few hundred GeV, consistent with previous observations but with significantly improved precision. We investigate the spectral differences between protons and helium, finding that their spectral indices are distinct, suggesting different source populations or propagation effects. Furthermore, we explore the implications of these spectral features for cosmic-ray propagation models, considering the roles of diffusion, convection, and re-acceleration. The observed spectral hardening may indicate the presence of nearby cosmic-ray sources or modifications to the diffusion coefficient at high energies. These findings contribute to a more comprehensive understanding of the cosmic-ray energy spectrum and provide crucial constraints for theoretical models of cosmic-ray origin and propagation.\n"
  },
  {
    "id": 69,
    "prompt": "The enigma of GCIRS 3 - Constraining the properties of the mid-infrared reference star of the central parsec of the Milky Way with optical long baseline interferometry",
    "HWT": "GCIRS3 is the most prominent MIR source in the central pc of the Galaxy. NIR spectroscopy failed to solve the enigma of its nature. The properties of extreme individual objects of the central stellar cluster contribute to our knowledge of star and dust formation close to a supermassive black hole. We initiated an interferometric experiment to understand IRS3 and investigate its properties as spectroscopic and interferometric reference star at 10um. VISIR imaging separates a compact source from diffuse, surrounding emission. The VLTI/MIDI instrument was used to measure visibilities at 10mas resolution of that compact 10um source, still unresolved by a single VLT. Photometry data were added to enable simple SED- and full radiative transfer-models of the data. The luminosity and size estimates show that IRS3 is probably a cool carbon star enshrouded by a complex dust distribution. Dust temperatures were derived. The coinciding interpretation of multiple datasets confirm dust emission at several spatial scales. The IF data resolve the innermost area of dust formation. Despite observed deep silicate absorption towards IRS3 we favor a carbon rich chemistry of the circumstellar dust shell. The silicate absorption most probably takes place in the outer diffuse dust, which is mostly ignored by MIDI measurements. This indicates physically and chemically distinct conditions of the local dust, changing with the distance to IRS3. We have demonstrated that optical long baseline interferometry at infrared wavelengths is an indispensable tool to investigate sources at the Galactic Center. Our findings suggest further studies of the composition of interstellar dust and the shape of the 10um silicate feature at this outstanding region.",
    "MGT": "GCIRS 3, a luminous mid-infrared source in the central parsec of the Milky Way, has long served as a crucial reference star for astronomical observations in this crowded and dynamically complex region. Despite its importance, its fundamental properties remain poorly constrained, hindering precise astrometry and high-resolution imaging efforts. This study presents new optical long baseline interferometry observations of GCIRS 3 obtained with the VLTI/GRAVITY instrument. These measurements provide unprecedented angular resolution, resolving GCIRS 3 and enabling a detailed investigation of its morphology and size.\n\nOur analysis reveals a significantly larger angular diameter for GCIRS 3 than previously estimated from diffraction-limited observations. We derive a uniform disk diameter of 1.65 ± 0.08 milliarcseconds at 2.2 µm. Modeling the spectral energy distribution (SED) of GCIRS 3, incorporating archival near-infrared photometry and our new interferometric measurements, allows us to estimate its effective temperature and luminosity. We find an effective temperature of approximately 3500 K and a luminosity of ~ 1000 Lsun, placing it on the asymptotic giant branch (AGB).\n\nFurthermore, our observations suggest that GCIRS 3 may exhibit a slight elongation, possibly indicating a non-spherical geometry or the presence of a circumstellar disk. We explore various scenarios to explain the observed morphology, including stellar rotation, tidal distortion, and the influence of a close companion. While definitive conclusions require further observations, our findings suggest that GCIRS 3 is a more complex system than previously assumed. This revised understanding of GCIRS 3's properties is crucial for improving the accuracy of astrometric calibrations and enhancing the quality of high-resolution images obtained in the Galactic Center. Future observations, particularly at different wavelengths and epochs, will be essential to fully unravel the enigma of GCIRS 3 and its role as a reference star in the central parsec.\n"
  },
  {
    "id": 70,
    "prompt": "Subvacuum effects of the quantum field on the dynamics of a test particle",
    "HWT": "We study the effects of the electromagnetic subvacuum fluctuations on the dynamics of a nonrelativistic charged particle in a wavepacket. The influence from the quantum field is expected to give an additional effect to the velocity uncertainty of the particle. In the case of a static wavepacket, the observed velocity dispersion is smaller in the electromagnetic squeezed vacuum background than in the normal vacuum background. This leads to the subvacuum effect. The extent of reduction in velocity dispersion associated with this subvacuum effect is further studied by introducing a switching function. It is shown that the slow switching process may make this subvacuum effect insignificant. We also point out that when the center of the wavepacket undergoes non-inertial motion, reduction in the velocity dispersion becomes less effective with its evolution, no matter how we manipulate the nonstationary quantum noise via the choice of the squeeze parameters. The role of the underlying fluctuation-dissipation relation is discussed.",
    "MGT": "The dynamics of a classical test particle interacting with a quantized scalar field in vacuum are investigated, focusing on the subtle influence of subvacuum fluctuations. We explore a regime where the particle's motion is primarily governed by its inertia, with small deviations induced by the quantum field. A perturbative approach is employed to analyze the particle's trajectory and momentum, considering both massless and massive scalar fields. Our calculations reveal that, even in the absence of real particles, the quantum field exerts a non-negligible force on the test particle, manifesting as a stochastic perturbation. This force originates from the continuous creation and annihilation of virtual particles, leading to a fluctuating momentum transfer. The magnitude of these fluctuations depends on the particle's coupling strength to the field, as well as the field's mass. We further analyze the statistical properties of the particle's motion, quantifying the diffusion coefficient and the correlation time associated with the subvacuum-induced fluctuations. The results demonstrate that the test particle exhibits a Brownian-like motion, albeit with unique characteristics stemming from the quantum nature of the driving force. This work provides insights into the fundamental interplay between classical objects and quantum fields, highlighting the observable consequences of vacuum fluctuations.\n"
  },
  {
    "id": 71,
    "prompt": "Quantifying parameter errors due to the peculiar velocities of type Ia supernovae",
    "HWT": "The fitting of the observed redshifts and magnitudes of type Ia supernovae to what we would see in homogeneous cosmological models has led to constraints on cosmological parameters. However, in doing such fits it is assumed that the sampled supernovae are moving with the Hubble flow, i.e. that their peculiar velocities are zero. In reality, peculiar velocities will modify supernova data in a way that can impact best-fit cosmological parameters. We theoretically quantify this effect in the nonlinear regime with a Monte-Carlo analysis, using data from semi-analytic galaxy catalogs that are built from the Millennium N-body simulation. We find scaling relations for the errors in best-fit parameters resulting solely from peculiar velocities, as a function of the total number of sources in a supernova survey N and its maximum redshift z_max. For low redshift surveys, we find that these errors can be of the same order of magnitude as the errors due to an intrinsic magnitude scatter of 0.1 mag. For a survey with N=2000 and z_max=1.7, we estimate that the expected peculiar velocity-induced errors in the best-fit cosmological constant density and equation of state can be sigma_Lambda~0.009 and sigma_w~0.01, respectively, which are subdominant to the errors due to the intrinsic scatter. We further find that throwing away supernova data below a redshift z~0.01-0.02 can reduce the combined error, due to peculiar velocities and the intrinsic scatter, but by only about 10%.",
    "MGT": "Type Ia supernovae (SNe Ia) are crucial cosmological distance indicators, enabling precise measurements of the Universe's expansion history. However, peculiar velocities, deviations from the smooth Hubble flow, introduce systematic uncertainties in distance estimates, ultimately affecting cosmological parameter inference. This study investigates the impact of peculiar velocities on the determination of key cosmological parameters, specifically focusing on the equation of state of dark energy, *w*, and the Hubble constant, *H*<sub>0</sub>. We develop a novel methodology to quantify the bias and increased uncertainty in these parameters resulting from uncorrected peculiar velocity fields. Our approach incorporates a realistic simulation of SNe Ia observations, incorporating both distance modulus errors and a representative peculiar velocity field derived from large-scale structure simulations. We employ a Bayesian framework to marginalize over nuisance parameters associated with the SN Ia light-curve standardization process, while simultaneously fitting for cosmological parameters. We find that neglecting peculiar velocities leads to a statistically significant bias in the inferred value of *w*, particularly at higher redshifts where the peculiar velocity contribution becomes more prominent relative to the Hubble flow. The magnitude of this bias is dependent on the survey depth and the assumed peculiar velocity model. Furthermore, we demonstrate that peculiar velocities inflate the uncertainty in *H*<sub>0</sub>, potentially exacerbating the existing tension between local measurements and those derived from the cosmic microwave background. Our analysis highlights the importance of accurate peculiar velocity modeling and mitigation strategies in future SN Ia cosmology surveys aimed at precisely constraining the nature of dark energy and the Hubble constant. We propose a novel iterative fitting scheme that incorporates a self-consistent peculiar velocity correction, demonstrating its potential to reduce parameter biases and improve the precision of cosmological measurements.\n"
  },
  {
    "id": 72,
    "prompt": "Smallest 90o domains in epitaxial ferroelectric films",
    "HWT": "Ferroelectrics display spontaneous and switchable electrical polarization. Until recently, ferroelectricity was believed to disappear at the nanoscale; now, nano-ferroelectrics are being considered in numerous applications. This renewed interest was partly fuelled by the observation of ferroelectric domains in films of a few unit cells thickness, promising further size reduction of ferroelectric devices. It turns out that at reduced scales and dimensionalities the material's properties depend crucially on the intricacies of domain formation, that is, the way the crystal splits into regions with polarization oriented along the different energetically equivalent directions, typically at 180o and 90o from each other. Here we present a step forward in the manipulation and control of ferroelectric domains by the growth of thin films with regular self-patterned arrays of 90o domains only 7 nm wide. This is the narrowest width for 90o domains in epitaxial ferroelectrics that preserves the film lateral coherence, independently of the substrate.",
    "MGT": "Ferroelectric thin films offer potential for nanoscale devices, but controlling domain size remains a challenge. This study investigates the formation and characteristics of the smallest possible 90° ferroelectric domains in epitaxial thin films, focusing on strain-engineered barium titanate (BaTiO3) grown on strontium titanate (SrTiO3) substrates. Using phase-field simulations, we demonstrate that nanoscale 90° domains, with widths approaching the theoretical limit dictated by the domain wall width, can be stabilized by precisely tuning the epitaxial strain. These minimal domains exhibit unique polarization configurations and enhanced piezoelectric response compared to larger domains. We explore the influence of film thickness and temperature on the domain stability and morphology. Our results reveal that these smallest domains are energetically favorable within a narrow strain window. Furthermore, we confirm these simulation results with piezoresponse force microscopy (PFM) on pulsed laser deposited BaTiO3 thin films, observing domain widths consistent with the predicted nanoscale dimensions. These findings provide insights into manipulating ferroelectric domain structures at the fundamental limit, paving the way for novel ferroelectric-based nanodevices with tailored functionalities.\n"
  },
  {
    "id": 73,
    "prompt": "Contact processes on random graphs with power law degree distributions have critical value 0",
    "HWT": "If we consider the contact process with infection rate $\\lambda$ on a random graph on $n$ vertices with power law degree distributions, mean field calculations suggest that the critical value $\\lambda_c$ of the infection rate is positive if the power $\\alpha>3$. Physicists seem to regard this as an established fact, since the result has recently been generalized to bipartite graphs by G\\'{o}mez-Garde\\~{n}es et al. [Proc. Natl. Acad. Sci. USA 105 (2008) 1399--1404]. Here, we show that the critical value $\\lambda_c$ is zero for any value of $\\alpha>3$, and the contact process starting from all vertices infected, with a probability tending to 1 as $n\\to\\infty$, maintains a positive density of infected sites for time at least $\\exp(n^{1-\\delta})$ for any $\\delta>0$. Using the last result, together with the contact process duality, we can establish the existence of a quasi-stationary distribution in which a randomly chosen vertex is occupied with probability $\\rho(\\lambda)$. It is expected that $\\rho(\\lambda)\\sim C\\lambda^{\\beta}$ as $\\lambda \\to0$. Here we show that $\\alpha-1\\le\\beta\\le2\\alpha-3$, and so $\\beta>2$ for $\\alpha>3$. Thus even though the graph is locally tree-like, $\\beta$ does not take the mean field critical value $\\beta=1$.",
    "MGT": "The contact process, a fundamental model in interacting particle systems, describes the spread of infection on a graph. A site is either infected or healthy, and infected sites recover at rate 1. Infected sites infect each neighboring healthy site at rate λ. The central question is whether the infection can survive indefinitely, which depends on the infection rate λ and the underlying graph structure. We study the contact process on random graphs with power-law degree distributions, specifically those where the degree distribution follows P(k) ~ k^(-τ) for large k. These graphs, often referred to as scale-free networks, are ubiquitous in real-world systems, including social networks, the internet, and biological networks. A key parameter is τ, the power-law exponent, which governs the heterogeneity of the degree distribution.\n\nPrevious work has shown that for τ > 3, the critical infection rate λ_c, below which the infection dies out almost surely, is strictly positive. However, for 2 < τ < 3, it has been conjectured that λ_c = 0, indicating that the infection can survive even for arbitrarily small infection rates. This conjecture arises from the presence of high-degree nodes that can sustain the infection. We provide a rigorous proof of this conjecture. Our main result demonstrates that for the contact process on a random graph with a power-law degree distribution where 2 < τ < 3, the critical infection rate is indeed zero.\n\nOur proof relies on a novel multi-scale analysis that carefully tracks the spread of infection across different scales of the graph. We develop a refined estimate for the probability of long-range infection, taking into account the specific structure of the power-law graph. We show that the presence of hubs, nodes with very high degree, facilitates the spread of infection even for small infection rates. This result has significant implications for understanding the dynamics of epidemics and other spreading phenomena on real-world networks. Furthermore, our techniques provide a general framework for analyzing interacting particle systems on heterogeneous graphs.\n"
  },
  {
    "id": 74,
    "prompt": "Comparison of covariant and orthogonal Lyapunov vectors",
    "HWT": "Two sets of vectors, covariant and orthogonal Lyapunov vectors (CLVs/OLVs), are currently used to characterize the linear stability of chaotic systems. A comparison is made to show their similarity and difference, especially with respect to the influence on hydrodynamic Lyapunov modes (HLMs). Our numerical simulations show that in both Hamiltonian and dissipative systems HLMs formerly detected via OLVs survive if CLVs are used instead. Moreover the previous classification of two universality classes works for CLVs as well, i.e. the dispersion relation is linear for Hamiltonian systems and quadratic for dissipative systems respectively. The significance of HLMs changes in different ways for Hamiltonian and dissipative systems with the replacement of OLVs by CLVs. For general dissipative systems with nonhyperbolic dynamics the long wave length structure in Lyapunov vectors corresponding to near-zero Lyapunov exponents is strongly reduced if CLVs are used instead, whereas for highly hyperbolic dissipative systems the significance of HLMs is nearly identical for CLVs and OLVs. In contrast the HLM significance of Hamiltonian systems is always comparable for CLVs and OLVs irrespective of hyperbolicity. We also find that in Hamiltonian systems different symmetry relations between conjugate pairs are observed for CLVs and OLVs. Especially, CLVs in a conjugate pair are statistically indistinguishable in consequence of the micro- reversibility of Hamiltonian systems. Transformation properties of Lyapunov exponents, CLVs and hyperbolicity under changes of coordinate are discussed in appendices.",
    "MGT": "Lyapunov vectors (LVs) provide valuable insights into the stability and dynamics of chaotic systems. Two primary types exist: covariant Lyapunov vectors (CLVs) and orthogonal Lyapunov vectors (OLVs), each derived from distinct mathematical formalisms. This study offers a comprehensive comparison of these two approaches, focusing on their theoretical underpinnings, computational implementation, and practical applications. CLVs, obtained through a Gram-Schmidt orthogonalization-free method, capture the true directions of instability and stability along a trajectory, revealing the system's underlying covariant Lyapunov exponents. However, their computation can be numerically demanding, particularly for high-dimensional systems. OLVs, generated using iterative orthogonalization procedures, offer a computationally efficient alternative but represent directions orthogonal to the past evolution, potentially masking the true covariant structure. We investigate the relationship between CLVs and OLVs by examining their alignment properties and spectral characteristics in various chaotic systems, including the Lorenz-96 model and a coupled map lattice. Our analysis reveals that while OLVs can approximate the CLVs in certain regimes, significant discrepancies arise in strongly chaotic or spatially extended systems. We further explore the implications of these differences for applications such as data assimilation, reduced-order modeling, and the identification of coherent structures. By highlighting the strengths and limitations of each method, this study provides guidance for selecting the appropriate Lyapunov vector technique based on the specific research question and computational resources available.\n"
  },
  {
    "id": 75,
    "prompt": "Chemical control of orbital polarization in artificially structured transition-metal oxides: La2NiXO6 (X=B, Al, Ga, In) from first principles",
    "HWT": "The application of modern layer-by-layer growth techniques to transition-metal oxide materials raises the possibility of creating new classes of materials with rationally designed correlated electron properties. An important step toward this goal is the demonstration that electronic structure can be controlled by atomic composition. In compounds with partially occupied transition-metal d shells, one important aspect of the electronic structure is the relative occupancy of different d orbitals. Previous work has established that strain and quantum confinement can be used to influence orbital occupancy. In this paper we demonstrate a different modality for orbital control in transition-metal oxide heterostructures, using density-functional band calculations supplemented by a tight-binding analysis to show that the choice of nontransition-metal counterion X in transition-metal oxide heterostructures composed of alternating LaNiO3 and LaXO3 units strongly affects orbital occupancy, changing the magnitude and in some cases the sign of the orbital polarization.",
    "MGT": "We investigate the chemical control of orbital polarization in artificially structured La2NiXO6 (X = B, Al, Ga, In) using density functional theory.  The compounds consist of alternating layers of LaNiO3 and LaXO3, where the choice of X modulates the electronic and orbital properties of the Ni site.  Our calculations reveal a strong dependence of the Ni eg orbital polarization on the chemical identity of X, correlating with the electronegativity difference between Ni and X.  Specifically, we observe a transition from dx2-y2 orbital occupancy for X = B and Al to dz2 occupancy for X = Ga and In.  This transition is driven by the varying degrees of charge transfer and hybridization between the Ni 3d and X p orbitals, influencing the local crystal field splitting at the Ni site.  We analyze the electronic band structures and density of states to elucidate the underlying mechanisms, providing insights into designing materials with tailored orbital polarization for potential applications in correlated electron systems and oxide electronics.  The results demonstrate the feasibility of using chemical substitution to manipulate orbital order in complex oxide heterostructures.\n"
  },
  {
    "id": 76,
    "prompt": "Results and questions on a nonlinear approximation approach for solving high-dimensional partial differential equations",
    "HWT": "We investigate mathematically a nonlinear approximation type approach recently introduced in [A. Ammar et al., J. Non-Newtonian Fluid Mech., 2006] to solve high dimensional partial differential equations. We show the link between the approach and the greedy algorithms of approximation theory studied e.g. in [R. A. DeVore and V. N. Temlyakov, Adv. Comput. Math., 1996]. On the prototypical case of the Poisson equation, we show that a variational version of the approach, based on minimization of energies, converges. On the other hand, we show various theoretical and numerical difficulties arising with the non variational version of the approach, consisting of simply solving the first order optimality equations of the problem. Several unsolved issues are indicated in order to motivate further research.",
    "MGT": "This work explores a nonlinear approximation approach for solving high-dimensional partial differential equations (PDEs), a challenging problem in scientific computing. We investigate the method's performance on several benchmark problems, including high-dimensional heat equations and Black-Scholes equations, demonstrating its capability to overcome the curse of dimensionality to a certain extent. The core idea involves representing the solution using a neural network and training it to satisfy the PDE in a weak sense. We present numerical results showcasing the method's accuracy and efficiency compared to traditional techniques. However, the method also exhibits limitations, particularly in problems with high degrees of nonlinearity or complex boundary conditions. We delve into these limitations, analyzing the factors that contribute to performance degradation, such as the choice of network architecture, optimization algorithms, and sampling strategies. Furthermore, we raise key questions concerning the theoretical underpinnings of the method, including convergence guarantees, error bounds, and generalization capabilities. These open questions highlight the need for further research to fully understand and improve the nonlinear approximation approach for solving high-dimensional PDEs.\n"
  },
  {
    "id": 77,
    "prompt": "On the correct formula for the lifetime broadened superconducting density of states",
    "HWT": "We argue that the well known Dynes formula [Dynes R C {\\it et al. } 1978 {\\it Phys. Rev. Lett. } {\\bf 41} 1509] for the superconducting quasiparticle density of states, which tries to incorporate the lifetime broadening in an approximate way, cannot be justified microscopically for conventional superconductors. Instead, we propose a new simple formula in which the energy gap has a finite imaginary part $-\\Delta_2$ and the quasiparticle energy is real. We prove that in the quasiparticle approximation 2$\\Delta_2$ gives the quasiparticle decay rate at the gap edge for conventional superconductors. This conclusion does not depend on the nature of interactions that cause the quasiparticle decay. The new formula is tested on the case of a strong coupling superconductor Pb$_{0.9}$Bi$_{0.1}$ and an excellent agreement with theoretical predictions is obtained. While both the Dynes formula and the one proposed in this work give good fits and fit parameters for Pb$_{0.9}$Bi$_{0.1}$, only the latter formula can be justified microscopically.",
    "MGT": "The superconducting density of states (DOS) is a fundamental quantity that describes the available electronic states for quasiparticles in a superconductor. In the presence of disorder or other scattering mechanisms, the DOS is broadened, leading to a finite quasiparticle lifetime. While the Bardeen-Cooper-Schrieffer (BCS) theory provides a foundational understanding, accurately capturing the lifetime broadening effects requires careful consideration. This article revisits the established formula for the lifetime broadened superconducting DOS, critically examining its derivation and implicit assumptions. We demonstrate that the commonly used expression, obtained via a simple replacement of the energy variable with a complex energy incorporating a constant broadening parameter, can lead to inaccuracies, particularly in the limit of strong broadening or at energies comparable to the superconducting gap. Through a rigorous Green's function approach, we derive a corrected formula that explicitly accounts for the self-energy effects arising from quasiparticle scattering. This corrected formula is shown to satisfy crucial sum rules and thermodynamic consistency requirements, which are violated by the traditional expression. We present a detailed comparison of the two formulas across a range of broadening parameters and discuss the implications for interpreting experimental measurements, such as tunneling spectroscopy, which are sensitive to the DOS. Furthermore, we explore the impact of the corrected DOS on calculations of thermodynamic properties, highlighting the potential for significant deviations from predictions based on the standard broadened BCS theory. Our findings offer a refined understanding of quasiparticle behavior in disordered superconductors and provide a more accurate framework for analyzing experimental data and theoretical modeling.\n"
  },
  {
    "id": 78,
    "prompt": "2006 SQ372: A Likely Long-Period Comet from the Inner Oort Cloud",
    "HWT": "We report the discovery of a minor planet (2006 SQ372) on an orbit with a perihelion of 24 AU and a semimajor axis of 796 AU. Dynamical simulations show that this is a transient orbit and is unstable on a timescale of 200 Myrs. Falling near the upper semimajor axis range of the scattered disk and the lower semimajor axis range of the Oort Cloud, previous membership in either class is possible. By modeling the production of similar orbits from the Oort Cloud as well as from the scattered disk, we find that the Oort Cloud produces 16 times as many objects on SQ372-like orbits as the scattered disk. Given this result, we believe this to be the most distant long-period comet ever discovered. Furthermore, our simulation results also indicate that 2000 OO67 has had a similar dynamical history. Unaffected by the \"Jupiter-Saturn Barrier, \" these two objects are most likely long-period comets from the inner Oort Cloud.",
    "MGT": "The trans-Neptunian object 2006 SQ372 exhibits an exceptionally high perihelion distance (~24 AU) and a semi-major axis exceeding 1000 AU, placing it on an orbit highly susceptible to perturbations from the galactic tide and passing stars. This study investigates the dynamical origin and long-term evolution of 2006 SQ372, employing N-body simulations to trace its trajectory backward and forward in time. Our backward integrations reveal a high probability that 2006 SQ372 originated from the inner Oort cloud, a region dynamically decoupled from the planetary region but still influenced by stellar encounters and the galactic tide. Forward integrations demonstrate the object's orbital instability, with a significant fraction of clones being ejected from the solar system or scattered inward to become observable long-period comets. We explore the object's physical properties, including its estimated size and albedo, and discuss the implications of its inner Oort cloud origin for understanding the formation and evolution of the outer solar system. The findings suggest that 2006 SQ372 represents a dynamically rare population of objects that serve as a crucial link between the scattered disk and the more distant Oort cloud, providing valuable insights into the early solar system's planetesimal disk.\n"
  },
  {
    "id": 79,
    "prompt": "An Optimal Algorithm to Generate Pointed Trivalent Diagrams and Pointed Triangular Maps",
    "HWT": "A trivalent diagram is a connected, two-colored bipartite graph (parallel edges allowed but not loops) such that every black vertex is of degree 1 or 3 and every white vertex is of degree 1 or 2, with a cyclic order imposed on every set of edges incident to to a same vertex. A rooted trivalent diagram is a trivalent diagram with a distinguished edge, its root. We shall describe and analyze an algorithm giving an exhaustive list of rooted trivalent diagrams of a given size (number of edges), the list being non-redundant in that no two diagrams of the list are isomorphic. The algorithm will be shown to have optimal performance in that the time necessary to generate a diagram will be seen to be bounded in the amortized sense, the bound being independent of the size of the diagrams. That's what we call the CAT property. One objective of the paper is to provide a reusable theoretical framework for algorithms generating exhaustive lists of complex combinatorial structures with attention paid to the case of unlabeled structures and to those generators having the CAT property.",
    "MGT": "This paper introduces an efficient algorithm for generating pointed trivalent diagrams and pointed triangular maps, two fundamental structures in areas like quantum gravity, combinatorics, and theoretical physics. We present a novel approach based on a combination of tree traversal, edge contraction, and combinatorial enumeration techniques. The algorithm is designed to systematically construct all possible diagrams and maps of a given size (number of vertices/faces) without redundancy.\n\nThe core innovation lies in a carefully crafted set of constraints and rules that ensure the generation of only valid and unique structures. We prove the optimality of our algorithm, demonstrating that its time complexity is bounded by a polynomial function of the number of generated objects. Furthermore, we provide empirical evidence supporting the theoretical analysis through extensive computational experiments. These experiments compare our algorithm's performance against existing methods, showcasing significant improvements in terms of speed and memory usage, especially for larger instances.\n\nThe generated datasets of pointed trivalent diagrams and pointed triangular maps are valuable resources for researchers in related fields, facilitating further investigations into their properties and applications. We discuss potential avenues for future research, including extensions to other types of maps and diagrams, as well as applications in the development of new analytical and computational tools.\n"
  },
  {
    "id": 80,
    "prompt": "Gauge invariant approach to low-spin anomalous conformal currents and shadow fields",
    "HWT": "Conformal low-spin anomalous currents and shadow fields in flat space-time of dimension greater than or equal to four are studied. Gauge invariant formulation for such currents and shadow fields is developed. Gauge symmetries are realized by involving Stueckelberg and auxiliary fields. Gauge invariant differential constraints for anomalous currents and shadow fields and realization of global conformal symmetries are obtained. Gauge invariant two-point vertices for anomalous shadow fields are also obtained. In Stueckelberg gauge frame, these gauge invariant vertices become the standard two-point vertices of CFT. Light-cone gauge two-point vertices of the anomalous shadow fields are derived. AdS/CFT correspondence for anomalous currents and shadow fields and the respective normalizable and non-normalizable solutions of massive low-spin AdS fields is studied. The bulk fields are considered in modified de Donder gauge that leads to decoupled equations of motion. We demonstrate that leftover on-shell gauge symmetries of bulk massive fields correspond to gauge symmetries of boundary anomalous currents and shadow fields, while the modified (Lorentz) de Donder gauge conditions for bulk massive fields correspond to differential constraints for boundary anomalous currents and shadow fields.",
    "MGT": "This work explores the construction of gauge-invariant anomalous conformal currents associated with spin-0 and spin-1 conserved currents in conformal field theories (CFTs). We present a systematic method for deriving these anomalous currents by leveraging the properties of shadow fields and their interaction with background gauge fields. Our approach emphasizes maintaining gauge invariance throughout the construction, ensuring consistency with the underlying physical principles of CFTs coupled to external sources.\n\nSpecifically, we focus on the cases where the anomalous currents have low spin, providing explicit expressions for the spin-1 and spin-2 anomalous currents arising from spin-0 and spin-1 conserved currents, respectively. These expressions are derived by carefully analyzing the conformal Ward identities and incorporating the effects of the conformal anomaly. We demonstrate how the shadow formalism naturally incorporates the necessary non-local terms required to ensure gauge invariance of the anomalous currents.\n\nFurthermore, we investigate the relationship between these anomalous currents and the corresponding shadow fields. We show that the anomalous currents can be obtained as specific components of the shadow field equations of motion, providing a deeper understanding of their origin and properties. This connection highlights the crucial role played by shadow fields in understanding the structure of conformal anomalies and their impact on the dynamics of CFTs. Our results provide a valuable framework for studying anomalous transport phenomena and other related aspects of CFTs in various physical contexts.\n"
  },
  {
    "id": 81,
    "prompt": "Low-lying magnetic excitations of doubly-closed-shell nuclei and nucleon-nucleon effective interactions",
    "HWT": "We have studied the low lying magnetic spectra of 12C, 16O, 40Ca, 48Ca and 208Pb nuclei within the Random Phase Approximation (RPA) theory, finding that the description of low-lying magnetic states of doubly-closed-shell nuclei imposes severe constraints on the spin and tensor terms of the nucleon-nucleon effective interaction. We have first made an investigation by using four phenomenological effective interactions and we have obtained good agreement with the experimental magnetic spectra, and, to a lesser extent, with the electron scattering responses. Then we have made self-consistent RPA calculations to test the validity of the finite-range D1 Gogny interaction. For all the nuclei under study we have found that this interaction inverts the energies of all the magnetic states forming isospin doublets.",
    "MGT": "The properties of low-lying magnetic dipole (M1) excitations in doubly-closed-shell nuclei are investigated within the framework of the quasiparticle random phase approximation (QRPA). We employ a separable form of the nucleon-nucleon effective interaction, allowing for a systematic study of the spin-isospin dependence of the M1 response. The focus is on understanding the interplay between different components of the effective interaction and their influence on the energy and strength distribution of these excitations. Detailed calculations are performed for $^{16}$O, $^{40}$Ca, $^{48}$Ca, $^{90}$Zr, and $^{208}$Pb. The results are compared with available experimental data to assess the validity of the effective interaction employed. We find that the spin-isospin terms, particularly the spin-density and spin-current terms, play a crucial role in determining the characteristics of the M1 excitations. The study highlights the sensitivity of these excitations to the details of the effective interaction and provides valuable insights into the nuclear spin response. The observed discrepancies between theoretical predictions and experimental data suggest the need for further refinements in the effective interaction, especially regarding tensor force contributions.\n"
  },
  {
    "id": 82,
    "prompt": "Phase transitions in LaFeAsO: structural, magnetic, elastic, and transport properties, heat capacity and Mossbauer spectra",
    "HWT": "We present results from a detailed experimental investigation of LaFeAsO, the parent material in the series of \"FeAs\" based oxypnictide superconductors. Upon cooling this material undergoes a tetragonal-orthorhombic crystallographic phase transition at ~160 K followed closely by an antiferromagnetic ordering near 145 K. Analysis of these phase transitions using temperature dependent powder X-ray and neutron diffraction measurements is presented. A magnetic moment of ~0.35 Bohr magnetons per iron is derived from Mossbauer spectra in the low temperature phase. Evidence of the structural transition is observed at temperatures well above the structural transition (up to near 200 K) in the diffraction data as well as the polycrystalline elastic moduli probed by resonant ultrasound spectroscopy measurements. The effects of the two phase transitions on the transport properties (resistivity, thermal conductivity, Seebeck coefficient, Hall coefficient), heat capacity, and magnetization of LaFeAsO are also reported, including a dramatic increase in the magnitude of the Hall coefficient below 160 K. The results suggest that the structural distortion leads to a localization of carriers on Fe, producing small local magnetic moments which subsequently order antiferromagnetically upon further cooling. Evidence of strong electron-phonon interactions in the high-temperature tetragonal phase is also observed.",
    "MGT": "The iron-based oxypnictide LaFeAsO exhibits a complex interplay of structural, magnetic, electronic, and vibrational degrees of freedom, leading to a series of phase transitions at low temperatures. This article presents a comprehensive investigation of these transitions using a combination of experimental techniques, including x-ray diffraction, magnetization measurements, elastic modulus determination using resonant ultrasound spectroscopy, electrical resistivity and Hall effect measurements, heat capacity, and Mössbauer spectroscopy. Our high-resolution x-ray diffraction data reveal a tetragonal-to-orthorhombic structural transition around 155 K, accompanied by a pronounced anomaly in the elastic modulus, indicating significant lattice softening. Magnetization measurements confirm the presence of a spin-density-wave (SDW) antiferromagnetic transition at approximately 140 K, characterized by a sharp drop in susceptibility. Electrical resistivity exhibits a clear anomaly at the SDW transition, while the Hall coefficient shows a change in carrier concentration, suggesting a reconstruction of the Fermi surface. Heat capacity measurements reveal a lambda-like anomaly at the magnetic transition, allowing for the determination of the associated entropy change. Furthermore, Mössbauer spectroscopy provides microscopic evidence for the magnetic ordering, with the appearance of a magnetic hyperfine splitting below the Néel temperature. The observed temperature dependence of the hyperfine field is consistent with the development of long-range antiferromagnetic order. The combined results provide a detailed picture of the coupled structural and magnetic transitions in LaFeAsO, highlighting the importance of electron-lattice interactions in driving the observed phenomena.\n"
  },
  {
    "id": 83,
    "prompt": "Controlling exchange bias in Co-CoOx nanoparticles by oxygen content",
    "HWT": "We report on the occurrence of exchange bias on laser-ablated granular thin films composed of Co nanoparticles embedded in amorphous zirconia matrix. The deposition method allows controlling the degree of oxidation of the Co particles by tuning the oxygen pressure at the vacuum chamber (from 2x10^{-5} to 10^{-1} mbar). The nature of the nanoparticles embedded in the nonmagnetic matrix is monitored from metallic, ferromagnetic (FM) Co to antiferromagnetic (AFM) CoOx, with a FM/AFM intermediate regime for which the percentage of the AFM phase can be increased at the expense of the FM phase, leading to the occurrence of exchange bias in particles of about 2 nm in size. For oxygen pressure of about 10-3 mbar the ratio between the FM and AFM phases is optimum with an exchange bias field about 900 Oe at 1.8 K. The mutual exchange coupling between the AFM and FM is also at the origin of the induced exchange anisotropy on the FM leading to high irreversible hysteresis loops, and the blocking of the AFM clusters due to proximity to the FM phase.",
    "MGT": "Exchange bias (EB) in core-shell Co-CoOx nanoparticles is investigated by varying the oxygen partial pressure during the Co oxidation process. The oxygen content significantly influences the structural and magnetic properties of the CoOx shell, subsequently impacting the EB field (HEB). X-ray diffraction (XRD) reveals the formation of different CoOx phases depending on the oxygen pressure. High oxygen partial pressures result in a thicker Co3O4 shell, while lower pressures lead to a predominantly CoO shell. Transmission electron microscopy (TEM) confirms the core-shell morphology and provides insights into the shell thickness. Magnetic measurements demonstrate a clear EB effect, with HEB values tunable by controlling the oxygen content. The largest HEB is observed for intermediate oxygen pressures, corresponding to a mixed CoO/Co3O4 shell composition. This behavior is attributed to the competition between antiferromagnetic CoO and ferrimagnetic Co3O4 contributions to the exchange coupling at the Co-CoOx interface. The blocking temperature (TB) also varies with oxygen content, reflecting changes in the anisotropy of the CoOx shell. These findings demonstrate a facile route for tailoring the EB properties of Co-CoOx nanoparticles through precise control of the oxidation process, offering potential applications in spintronic devices and magnetic sensors. Furthermore, the systematic study of the oxygen content allows for a better understanding of the fundamental mechanisms governing the exchange coupling in these core-shell nanostructures.\n"
  },
  {
    "id": 84,
    "prompt": "Holography of the Quark Matter Triple Point",
    "HWT": "The quark matter phase diagram is believed to contain two distinguished points, lying on the boundary of the Quark-Gluon Plasma phase: a critical point and a triple point. In the holographic [\"AdS/QCD\"] approach, the region of relatively low chemical potentials around the phase transition near the critical point may be described using generalizations of the Hawking-Page transition. We propose that the \\emph{other} QGP phase line, beginning at the triple point and rising towards the region of extremely high temperatures and chemical potentials, is described instead by a non-perturbative string effect discovered by Seiberg and Witten. Using an assumed position for the critical point, we are able to use this proposal to obtain a holographic lower bound on the temperature of the triple point. Combined with Shuryak's upper bound on this temperature, this leads to a rough estimate of the location of the triple point, at a temperature of around 70 MeV, and a chemical potential of about 1100 MeV.",
    "MGT": "The strongly coupled quark-gluon plasma (QGP) created in heavy-ion collisions is theorized to undergo a phase transition to a hadronic gas at lower temperatures and high baryon densities. The point where this transition becomes first-order is known as the critical point, and its existence implies the presence of a triple point where the QGP, hadronic gas, and a possible intermediate quarkyonic phase coexist. Identifying and characterizing this triple point is a major goal of heavy-ion physics. We employ holographic techniques, specifically the Sakai-Sugimoto model, to explore the phase diagram of strongly interacting matter in the presence of finite temperature, baryon chemical potential, and isospin chemical potential. This model, derived from string theory, provides a powerful tool to study non-perturbative aspects of QCD. We map out the phase diagram, identifying the regions corresponding to the QGP, hadronic gas, and quarkyonic matter. We analyze the thermodynamic properties of each phase, including the equation of state and the speed of sound. Our results suggest the existence of a triple point at finite temperature and chemical potentials, and we determine its approximate location in the phase diagram. We further investigate the properties of the system near the triple point, focusing on the fluctuations of conserved charges such as baryon number and isospin. These fluctuations are expected to exhibit critical behavior near the triple point and can potentially be used as experimental signatures in heavy-ion collisions.\n"
  },
  {
    "id": 85,
    "prompt": "Stability of pulsar rotational and orbital periods",
    "HWT": "Millisecond and binary pulsars are the most stable astronomical standards of frequency. They can be applied to solving a number of problems in astronomy and time-keeping metrology including the search for a stochastic gravitational wave background in the early universe, testing general relativity, and establishing a new time-scale. The full exploration of pulsar properties requires that proper unbiased estimates of spin and orbital parameters of the pulsar be obtained. These estimates depend essentially on the random noise components present in pulsar timing residuals. The instrumental white noise has predictable statistical properties and makes no harm for interpretation of timing observations, while the astrophysical/geophysical low-frequency noise corrupts them, thus, reducing the quality of tests of general relativity and decreasing the stability of the pulsar time scale.",
    "MGT": "Pulsars, highly magnetized rotating neutron stars, exhibit remarkably stable rotational periods, making them precise cosmic clocks. This stability is crucial for various astrophysical applications, including gravitational wave detection and tests of general relativity. This study investigates the long-term stability of both pulsar rotational and orbital periods, focusing on a sample of millisecond pulsars (MSPs) in binary systems. We analyze timing data spanning several decades, obtained from radio telescopes worldwide. Our analysis employs advanced statistical techniques to model and remove timing noise, allowing for precise measurements of period derivatives. We present updated measurements of rotational period derivatives for a select group of MSPs, providing tighter constraints on their braking indices and magnetic field strengths. Furthermore, we examine the orbital period stability of binary pulsars, searching for evidence of orbital decay due to gravitational wave emission or other physical mechanisms. Our findings contribute to a deeper understanding of pulsar evolution and the underlying physics governing their stability.\n"
  },
  {
    "id": 86,
    "prompt": "Ad-hoc Limited Scale-Free Models for Unstructured Peer-to-Peer Networks",
    "HWT": "Several protocol efficiency metrics (e.g., scalability, search success rate, routing reachability and stability) depend on the capability of preserving structure even over the churn caused by the ad-hoc nodes joining or leaving the network. Preserving the structure becomes more prohibitive due to the distributed and potentially uncooperative nature of such networks, as in the peer-to-peer (P2P) networks. Thus, most practical solutions involve unstructured approaches while attempting to maintain the structure at various levels of protocol stack. The primary focus of this paper is to investigate construction and maintenance of scale-free topologies in a distributed manner without requiring global topology information at the time when nodes join or leave. We consider the uncooperative behavior of peers by limiting the number of neighbors to a pre-defined hard cutoff value (i.e., no peer is a major hub), and the ad-hoc behavior of peers by rewiring the neighbors of nodes leaving the network. We also investigate the effect of these hard cutoffs and rewiring of ad-hoc nodes on the P2P search efficiency.",
    "MGT": "Unstructured peer-to-peer (P2P) networks offer a decentralized and resilient architecture for resource sharing and data dissemination. However, their reliance on flooding-based search mechanisms often leads to significant network overhead and scalability limitations. Scale-free networks, characterized by a power-law degree distribution, have been shown to enhance search efficiency in structured P2P systems. This paper explores the potential of introducing scale-free properties into unstructured P2P networks on an ad-hoc and limited scale, aiming to improve search performance without compromising the inherent flexibility and simplicity of unstructured designs.\n\nWe propose a novel approach that allows peers to selectively establish preferential connections with higher-degree neighbors based on locally available information. This localized adaptation fosters the emergence of a partial scale-free topology within a limited neighborhood of each peer, mitigating the global restructuring overhead associated with fully structured approaches. Through extensive simulations, we evaluate the performance of our proposed model in terms of search success rate, query latency, and network traffic. Our results demonstrate that even a limited degree of scale-free organization can significantly improve search efficiency in unstructured P2P networks, achieving a substantial reduction in query latency and network traffic compared to traditional flooding-based methods. Furthermore, we analyze the robustness of our model under dynamic network conditions, including peer churn and node failures, highlighting its resilience and adaptability. The findings suggest a promising direction for optimizing unstructured P2P networks by leveraging the benefits of scale-free properties in a decentralized and ad-hoc manner.\n"
  },
  {
    "id": 87,
    "prompt": "DFT-based calculation of Coulomb blockade in molecular junction",
    "HWT": "Quantum transport through single molecules is very sensitive to the strength of the molecule-electrode contact. When a molecular junction weakly coupled to external electrodes, charging effects do play an important role (Coulomb blockade regime). In this regime, the non-equilibrium Green function is usually substituted with master equation approaches, which prevents the density functional theory from describing Coulomb blockade in non-equilibrium case. Last year, we proposed an Ansatz to combine the non-equilibrium Green function technique with the equation of motion method. With help of it, Coulomb blockade was obtained by non-equilibrium Green function, and completely agrees with the master equation results [Phys. Rev. B \\textbf{76}, 045408 (2007)]. Here, by the Ansatz, we show a new way to introduce Coulomb blockade correction to DFT calculation in non-equilibrium case. And the characteristics of Coulomb blockade are obtained in the calculation of a $toy$ molecule correctly.",
    "MGT": "The electronic transport properties of a molecular junction exhibiting Coulomb blockade are investigated using density functional theory (DFT) combined with a master equation approach. The system comprises a single organic molecule bridging two metallic electrodes, where the charging energy of the molecule plays a crucial role in the transport characteristics. Our DFT calculations provide accurate electronic structure information, including the energy levels and coupling strengths between the molecule and the electrodes. These parameters are then used within a master equation framework to model the sequential tunneling of electrons through the molecule. We explore the influence of gate voltage and bias voltage on the current-voltage characteristics, revealing the presence of Coulomb blockade oscillations. The simulations demonstrate a clear suppression of current at low bias voltages, followed by stepwise increases as the bias overcomes the charging energy. Furthermore, we analyze the effect of molecular structure and electrode configuration on the Coulomb blockade behavior. The results highlight the importance of accurately capturing the electronic structure details for understanding and predicting the transport properties of molecular junctions in the Coulomb blockade regime. This approach provides valuable insights for designing and optimizing molecular electronic devices.\n"
  },
  {
    "id": 88,
    "prompt": "A well-posedness theory in measures for some kinetic models of collective motion",
    "HWT": "We present existence, uniqueness and continuous dependence results for some kinetic equations motivated by models for the collective behavior of large groups of individuals. Models of this kind have been recently proposed to study the behavior of large groups of animals, such as flocks of birds, swarms, or schools of fish. Our aim is to give a well-posedness theory for general models which possibly include a variety of effects: an interaction through a potential, such as a short-range repulsion and long-range attraction; a velocity-averaging effect where individuals try to adapt their own velocity to that of other individuals in their surroundings; and self-propulsion effects, which take into account effects on one individual that are independent of the others. We develop our theory in a space of measures, using mass transportation distances. As consequences of our theory we show also the convergence of particle systems to their corresponding kinetic equations, and the local-in-time convergence to the hydrodynamic limit for one of the models.",
    "MGT": "This work establishes a well-posedness theory for a class of kinetic models describing collective motion, specifically focusing on scenarios where the distribution function of agents is a measure. The models under consideration incorporate alignment and repulsion forces, crucial for capturing realistic group behaviors. We address the challenge of measure-valued solutions, which arise naturally when agents exhibit strong aggregation or singular interactions. We introduce a novel notion of weak solutions tailored to measure-valued distributions, relying on duality arguments and test function spaces carefully chosen to handle the singularities inherent in the interaction kernels. This framework allows us to prove the existence of weak solutions using a combination of compactness arguments and a refined version of the method of characteristics. Furthermore, we establish uniqueness of solutions under specific conditions on the interaction kernels, requiring sufficient regularity and decay. The well-posedness results are obtained in a functional space of probability measures, ensuring that the solutions remain physically meaningful throughout their evolution. The stability of solutions with respect to initial data is also investigated, providing insights into the robustness of the model's predictions. Numerical simulations complement the theoretical analysis, illustrating the behavior of measure-valued solutions and validating the theoretical findings.\n"
  },
  {
    "id": 89,
    "prompt": "The origin of a$_{1g}$ and e$_g$' orderings in Na$_x$CoO$_2$",
    "HWT": "It has often been suggested that correlation effects suppress the small e_g' Fermi surface pockets of NaxCoO_2 that are predicted by LDA, but absent in ARPES measurements. It appears that within the dynamical mean field theory (DMFT) the ARPES can be reproduced only if the on-site energy of the eg' complex is lower than that of the a1g complex at the one-electron level, prior to the addition of local correlation effects. Current estimates regarding the order of the two orbital complexes range from -200 meV to 315 meV in therms of the energy difference. In this work, we perform density functional theory calculations of this one-electron splitting \\Delta= \\epsilon_a1g-\\epsilon_e_g' for the full two-layer compound, Na2xCo2O4, accounting for the effects of Na ordering, interplanar interactions and octahedral distortion. We find that \\epsilon a_1g-\\epsilon e_g' is negative for all Na fillings and that this is primarily due to the strongly positive Coulomb field created by Na+ ions in the intercalant plane. This field disproportionately affects the a_1g orbital which protrudes farther upward from the Co plane than the e_g' orbitals. We discuss also the secondary effects of octahedral compression and multi-orbital filling on the value of \\Delta as a function of Na content. Our results indicate that if the e_g' pockets are indeed suppressed that can only be due to nonlocal correlation effects beyond the standard DMFT.",
    "MGT": "The intriguing electronic and magnetic properties of Na$_x$CoO$_2$ have spurred significant research, particularly concerning the origin and interplay of its various ordered phases. This study investigates the electronic structure and orbital ordering in Na$_x$CoO$_2$ using a combination of density functional theory (DFT) and cluster perturbation theory (CPT) calculations, focusing on the emergence of $a_{1g}$ and $e_g'$ orbital orderings. Our analysis reveals that the relative energies and ordering patterns of these orbitals are highly sensitive to the sodium concentration ($x$) and the associated changes in the CoO$_2$ layer's electronic filling. We demonstrate that at low Na concentrations, corresponding to higher Co oxidation states, the crystal field splitting and Hund's exchange interaction favor the occupation of the $a_{1g}$ orbital, leading to an $a_{1g}$ ground state. As $x$ increases and the Co oxidation state decreases, the increased electron-electron repulsion and the enhanced hybridization between the Co $d$ orbitals and the O $p$ orbitals drive a transition towards an $e_g'$ dominated ground state. Furthermore, we explore the role of trigonal distortion in modulating the $a_{1g}$-$e_g'$ energy splitting. Our findings suggest that the trigonal distortion, inherent to the layered structure of Na$_x$CoO$_2$, significantly influences the orbital ordering, contributing to the stabilization of specific phases. We analyze the orbital character of the electronic bands near the Fermi level and discuss the implications of the $a_{1g}$ and $e_g'$ orbital orderings for the observed electronic transport and magnetic properties. The results provide a comprehensive understanding of the electronic structure and orbital ordering in Na$_x$CoO$_2$, shedding light on the origin of its complex phase diagram and offering insights for future materials design.\n"
  },
  {
    "id": 90,
    "prompt": "Optimal power allocation for downstream xDSL with per-modem total power constraints: Broadcast Channel Optimal Spectrum Balancing (BC-OSB)",
    "HWT": "Recently, the duality between Multiple Input Multiple Output (MIMO) Multiple Access Channels (MAC) and MIMO Broadcast Channels (BC) has been established under a total power constraint. The same set of rates for MAC can be achieved in BC exploiting the MAC-BC duality formulas while preserving the total power constraint. In this paper, we describe the BC optimal power allo- cation applying this duality in a downstream x-Digital Subscriber Lines (xDSL) context under a total power constraint for all modems over all tones. Then, a new algorithm called BC-Optimal Spectrum Balancing (BC-OSB) is devised for a more realistic power allocation under per-modem total power constraints. The capacity region of the primal BC problem under per-modem total power constraints is found by the dual optimization problem for the BC under per-modem total power constraints which can be rewritten as a dual optimization problem in the MAC by means of a precoder matrix based on the Lagrange multipliers. We show that the duality gap between the two problems is zero. The multi-user power allocation problem has been solved for interference channels and MAC using the OSB algorithm. In this paper we solve the problem of multi-user power allocation for the BC case using the OSB algorithm as well and we derive a computational efficient algorithm that will be referred to as BC-OSB. Simulation results are provided for two VDSL2 scenarios: the first one with Differential-Mode (DM) transmission only and the second one with both DM and Phantom- Mode (PM) transmissions.",
    "MGT": "This paper addresses the problem of optimal power allocation for downstream xDSL systems, focusing on scenarios with per-modem total power constraints. This is a crucial consideration in practical deployments where individual modems may have limitations on their power consumption due to hardware constraints or regulatory requirements. We propose a novel algorithm, termed Broadcast Channel Optimal Spectrum Balancing (BC-OSB), specifically designed to efficiently solve this power allocation problem. BC-OSB leverages the inherent broadcast channel structure of the downstream xDSL system, formulating the optimization as a multi-user water-filling problem with individual power constraints. The algorithm iteratively adjusts the power spectral density (PSD) of each modem, optimizing the overall system data rate while adhering to the specified power limits.\n\nThe core of BC-OSB involves a decomposition of the original problem into a series of simpler, single-user power allocation problems. Each modem's PSD is updated based on a water-filling solution, considering the interference from other modems and the imposed power constraint. The algorithm incorporates a carefully designed iterative process that ensures convergence to a near-optimal solution. To enhance computational efficiency, we introduce techniques that reduce the complexity of the water-filling calculations, making the algorithm suitable for real-time implementation in DSL access multiplexers (DSLAMs).\n\nSimulation results demonstrate that BC-OSB significantly outperforms existing power allocation schemes, particularly in scenarios with heterogeneous power constraints across different modems. The algorithm exhibits superior data rate performance compared to traditional water-filling approaches that neglect per-modem power limitations. Furthermore, the convergence properties of BC-OSB are analyzed, showing rapid convergence to a stable solution. The robustness of the algorithm is also evaluated under various channel conditions and noise scenarios, confirming its applicability in diverse xDSL deployment environments. These findings highlight the potential of BC-OSB to improve the performance and efficiency of downstream xDSL systems while respecting individual modem power constraints.\n"
  },
  {
    "id": 91,
    "prompt": "Comparing and interpolating distributions on manifold",
    "HWT": "We are interested in comparing probability distributions defined on Riemannian manifold. The traditional approach to study a distribution relies on locating its mean point and finding the dispersion about that point. On a general manifold however, even if two distributions are sufficiently concentrated and have unique means, a comparison of their covariances is not possible due to the difference in local parametrizations. To circumvent the problem we associate a covariance field with each distribution and compare them at common points by applying a similarity invariant function on their representing matrices. In this way we are able to define distances between distributions. We also propose new approach for interpolating discrete distributions and derive some criteria that assure consistent results. Finally, we illustrate with some experimental results on the unit 2-sphere.",
    "MGT": "This article investigates the problem of comparing and interpolating probability distributions defined on Riemannian manifolds. We propose a novel framework that leverages optimal transport theory to define a geometrically meaningful distance between distributions, accounting for the manifold's curvature and intrinsic structure. This distance, derived from the Wasserstein metric, provides a robust measure of dissimilarity, enabling accurate comparison even when distributions exhibit significant differences. Furthermore, we introduce a geodesic interpolation scheme that generates a sequence of intermediate distributions smoothly transitioning between two given distributions along the manifold. This interpolation respects the manifold's geometry, avoiding artifacts commonly observed with Euclidean-based approaches. We demonstrate the efficacy of our framework through several experiments on synthetic and real-world data, including image manifolds and shape spaces. The results highlight the advantages of our approach in capturing complex distributional relationships and generating realistic interpolations, paving the way for applications in areas such as generative modeling, data analysis, and statistical inference on manifolds.\n"
  },
  {
    "id": 92,
    "prompt": "USUM: Update Summary Generation System",
    "HWT": "Huge amount of information is present in the World Wide Web and a large amount is being added to it frequently. A query-specific summary of multiple documents is very helpful to the user in this context. Currently, few systems have been proposed for query-specific, extractive multi-document summarization. If a summary is available for a set of documents on a given query and if a new document is added to the corpus, generating an updated summary from the scratch is time consuming and many a times it is not practical/possible. In this paper we propose a solution to this problem. This is especially useful in a scenario where the source documents are not accessible. We cleverly embed the sentences of the current summary into the new document and then perform query-specific summary generation on that document. Our experimental results show that the performance of the proposed approach is good in terms of both quality and efficiency.",
    "MGT": "The increasing volume of software updates presents a challenge for users seeking to understand the changes introduced. This paper introduces USUM, an automated system designed to generate concise and informative summaries of software updates. USUM leverages a combination of natural language processing techniques, including text summarization and semantic analysis, to extract key information from update release notes, commit messages, and code diffs. The system employs a multi-stage pipeline, first identifying relevant sentences and code changes, then clustering them based on semantic similarity, and finally generating a summary that highlights the most significant modifications. We evaluate USUM on a diverse set of software updates from various projects, comparing its performance against existing summarization methods and human-written summaries. Results demonstrate that USUM produces summaries that are more informative and coherent, capturing the essential changes introduced by the update. Furthermore, a user study reveals that participants find USUM-generated summaries to be more helpful in understanding software updates compared to baseline approaches. The USUM system provides a valuable tool for developers and end-users alike, enabling them to quickly grasp the impact of software updates and make informed decisions about installation.\n"
  },
  {
    "id": 93,
    "prompt": "Twisted generalized Weyl algebras, polynomial Cartan matrices and Serre-type relations",
    "HWT": "Twisted generalized Weyl algebras (TGWAs) are defined as the quotient of a certain graded algebra by the maximal graded ideal I with trivial zero component, analogous to how Kac-Moody algebras can be defined. In this paper we introduce the class of locally finite TGWAs, and show that one can associate to such an algebra a polynomial Cartan matrix (a notion extending the usual generalized Cartan matrices appearing in Kac-Moody algebra theory) and that the corresponding generalized Serre relations hold in the TGWA. We also give an explicit construction of a family of locally finite TGWAs depending on a symmetric generalized Cartan matrix C and some scalars. The polynomial Cartan matrix of an algebra in this family may be regarded as a deformation of the original matrix C and gives rise to quantum Serre relations in the TGWA. We conjecture that these relations generate the graded ideal I for these algebras, and prove it in type A_2.",
    "MGT": "This article investigates a class of non-commutative algebras termed twisted generalized Weyl algebras (TGWAs), focusing on their structural properties and connections to representation theory.  We define TGWAs through a set of generators and relations, generalizing the concept of classical generalized Weyl algebras by introducing a twisting automorphism. The twisting is defined by a polynomial in one variable, which is evaluated at a set of elements spanning the base algebra. We establish necessary and sufficient conditions for these algebras to possess a PBW (Poincaré-Birkhoff-Witt) basis.\n\nFurthermore, we delve into the construction of polynomial Cartan matrices associated with specific TGWAs. These matrices, whose entries are polynomials, encode crucial information about the algebra's structure and its representation theory. We show how the entries of the Cartan matrix can be derived from the defining parameters of the TWGA. We also investigate the Serre-type relations satisfied by the generators of these algebras. These relations provide a concise algebraic description of the algebras and are instrumental in understanding their representation theory.  Finally, the article provides specific examples of TGWAs arising from Lie algebras and quantum groups, demonstrating the applicability of the developed theory in these contexts.  The results contribute to the broader understanding of non-commutative algebras and their connections to representation theory, potentially leading to new insights in areas such as quantum physics and mathematical physics.\n"
  },
  {
    "id": 94,
    "prompt": "Adjusted Bayesian inference for selected parameters",
    "HWT": "We address the problem of providing inference from a Bayesian perspective for parameters selected after viewing the data. We present a Bayesian framework for providing inference for selected parameters, based on the observation that providing Bayesian inference for selected parameters is a truncated data problem. We show that if the prior for the parameter is non-informative, or if the parameter is a \"fixed\" unknown constant, then it is necessary to adjust the Bayesian inference for selection. Our second contribution is the introduction of Bayesian False Discovery Rate controlling methodology, which generalizes existing Bayesian FDR methods that are only defined in the two-group mixture model. We illustrate our results by applying them to simulated data and data froma microarray experiment.",
    "MGT": "This article introduces an adjusted Bayesian inference approach for situations where interest lies in drawing conclusions about a subset of parameters within a larger Bayesian model. Standard Bayesian inference, while powerful, can sometimes produce marginal posteriors for selected parameters that are overly influenced by model misspecification or prior sensitivity in the nuisance parameters. The proposed adjustment modifies the likelihood function to incorporate information about the properties of estimators for the selected parameters, such as bias or frequentist coverage. This adjustment aims to produce more robust and reliable posterior inference for the parameters of interest, even when the full model is not perfectly specified. We demonstrate the methodology through simulation studies and real-world applications, including examples from hierarchical modeling and variable selection. The results suggest that the adjusted Bayesian approach can provide improved performance in terms of frequentist properties and robustness to model misspecification, while retaining the desirable features of Bayesian inference.\n"
  },
  {
    "id": 95,
    "prompt": "Melting of hexane monolayers adsorbed on graphite: the role of domains and defect formation",
    "HWT": "We present the first large-scale molecular dynamics simulations of hexane on graphite that completely reproduces all experimental features of the melting transition. The canonical ensemble simulations required and used the most realistic model of the system: (i) fully atomistic representation of hexane; (ii) explicit site-by-site interaction with carbon atoms in graphite; (iii) CHARMM force field with carefully chosen adjustable parameters of non-bonded interaction; (iv) numerous $\\ge$ 100 ns runs, requiring a total computation time of ca. 10 CPU-years. This has allowed us to determine correctly the mechanism of the transition: molecular reorientation within lamellae without perturbation of the overall adsorbed film structure. We observe that the melted phase has a dynamically reorienting domain-type structure whose orientations reflect that of graphite.",
    "MGT": "The melting transition of hexane monolayers adsorbed on graphite was investigated using molecular dynamics simulations. We explored the structural and thermodynamic properties of the monolayer as a function of temperature, focusing on the interplay between domain formation, defect generation, and the overall melting process. Our simulations reveal that at low temperatures, the hexane molecules form ordered, commensurate domains aligned with the underlying graphite lattice. As the temperature increases, defects such as vacancies and dislocations begin to appear within these domains, disrupting the long-range order. The melting transition is characterized by a gradual increase in the density of these defects, leading to domain fragmentation and eventual disordering of the monolayer. Analysis of the radial distribution function and orientational order parameter confirms the transition from a solid-like to a liquid-like state. Furthermore, we examine the role of substrate corrugation in influencing the melting temperature and the nature of the defects formed. The findings provide insights into the fundamental mechanisms governing the phase behavior of confined organic monolayers and their interactions with solid surfaces.\n"
  },
  {
    "id": 96,
    "prompt": "On $p$-adic Gibbs measures of countable state Potts model on the Cayley tree",
    "HWT": "In the present paper we consider countable state $p$-adic Potts model on the Cayley tree. A construction of $p$-adic Gibbs measures which depends on weights $\\l$ is given, and an investigation of such measures is reduced to examination of an infinite-dimensional recursion equation. Studying of the derived equation under some condition on weights, we prove absence of the phase transition. Note that the condition does not depend on values of the prime $p$, and an analogues fact is not true when the number of spins is finite. For homogeneous model it is shown that the recursive equation has only one solution under that condition on weights. This means that there is only one $p$-adic Gibbs measure $\\m_\\l$. The boundedness of the measure is also established. Moreover, continuous dependence the measure $\\m_\\l$ on $\\l$ is proved. At the end we formulate one limit theorem for $\\m_\\l$.",
    "MGT": "This article investigates $p$-adic Gibbs measures for the countable state Potts model defined on the Cayley tree of order $k \\geq 1$. The Potts model, a generalization of the Ising model, allows for a countable number of spin states, making it a rich and complex system for studying phase transitions and critical phenomena. Our focus is on the $p$-adic analogue, where interactions and spin values are elements of the field of $p$-adic numbers, a non-Archimedean completion of the rational numbers. We establish the existence and uniqueness of $p$-adic Gibbs measures under suitable conditions on the model parameters, particularly the interaction strength and the external field. We employ the Kolmogorov extension theorem and fixed-point arguments in the space of continuous functions defined on the boundary of the Cayley tree to prove our main results. Furthermore, we analyze the structure of the set of $p$-adic Gibbs measures, exploring its dependence on the temperature and the $p$-adic norm of the interaction parameters. The findings contribute to a deeper understanding of the behavior of statistical mechanical models in non-Archimedean settings and provide insights into the connections between $p$-adic analysis and the theory of Gibbs measures. Examples are provided to illustrate the theoretical results.\n"
  },
  {
    "id": 97,
    "prompt": "Pattern formation during the evaporation of a colloidal nanoliter drop: a numerical and experimental study",
    "HWT": "An efficient way to precisely pattern particles on solid surfaces is to dispense and evaporate colloidal drops, as for bioassays. The dried deposits often exhibit complex structures exemplified by the coffee ring pattern, where most particles have accumulated at the periphery of the deposit. In this work, the formation of deposits during the drying of nanoliter colloidal drops on a flat substrate is investigated numerically and experimentally. A finite-element numerical model is developed that solves the Navier-Stokes, heat and mass transport equations in a Lagrangian framework. The diffusion of vapor in the atmosphere is solved numerically, providing an exact boundary condition for the evaporative flux at the droplet-air interface. Laplace stresses and thermal Marangoni stresses are accounted for. The particle concentration is tracked by solving a continuum advection-diffusion equation. Wetting line motion and the interaction of the free surface of the drop with the growing deposit are modeled based on criteria on wetting angles. Numerical results for evaporation times and flow field are in very good agreement with published experimental and theoretical results. We also performed transient visualization experiments of water and isopropanol drops loaded with polystyrene microsphere evaporating on respectively glass and polydimethylsiloxane substrates. Measured evaporation times, deposit shape and sizes, and flow fields are in very good agreement with the numerical results. Different flow patterns caused by the competition of Marangoni loops and radial flow are shown to determine the deposit shape to be either a ring-like pattern or a homogeneous bump.",
    "MGT": "The evaporation of colloidal droplets is a complex phenomenon involving coupled transport processes, leading to a rich variety of self-assembled structures. This study investigates the pattern formation during the evaporation of nanoliter colloidal drops, combining experimental observations with numerical simulations. We focus on the interplay between capillary flow, diffusion, and particle deposition, which governs the final morphology of the dried deposit.\n\nExperimentally, we utilize droplet microfluidics to generate and deposit monodisperse colloidal suspensions of polystyrene particles with controlled size and concentration onto hydrophobic substrates. Time-lapse optical microscopy is employed to track the droplet shape, contact line dynamics, and particle accumulation throughout the evaporation process. These observations reveal distinct stages of pattern evolution, including the initial formation of a coffee-ring structure, followed by the emergence of radial cracks and the subsequent development of complex particle arrangements within the central region of the deposit.\n\nComplementary to the experiments, we develop a three-dimensional numerical model based on the finite element method. The model incorporates the coupled equations of fluid dynamics, diffusion, and particle convection, accounting for the effects of surface tension, evaporation rate, and particle-substrate interactions. The simulation results provide detailed insights into the spatial and temporal evolution of the concentration field, flow velocity, and deposition rate within the evaporating droplet. We find that the competition between the outward capillary flow and the inward diffusion of particles plays a crucial role in determining the final deposit morphology.\n\nBy comparing the experimental observations with the numerical predictions, we establish a quantitative understanding of the underlying mechanisms governing pattern formation in evaporating colloidal nanodrops. Our findings demonstrate that the model accurately captures the key features of the experimental data, including the coffee-ring effect, crack formation, and the spatial distribution of particles within the deposit. This combined experimental and numerical approach provides a powerful tool for optimizing the self-assembly of colloidal particles in nanoliter droplets for various applications, such as microfabrication, biosensing, and drug delivery.\n"
  },
  {
    "id": 98,
    "prompt": "Quantum Radiation of Oscillons",
    "HWT": "Many classical scalar field theories possess remarkable solutions: coherently oscillating, localized clumps, known as oscillons. In many cases, the decay rate of classical small amplitude oscillons is known to be exponentially suppressed and so they are extremely long lived. In this work we compute the decay rate of quantized oscillons. We find it to be a power law in the amplitude and couplings of the theory. Therefore, the quantum decay rate is very different to the classical decay rate and is often dominant. We show that essentially all oscillons eventually decay by producing outgoing radiation. In single field theories the outgoing radiation has typically linear growth, while if the oscillon is coupled to other bosons the outgoing radiation can have exponential growth. The latter is a form of parametric resonance: explosive energy transfer from a localized clump into daughter fields. This may lead to interesting phenomenology in the early universe. Our results are obtained from a perturbative analysis, a non-perturbative Floquet analysis, and numerics.",
    "MGT": "Oscillons, localized, time-dependent solutions in scalar field theories, are typically studied within a classical framework. However, quantum effects can significantly alter their dynamics and stability. This article investigates the quantum radiation of oscillons, focusing on the decay process mediated by the emission of scalar particles. We employ a combination of analytical and numerical techniques to calculate the quantum radiation rate. Specifically, we use the in-in formalism of quantum field theory to derive an expression for the energy loss rate, taking into account the backreaction of the emitted radiation on the oscillon's evolution.\n\nOur analysis reveals that the quantum radiation rate depends sensitively on the oscillon's amplitude and frequency, as well as the mass of the emitted scalar particles. We find that oscillons with smaller amplitudes and frequencies are more susceptible to quantum decay. Furthermore, we explore the parameter space of the scalar field theory to identify regions where quantum radiation dominates over classical decay mechanisms. Numerical simulations, using lattice field theory, corroborate our analytical predictions and provide insights into the long-term evolution of oscillons under the influence of quantum effects. The implications of our findings extend to various areas of physics, including early universe cosmology, particle physics, and condensed matter physics, where oscillon-like structures may play a crucial role.\n"
  },
  {
    "id": 99,
    "prompt": "Experimental Results Related to Discrete Nonlinear Schr\\\"odinger Equations",
    "HWT": "In this chapter, we discuss experiments that realize the discrete nonlinear Schr\\\"odinger (DNLS) equations. The relevance of such descriptions arises from the competition of three common features: nonlinearity, dispersion, and a medium to large level of (periodic, quasiperiodic, or random) discreteness in space. DNLS equations have been especially prevalent in atomic and molecular physics in the study of Bose-Einstein condensates in optical lattices or superlattices; and in nonlinear optics in the description of pulse propagation in waveguide arrays and photorefractive crystals. New experiments in both nonlinear optics and Bose-Einstein condensation provide new challenges for DNLS models, and DNLS and related equations have also recently been used to make important predictions in novel physical settings such as the study of composite metamaterials and arrays of superconducting devices.",
    "MGT": "This work explores the dynamics of discrete nonlinear Schrödinger (DNLS) equations through experimental investigations. We focus on a physical realization of the DNLS equation using weakly coupled optical waveguides fabricated via femtosecond laser inscription in fused silica. By varying the input power and coupling strength, we experimentally observe phenomena predicted by the DNLS model, including discrete diffraction, self-trapping, and the formation of discrete solitons. We present detailed measurements of the beam propagation and output intensity profiles, comparing them with numerical simulations of the DNLS equation. The experimental results demonstrate the crucial role of nonlinearity and discreteness in shaping the wave dynamics. Further, we investigate the stability of the observed discrete solitons against perturbations. These findings provide valuable insights into the fundamental properties of DNLS systems and offer potential applications in all-optical signal processing and manipulation of light in structured media. The experimental platform provides a versatile tool for studying complex nonlinear phenomena in discrete systems.\n"
  },
  {
    "id": 100,
    "prompt": "High-Resolution Rotation Curves and Galaxy Mass Models from THINGS",
    "HWT": "We present rotation curves of 19 galaxies from THINGS, The HI Nearby Galaxy Survey. The high spatial and velocity resolution of THINGS make these the highest quality HI rotation curves available to date for a large sample of nearby galaxies, spanning a wide range of HI masses and luminosities. The high quality of the data allows us to derive the geometrical and dynamical parameters using HI data alone. We do not find any declining rotation curves unambiguously associated with a cut-off in the mass distribution out to the last measured point. The rotation curves are combined with 3.6 um data from SINGS (Spitzer Infrared Nearby Galaxies Survey) to construct mass models. Our best-fit, dynamical disk masses, derived from the rotation curves, are in good agreement with photometric disk masses derived from the 3.6 um images in combination with stellar population synthesis arguments and two different assumptions for the stellar Initial Mass Function (IMF). We test the Cold Dark Matter-motivated cusp model, and the observationally motivated central density core model and find that (independent of IMF) for massive, disk-dominated galaxies, all halo models fit apparently equally well; for low-mass galaxies, however, a core-dominated halo is clearly preferred over a cuspy halo. The empirically derived densities of the dark matter halos of the late-type galaxies in our sample are half of what is predicted by CDM simulations, again independent of the assumed IMF.",
    "MGT": "We present high-resolution rotation curves and mass models for a sample of 19 galaxies from The HI Nearby Galaxy Survey (THINGS). Our analysis leverages high-quality HI data, combined with optical surface photometry, to construct detailed rotation curves extending to radii significantly beyond the optical disk. We meticulously correct for beam smearing effects, radial motions, and asymmetric drift to derive accurate circular velocities. The rotation curves are then decomposed into contributions from the stellar disk, gas disk, and dark matter halo. We adopt a variety of halo models, including pseudo-isothermal (PI) and Navarro-Frenk-White (NFW) profiles, and explore their impact on the derived mass distribution parameters.\n\nOur results reveal a diversity of rotation curve shapes, ranging from slowly rising to nearly flat, and even declining at large radii. We find that the NFW profile provides a good fit to the observed rotation curves for a subset of galaxies, while the PI profile generally offers a better fit across the entire sample, particularly in the inner regions. The dark matter halo parameters, such as the core radius and halo concentration, exhibit a significant galaxy-to-galaxy scatter, reflecting the diverse formation histories and environmental influences of these galaxies. We investigate the correlations between halo parameters and global galaxy properties, such as luminosity, stellar mass, and gas fraction.\n\nFurthermore, we explore the mass discrepancy problem, quantifying the difference between the observed rotation curve and the rotation curve predicted by the baryonic components alone. We find that the dark matter contribution becomes increasingly dominant at larger radii, reinforcing the need for dark matter to explain the observed kinematics. Our high-resolution data allow us to probe the inner regions of the galaxies, where the baryonic and dark matter contributions are comparable, providing valuable constraints on the nature of dark matter and the interplay between baryonic and dark matter in galaxy formation. The derived mass models provide a crucial foundation for future studies of galaxy evolution, dark matter distribution, and the Tully-Fisher relation.\n"
  },
  {
    "id": 101,
    "prompt": "Are Newly Discovered HI High Velocity Clouds Minihalos in the Local Group?",
    "HWT": "A set of HI sources extracted from the north Galactic polar region by the ongoing ALFALFA survey has properties that are consistent with the interpretation that they are associated with isolated minihalos in the outskirts of the Local Group (LG). Unlike objects detected by previous surveys, such as the Compact High Velocity Clouds of Braun & Burton (1999), the HI clouds found by ALFALFA do not violate any structural requirements or halo scaling laws of the LambdaCDM structure paradigm, nor would they have been detected by extant HI surveys of nearby galaxy groups other than the LG. At a distance of d Mpc, their HI masses range between $5 x 10^4 d^2 and 10^6 d^2 solar and their HI radii between <0.4d and 1.6 d kpc. If they are parts of gravitationally bound halos, the total masses would be on order of 10^8--10^9 solar, their baryonic content would be signifcantly smaller than the cosmic fraction of 0.16 and present in a ionized gas phase of mass well exceeding that of the neutral phase. This study does not however prove that the minihalo interpretation is unique. Among possible alternatives would be that the clouds are shreds of the Leading Arm of the Magellanic Stream.",
    "MGT": "The nature of high-velocity clouds (HVCs) surrounding the Milky Way remains a long-standing puzzle in Galactic astronomy. A compelling hypothesis posits that some HVCs are individual dark matter minihalos containing neutral hydrogen (HI), potentially representing the low-mass end of the Local Group galaxy population. This work investigates whether recently discovered HVCs, identified through highly sensitive HI surveys, exhibit properties consistent with minihalo candidates. We analyze the spatial distribution, HI mass, kinematics, and velocity gradients of these newly detected HVCs, comparing them to theoretical predictions for minihaloes in a ΛCDM cosmology. Specifically, we examine their proximity to the Milky Way and M31, searching for evidence of tidal interactions or ram pressure stripping that might influence their observed morphology and kinematics. We also explore the possibility of detecting faint optical counterparts associated with these HVCs, which could provide crucial information about their stellar populations and distances. Furthermore, we investigate the relationship between HI column density and velocity dispersion within these clouds, comparing the observed trends with predictions from models of turbulent, self-gravitating gas within dark matter haloes. Our findings suggest that while some of the observed HVCs exhibit characteristics consistent with minihalos, others are more likely to be tidal debris or gas stripped from larger galaxies. We discuss the implications of these results for understanding the missing satellite problem and the formation and evolution of dwarf galaxies in the Local Group.\n"
  },
  {
    "id": 102,
    "prompt": "The occultation events of the Herbig Ae/Be star V1247 Ori",
    "HWT": "Aims: I study new deep (DeltaV ~ 1.20-1.65 mag) occultation events of the delta Scuti, Herbig Ae/Be star V1247 Ori in the Ori OB1 b association. Methods: I use the V-band ASAS light curve of V1247 Ori, which covers the last nine years, together with photometric data in the near-ultraviolet, visible, near-, and far-infrared taken from the literature. I carry out a periodogram analysis of the \"cleaned\" light curve and construct the spectral energy distribution of the star. Results: The star V1247 Ori is interesting for the study of the UX Orionis phenomenon, in which Herbig Ae/Be stars are occulted by their protoplanetary discs, for three reasons: brightness (V ~ 9.85 mag), large infrared excess at 20-100 mum (F_60 ~ 10 Jy), and photometric stability out of occultation (sigma(V) ~ 0.02 mag), which may help to determine the location and spatial structure of the occulting disc clumps.",
    "MGT": "V1247 Ori, a young Herbig Ae/Be star exhibiting UX Orionis-type variability, was observed photometrically to study the nature and frequency of its occultation events. High-cadence, multi-band observations were conducted over several observing seasons to capture the characteristic deep and irregular fadings indicative of circumstellar obscuration. We present a detailed analysis of these occultation events, focusing on their depths, durations, and color dependencies. Our findings reveal a diverse range of occultation profiles, suggesting a complex circumstellar environment with varying sizes and opacities of the occulting material. We explore potential models for the obscuring structures, including warped inner disks, dust clumps, and planetesimals transiting the star. The color dependencies observed during the occultation events provide insights into the dust grain properties within the occulting material. We discuss the implications of these observations for understanding the disk structure and the planet formation processes around young stars. Furthermore, we compare the observed occultation frequency and characteristics of V1247 Ori with those of other UX Orionis stars to shed light on the evolutionary stage and disk properties of this class of pre-main sequence stars. The results contribute to a better understanding of the complex interplay between circumstellar disks and the formation of planetary systems.\n"
  },
  {
    "id": 103,
    "prompt": "Towards Multimodal Content Representation",
    "HWT": "Multimodal interfaces, combining the use of speech, graphics, gestures, and facial expressions in input and output, promise to provide new possibilities to deal with information in more effective and efficient ways, supporting for instance: - the understanding of possibly imprecise, partial or ambiguous multimodal input; - the generation of coordinated, cohesive, and coherent multimodal presentations; - the management of multimodal interaction (e.g., task completion, adapting the interface, error prevention) by representing and exploiting models of the user, the domain, the task, the interactive context, and the media (e.g. text, audio, video). The present document is intended to support the discussion on multimodal content representation, its possible objectives and basic constraints, and how the definition of a generic representation framework for multimodal content representation may be approached. It takes into account the results of the Dagstuhl workshop, in particular those of the informal working group on multimodal meaning representation that was active during the workshop (see http: //www. dfki. de/~wahlster/Dagstuhl_Multi_Modality, Working Group 4).",
    "MGT": "Multimodal content representation aims to integrate information from diverse modalities, such as text, images, and audio, into a unified and coherent embedding space. This abstract explores novel approaches towards achieving robust and semantically meaningful multimodal representations, focusing on the challenges of modality alignment, fusion strategies, and effective learning objectives. We investigate the limitations of existing methods, which often struggle with capturing subtle inter-modal correlations and handling noisy or incomplete data. To address these issues, we propose a hierarchical fusion architecture that leverages attention mechanisms to dynamically weight the contribution of each modality based on its relevance to the task at hand. Furthermore, we introduce a contrastive learning objective that encourages representations of semantically similar multimodal instances to be closer in the embedding space, while pushing dissimilar instances further apart. Experiments on benchmark datasets for multimodal sentiment analysis and image captioning demonstrate that our approach outperforms state-of-the-art methods in terms of both accuracy and generalization ability. The results highlight the effectiveness of our proposed architecture and learning objective in capturing intricate relationships between modalities, paving the way for more sophisticated multimodal understanding and generation.\n"
  },
  {
    "id": 104,
    "prompt": "Modeling of the evolution of dielectric loss with processing temperature in ferroelectric and dielectric thin oxide films",
    "HWT": "It was experimentally found that the evolution of dielectric loss with processing temperature displays a common trend in ferroelectric and dielectric thin oxide films: firstly an increase and then a decrease in dielectric loss when the processing temperature is gradually raised. Such a dielectric response of ferroelectric/dielectric thin films has been theoretically addressed in this work. We propose that at the initial stage of the crystallization process in thin films, the transformation from amorphous to crystalline phase should increase substantially the dielectric loss; then, with further increase in the processing temperature, the coalescent growth of small crystalline grains into big ones could be helpful in reducing the dielectric loss by lowering grain boundary densities. The obtained experimental data for (Ba, Sr)TiO3 thin films with 500 nm in thickness were analyzed in terms of the model developed and shown to be in a reasonable agreement with the theoretical results.",
    "MGT": "The evolution of dielectric loss in ferroelectric and dielectric thin oxide films during processing is a critical factor impacting device performance and reliability. This work presents a comprehensive model to predict the temperature-dependent dielectric loss behavior in these materials, considering the interplay between intrinsic material properties and extrinsic defect contributions. The model incorporates contributions from oxygen vacancies, grain boundaries, and interfacial layers, each exhibiting distinct temperature dependencies. We utilize a defect chemistry approach to quantify the concentration of oxygen vacancies as a function of processing temperature and ambient oxygen partial pressure. The impact of grain size and interfacial layer thickness on dielectric loss is modeled using effective medium theory. The model is validated against experimental data for strontium titanate (STO) and barium titanate (BTO) thin films processed at various temperatures. Results demonstrate that the model accurately predicts the observed trends in dielectric loss, highlighting the dominant role of oxygen vacancies at elevated temperatures and the influence of grain boundary scattering at lower temperatures. This model provides a valuable tool for optimizing processing conditions to minimize dielectric loss and enhance the performance of ferroelectric and dielectric thin film devices. Further, the model is extended to explore the impact of dopants on the dielectric loss behavior.\n"
  },
  {
    "id": 105,
    "prompt": "The Rate Loss of Single-Letter Characterization: The \"Dirty\" Multiple Access Channel",
    "HWT": "For general memoryless systems, the typical information theoretic solution - when exists - has a \"single-letter\" form. This reflects the fact that optimum performance can be approached by a random code (or a random binning scheme), generated using independent and identically distributed copies of some single-letter distribution. Is that the form of the solution of any (information theoretic) problem? In fact, some counter examples are known. The most famous is the \"two help one\" problem: Korner and Marton showed that if we want to decode the modulo-two sum of two binary sources from their independent encodings, then linear coding is better than random coding. In this paper we provide another counter example, the \"doubly-dirty\" multiple access channel (MAC). Like the Korner-Marton problem, this is a multi-terminal scenario where side information is distributed among several terminals; each transmitter knows part of the channel interference but the receiver is not aware of any part of it. We give an explicit solution for the capacity region of a binary version of the doubly-dirty MAC, demonstrate how the capacity region can be approached using a linear coding scheme, and prove that the \"best known single-letter region\" is strictly contained in it. We also state a conjecture regarding a similar rate loss of single letter characterization in the Gaussian case.",
    "MGT": "The capacity region of the \"dirty\" multiple access channel (DMAC) remains a fundamental open problem in network information theory. This channel, characterized by the presence of non-causal side information known only to one encoder, presents significant challenges in achieving optimal transmission rates. While single-letter characterizations exist for certain special cases, a general single-letter solution remains elusive. This paper investigates the conditions under which single-letter characterizations are lost in the DMAC, focusing on the interplay between the side information structure and the achievable rate region. We demonstrate that the presence of even weak interference can disrupt the optimality of single-letter solutions that hold in simpler scenarios, such as the degraded DMAC. Specifically, we analyze a novel DMAC configuration where the side information, while correlated with one user's message, introduces an additional source of interference for the other user. Through a combination of information-theoretic analysis and novel coding schemes, we establish lower and upper bounds on the achievable rate region. Our results reveal that the optimal coding strategy necessitates a more complex approach than traditional superposition coding, requiring the careful management of interference and the exploitation of the side information to mitigate its negative impact. Furthermore, we show that the loss of single-letter characterization is directly linked to the non-degraded nature of the channel and the inability to effectively orthogonalize the users' signals in the presence of the \"dirty\" interference. The findings provide insights into the limitations of existing single-letter approaches and highlight the need for new analytical tools and coding techniques to fully characterize the capacity region of the general DMAC.\n"
  },
  {
    "id": 106,
    "prompt": "Ionization Equilibrium Timescales in Collisional Plasmas",
    "HWT": "Astrophysical shocks or bursts from a photoionizing source can disturb the typical collisional plasma found in galactic interstellar media or the intergalactic medium. The spectrum emitted by this plasma contains diagnostics that have been used to determine the time since the disturbing event, although this determination becomes uncertain as the elements in the plasma return to ionization equilibrium. A general solution for the equilibrium timescale for each element arises from the elegant eigenvector method of solution to the problem of a non-equilibrium plasma described by Masai (1984) and Hughes & Helfand (1985). In general the ionization evolution of an element Z in a constant electron temperature plasma is given by a coupled set of Z+1 first order differential equations. However, they can be recast as Z uncoupled first order differential equations using an eigenvector basis for the system. The solution is then Z separate exponential functions, with the time constants given by the eigenvalues of the rate matrix. The smallest of these eigenvalues gives the scale of slowest return to equilibrium independent of the initial conditions, while conversely the largest eigenvalue is the scale of the fastest change in the ion population. These results hold for an ionizing plasma, a recombining plasma, or even a plasma with random initial conditions, and will allow users of these diagnostics to determine directly if their best-fit result significantly limits the timescale since a disturbance or is so close to equilibrium as to include an arbitrarily-long time.",
    "MGT": "The establishment of ionization equilibrium in collisional plasmas is a fundamental process underpinning diverse astrophysical and laboratory phenomena. Understanding the timescales associated with this process is crucial for accurately modeling plasma behavior in environments ranging from stellar atmospheres to fusion reactors. This work investigates the temporal evolution of ionization states in collisional plasmas, focusing on the interplay between various atomic processes that govern ionization and recombination. A time-dependent collisional-radiative model is developed, incorporating electron impact ionization, radiative recombination, dielectronic recombination, and charge exchange processes. The model is applied to investigate the ionization balance of several astrophysically relevant elements, including hydrogen, helium, carbon, oxygen, and iron, over a wide range of temperatures and densities characteristic of astrophysical and laboratory plasmas.\n\nDetailed calculations are performed to determine the ionization equilibrium timescales for each element as a function of plasma parameters. The results reveal that ionization timescales vary significantly depending on the element, temperature, and density. At lower temperatures, recombination processes dominate, leading to longer ionization timescales. Conversely, at higher temperatures, electron impact ionization becomes more efficient, resulting in shorter timescales. The density dependence arises from the increased collision frequency, accelerating both ionization and recombination rates. Furthermore, the influence of charge exchange processes on the ionization balance and timescales is investigated, demonstrating their importance in certain temperature regimes, particularly for elements with similar ionization potentials.\n\nThe findings are compared with existing theoretical models and experimental observations, revealing both agreement and discrepancies. The discrepancies highlight the need for improved atomic data and a more comprehensive treatment of relevant atomic processes. The implications of these findings for the interpretation of astrophysical observations and the design of laboratory plasma experiments are discussed. Specifically, the results are used to assess the validity of assuming ionization equilibrium in various astrophysical environments and to provide guidance for the temporal resolution required in laboratory plasma diagnostics. The study underscores the importance of considering non-equilibrium effects when modeling collisional plasmas and provides a valuable tool for predicting the ionization state and its temporal evolution in a wide range of plasma conditions.\n"
  },
  {
    "id": 107,
    "prompt": "Gene regulation in continuous cultures: A unified theory for bacteria and yeasts",
    "HWT": "During batch growth on mixtures of two growth-limiting substrates, microbes consume the substrates either sequentially or simultaneously. These growth patterns are manifested in all types of bacteria and yeasts. The ubiquity of these growth patterns suggests that they are driven by a universal mechanism common to all microbial species. In previous work, we showed that a minimal model accounting only for enzyme induction and dilution explains the phenotypes observed in batch cultures of various wild-type and mutant/recombinant cells. Here, we examine the extension of the minimal model to continuous cultures. We show that: (1) Several enzymatic trends, usually attributed to specific regulatory mechanisms such as catabolite repression, are completely accounted for by dilution. (2) The bifurcation diagram of the minimal model for continuous cultures, which classifies the substrate consumption pattern at any given dilution rate and feed concentrations, provides a a precise explanation for the empirically observed correlation between the growth patterns in batch and continuous cultures. (3) Numerical simulations of the model are in excellent agreement with the data. The model captures the variation of the steady state substrate concentrations, cell densities, and enzyme levels during the single- and mixed-substrate growth of bacteria and yeasts at various dilution rates and feed concentrations. (4) This variation is well-approximated by simple analytical expressions that furnish physical insights into the steady states of continuous cultures. The minimal model provides a framework for quantitating the effect of regulatory mechanisms. We illustrate this by analyzing several data sets from the literature.",
    "MGT": "Continuous culture systems, like chemostats, provide a powerful platform for studying gene regulation under precisely controlled and stable environmental conditions. This allows for the decoupling of growth rate from nutrient availability, enabling researchers to investigate the direct impact of specific environmental factors on gene expression. While extensive research has explored gene regulation in continuous cultures of both bacteria and yeasts, a unified theoretical framework that captures the common underlying principles across these diverse organisms remains elusive. This article proposes such a unified theory, focusing on the interplay between transcriptional regulation, metabolic flux, and cellular resource allocation.\n\nOur theory posits that gene regulation in continuous cultures is fundamentally driven by the cell's need to optimize resource allocation to maximize growth rate under the imposed environmental constraints. We model this optimization problem using a constraint-based approach, where metabolic fluxes are constrained by enzyme capacities, and enzyme synthesis is regulated by transcription factors that respond to intracellular metabolite concentrations. Crucially, the model incorporates the costs associated with enzyme production and maintenance, as well as the effects of nutrient uptake kinetics.\n\nWe demonstrate that this framework can accurately predict the observed gene expression patterns in both bacteria and yeasts across a range of dilution rates and nutrient limitations. Specifically, we show how the model can explain the diauxic shift in yeast, the induction of stress response genes in bacteria under nutrient limitation, and the characteristic oscillatory behavior of certain genes in continuous cultures. Furthermore, we extend the model to incorporate the effects of mutations in key regulatory genes, providing insights into the evolutionary dynamics of gene regulatory networks in continuous culture environments.\n\nThe proposed unified theory provides a powerful tool for understanding and predicting gene regulation in continuous cultures. By integrating transcriptional regulation, metabolic flux, and resource allocation, it offers a more comprehensive understanding of cellular adaptation to changing environmental conditions. This framework has implications for a wide range of applications, including metabolic engineering, synthetic biology, and evolutionary biology.\n"
  },
  {
    "id": 108,
    "prompt": "Phase diagram of chiral quark matter: Fulde-Ferrell pairing from weak to strong coupling",
    "HWT": "We calculate the phase diagram of two-flavor quark matter in the temperature-flavor asymmetry plane in the case where there are three competing phases: the homogeneous Bardeen-Cooper-Schrieffer (BCS) phase, the unpaired phase, and a phase with broken spatial symmetry, which is here taken to be the counterpart of the Fulde-Ferrell (FF) phase in condensed matter physics. We show that the system belongs to the universality class of paramagnetic-ferromagnetic -helical systems. We vary the coupling constant of the theory, which is obtained from integrating out the gluonic degrees of freedom. In weak coupling, the FF phase is favored at arbitrary flavor asymmetries for sufficiently low temperatures; at intermediate coupling its occupancy domain is shifted towards larger asymmetries. Strong coupling features a new regime of an inhomogeneous FF state, which we identify with a current-carrying Bose-Einstein condensate of tightly bound up and down quarks. We analyze the occupation numbers and the Cooper-pair wave function and show that when the condensate momentum is orthogonal to the particle momentum the minority component contains a blocking region (breach) around the Fermi sphere in the weak-coupling limit, which engulfs more low-momentum modes as the coupling is increased, and eventually leads to a topological change in strong coupling, where the minority Fermi sphere contains either two occupied strips or an empty sphere. For non-orthogonal momenta, the blocking region is either reduced or extinct, i.e., no topological changes are observed.",
    "MGT": "We investigate the phase diagram of chiral quark matter at finite temperature and chemical potential, focusing on the emergence and evolution of Fulde-Ferrell (FF) pairing from weak to strong coupling regimes. Utilizing the Nambu-Jona-Lasinio (NJL) model as a representative effective theory for QCD, we incorporate both scalar diquark and pseudoscalar pion condensation channels to capture the interplay between chiral symmetry breaking and Cooper pairing. We extend our analysis beyond the mean-field approximation by including the effects of thermal fluctuations via the Functional Renormalization Group (FRG). This allows us to explore the sensitivity of the FF phase to the strength of the quark interactions and the impact of mesonic correlations.\n\nOur results reveal a rich phase structure characterized by distinct regions dominated by normal quark matter, chiral symmetry broken phases, and various superconducting phases, including the conventional Bardeen-Cooper-Schrieffer (BCS) and FF pairing.  We observe that the FF phase, characterized by a spatially modulated order parameter, is favored at intermediate coupling strengths and temperatures. The inclusion of pion fluctuations via the FRG significantly modifies the phase diagram, suppressing the FF region at lower temperatures and shifting the critical endpoint of the chiral phase transition.\n\nBy systematically varying the coupling strength within the NJL model, we trace the evolution of the FF phase from the weak-coupling regime, where it is driven by Fermi surface mismatch, to the strong-coupling regime, where it is influenced by preformed diquark correlations. Our findings suggest that the presence and extent of the FF phase are sensitive to the relative strength of the scalar diquark and pseudoscalar pion condensation channels, highlighting the importance of considering both chiral and diquark pairing effects in the study of dense quark matter.  These results have implications for understanding the properties of neutron star interiors and heavy-ion collision experiments.\n"
  },
  {
    "id": 109,
    "prompt": "Coulomb gas transitions in three-dimensional classical dimer models",
    "HWT": "Close-packed, classical dimer models on three-dimensional, bipartite lattices harbor a Coulomb phase with power-law correlations at infinite temperature. Here, we discuss the nature of the thermal phase transition out of this Coulomb phase for a variety of dimer models which energetically favor crystalline dimer states with columnar ordering. For a family of these models we find a direct thermal transition from the Coulomb phase to the dimer crystal. While some systems exhibit (strong) first-order transitions in correspondence with the Landau-Ginzburg-Wilson paradigm, we also find clear numerical evidence for continuous transitions. A second family of models undergoes two consecutive thermal transitions with an intermediate paramagnetic phase separating the Coulomb phase from the dimer crystal. We can describe all of these phase transitions in one unifying framework of candidate field theories with two complex Ginzburg-Landau fields coupled to a U(1) gauge field. We derive the symmetry-mandated Ginzburg-Landau actions in these field variables for the various dimer models and discuss implications for their respective phase transitions.",
    "MGT": "We investigate the three-dimensional classical dimer model on the cubic lattice, focusing on the transitions between different phases characterized by distinct Coulomb gas behaviors. Using a combination of Monte Carlo simulations and analytical arguments, we explore the phase diagram as a function of dimer density and interaction strength. We identify a transition from a disordered, gas-like phase at low densities to an ordered, crystalline phase at high densities, mediated by an intermediate phase exhibiting algebraic correlations characteristic of a Coulomb gas. We analyze the critical behavior at the transition points, extracting critical exponents and comparing them with theoretical predictions from renormalization group theory. Our results provide strong evidence for the universality class of these transitions, connecting them to known results for related systems such as the three-dimensional XY model and gauge theories. Furthermore, we examine the role of topological defects, such as dislocations and disclinations, in driving the phase transitions and influencing the Coulomb gas properties. The study advances our understanding of emergent electromagnetism in classical statistical mechanics and highlights the connection between dimer models and other physical systems exhibiting Coulomb gas behavior.\n"
  },
  {
    "id": 110,
    "prompt": "Self-Force Calculations with Matched Expansions and Quasinormal Mode Sums",
    "HWT": "We present the first application of the Poisson-Wiseman-Anderson method of matched expansions, to compute the self-force acting on a point particle moving in a curved spacetime. The method uses two expansions for the Green function, valid in `quasilocal' and `distant past' regimes, which are matched within the normal neighbourhood. We perform our calculation in a static region of the spherically symmetric Nariai spacetime (dS_2 x S^2), on which scalar perturbations are governed by a radial equation with a P\\\"oschl-Teller potential. We combine (i) a very high order quasilocal expansion, and (ii) an expansion in quasinormal modes, to determine the Green function globally. We show it is singular everywhere on the null wavefront (even outside the normal neighbourhood), and apply asymptotic methods to determine its singular structure. We find the Green function undergoes a transition every time the null wavefront passes through a caustic: the singular part follows a repeating four-fold sequence $\\delta(\\sigma)$, $1/\\pi \\sigma$, $-\\delta(\\sigma)$, $-1/\\pi \\sigma$ etc., where $\\sigma$ is Synge's world function. The matched expansion method provides new insight into the non-local properties of the self-force; we find the contribution from the segment of the worldline lying outside the normal neighbourhood is significant. We compute the scalar self-force acting on a static particle, and validate against an alternative method. Finally, we discuss wave propagation on black hole spacetimes (where any expansion in quasinormal modes will be augmented by a branch cut integral) and predict that the Green function in Schwarzschild spacetime will inherit the four-fold singular structure found here.",
    "MGT": "This article presents a novel method for calculating the self-force acting on a small compact object orbiting a massive black hole, combining the strengths of matched asymptotic expansions (MAE) and quasinormal mode (QNM) summation techniques. The self-force, arising from the object's own gravitational field interacting with its motion, is crucial for accurately modeling extreme-mass-ratio inspirals (EMRIs), which are primary targets for space-based gravitational wave observatories. While MAE provides a robust framework for calculating the local field near the small object, it often struggles with convergence issues at late times and requires significant computational resources to achieve high accuracy. Conversely, QNM sums offer an efficient means of approximating the late-time behavior of the field, but they rely on knowledge of the black hole's quasinormal mode spectrum and are less effective in the near zone of the small object.\n\nOur approach leverages the advantages of both methods by employing MAE to compute the regular part of the field in a region surrounding the small object, while simultaneously using QNM sums to approximate the field at a distance from the object. These two solutions are then matched in an intermediate region, allowing us to accurately determine the self-force without the need for computationally expensive global calculations. We develop a consistent matching procedure that ensures the continuity of the metric perturbation and its derivatives across the matching surface. We demonstrate the efficacy of our method by applying it to a scalar charge orbiting a Schwarzschild black hole, comparing our results to those obtained through traditional self-force calculations. Our results show excellent agreement, with our hybrid method achieving comparable accuracy at a significantly reduced computational cost. Furthermore, we explore the limitations of the method and discuss potential avenues for future research, including the application of this technique to more complex scenarios such as Kerr black holes and gravitational perturbations. This hybrid approach offers a promising path towards efficiently and accurately modeling EMRIs, paving the way for more precise gravitational wave data analysis and improved parameter estimation of black hole systems.\n"
  },
  {
    "id": 111,
    "prompt": "Nuclear Structure in 146Gd",
    "HWT": "In this work the experimental study of the 144Sm(alpha, 2n) fusion-evaporation reaction is presented. The experiment was carried out at the IKP of the University of Cologne in order to identify double octupole states and two-particle configurations in the 146Gd nucleus. The target was surrounded by a compact array of nine individual Ge detectors and one EUROBALL CLUSTER detector. A total of 44 new states have been identified, as well as new gamma rays corresponding to 26 previously known levels. Among these levels, new candidates for the two-particle configuration states have been found as well as for the (3- x 2+) and (3- x 3-) two phonon multiplets. A very important results of this work is the unequivocal assignment of the 6- member of the two phonon octupole state in 146Gd by identifying the E3 branching to the one phonon 3- state. This result presents the first conclusive observation of a 6+ to 3- to 0+ double E3 cascade in the decay of a two-phonon octupole state.",
    "MGT": "The nucleus $^{146}$Gd, with its semi-magic nature, provides a crucial testing ground for nuclear models near closed shells. This work presents a comprehensive investigation of the nuclear structure of $^{146}$Gd, employing high-resolution gamma-ray spectroscopy following fusion-evaporation reactions. Excited states in $^{146}$Gd were populated using a $^{100}$Mo($^{48}$Ca,2n)$\\ ^{146}$Gd reaction at a beam energy of 205 MeV. The experiment was performed at the Argonne Tandem Linac Accelerator System (ATLAS) using the Gammasphere array. Precise energy, intensity, and angular correlation measurements were performed to establish a detailed level scheme. Several new transitions and levels were identified, and spin and parity assignments were made based on angular correlation ratios and decay patterns. The experimental level scheme is compared with large-scale shell-model calculations using realistic effective interactions derived from the CD-Bonn nucleon-nucleon potential. The calculations reproduce the low-lying level structure and electromagnetic properties reasonably well, providing insights into the dominant configurations and the role of core excitations. Discrepancies between the experimental data and the theoretical predictions are discussed in terms of possible limitations of the model space and the effective interaction. The results contribute to a better understanding of nuclear forces and the evolution of nuclear structure near the N=82 closed shell.\n"
  },
  {
    "id": 112,
    "prompt": "Incommensurate spin-density wave and magnetic lock-in transition in CaFe4As3",
    "HWT": "The magnetic structure for the newly discovered iron-arsenide compound \\CaFeAs has been studied by neutron powder diffraction. Long-range magnetic order is detected below 85K, with an incommensurate modulation described by the propagation vector k=(0, $\\delta$,0), $\\delta\\sim$ 0.39. Below $\\sim$ 25K, our measurements detect a first-order phase transition where $\\delta$ locks into the commensurate value 3/8. A model of the magnetic structure is proposed for both temperature regimes, based on Rietveld refinements of the powder data and symmetry considerations. The structures correspond to longitudinal spin-density-waves with magnetic moments directed along the \\textit{b}-axis. A Landau analysis captures the change in thermodynamic quantities observed at the two magnetic transitions, in particular the drop in resistivity at the lock-in transition.",
    "MGT": "CaFe4As3 exhibits a complex magnetic ground state characterized by an incommensurate spin-density wave (IC-SDW) that undergoes a lock-in transition to a commensurate SDW (C-SDW) at lower temperatures. This study investigates the nature of this magnetic transition using a combination of neutron diffraction and Mössbauer spectroscopy. Our neutron diffraction measurements reveal the evolution of the IC-SDW propagation vector with temperature, showing a distinct change in slope near the lock-in transition temperature. Mössbauer spectroscopy provides local probe information about the magnetic ordering, confirming the onset of magnetic order at the IC-SDW transition and further revealing changes in the hyperfine field distribution across the lock-in transition. The combined results suggest that the lock-in transition is driven by subtle changes in the electronic structure, potentially involving Fermi surface nesting or changes in the Fe-As bonding environment. These findings provide crucial insights into the interplay between magnetism and structural distortions in CaFe4As3.\n"
  },
  {
    "id": 113,
    "prompt": "The Order of Phase Transitions in Barrier Crossing",
    "HWT": "A spatially extended classical system with metastable states subject to weak spatiotemporal noise can exhibit a transition in its activation behavior when one or more external parameters are varied. Depending on the potential, the transition can be first or second-order, but there exists no systematic theory of the relation between the order of the transition and the shape of the potential barrier. In this paper, we address that question in detail for a general class of systems whose order parameter is describable by a classical field that can vary both in space and time, and whose zero-noise dynamics are governed by a smooth polynomial potential. We show that a quartic potential barrier can only have second-order transitions, confirming an earlier conjecture [1]. We then derive, through a combination of analytical and numerical arguments, both necessary conditions and sufficient conditions to have a first-order vs. a second-order transition in noise-induced activation behavior, for a large class of systems with smooth polynomial potentials of arbitrary order. We find in particular that the order of the transition is especially sensitive to the potential behavior near the top of the barrier.",
    "MGT": "The Kramers problem, concerning escape from a metastable state, is revisited through the lens of phase transitions. Traditionally viewed as a rate process governed by an Arrhenius law at low temperatures, the escape dynamics exhibit more complex behavior as temperature increases. This study investigates the nature of these changes, specifically focusing on the order of phase transitions that occur in the barrier crossing process. We employ a path integral formulation coupled with mean-field theory to analyze the system's behavior across a range of temperatures and barrier heights. Our results reveal a sequence of phase transitions, from a low-temperature regime dominated by single, activated escape events, to a higher-temperature regime where multiple recrossings and complex trajectories become significant. We identify a critical temperature, beyond which the transition from the metastable state becomes qualitatively different, exhibiting characteristics of a first-order phase transition with hysteresis. Furthermore, we explore the influence of noise correlations on the transition dynamics, demonstrating that correlated noise can significantly alter the critical temperature and the order of the phase transitions. The study highlights the limitations of the standard Kramers theory in describing barrier crossing at higher temperatures and emphasizes the importance of considering phase transition phenomena for a comprehensive understanding of escape dynamics in complex systems. These findings have implications for diverse fields, including chemical kinetics, condensed matter physics, and biological systems, where barrier crossing plays a crucial role.\n"
  },
  {
    "id": 114,
    "prompt": "Implication of the observable spectral cutoff energy evolution in XTE J1550-564",
    "HWT": "The physical mechanisms responsible for production of the non-thermal emission in accreting black holes (BH) should be imprinted in the observational apperances of the power law tails in the X-ray spectra from these objects. Different spectral states exhibited by galactic BH binaries allow examination of the photon upscattering under different accretion regimes. We revisit the data collected by Rossi X-ray Timing Explorer (RXTE) from the BH X-ray binary XTE J1550-564 during two periods of X-ray activity in 1998 and 2000 focusing on the behavior of the high energy cutoff of the power law part of the spectrum. For the 1998 outburst the transition from the low-hard state to the intermediate state was accompanied by a gradual decrease in the cutoff energy which then showed an abrupt reversal to a clear increasing trend as the source evolved to the very high and high-soft states. The 2000 outburst showed only the decreasing part of this pattern. Notably, the photon indexes corresponding to the cutoff increase for the 1998 event are much higher than the index values reached during the 2000 rise transition. We attribute this difference in the cutoff energy behavior to the different partial contributions of the thermal and non-thermal (bulk motion) Comptonization in photon upscattering. Namely, during the 1998 event the higher accretion rate presumably provided more cooling to the Comptonizing media and thus reducing the effectiveness of the thermal upscattering process. Under these conditions the bulk motion takes a leading role in boosting the input soft photons. Monte Carlo simulations of the Comptonization in a bulk motion region near an accreting black hole by Laurent & Titarchuk 2010 strongly support this scenario.",
    "MGT": "This study investigates the evolution of the observable spectral cutoff energy (Ecut) in the black hole X-ray binary XTE J1550-564 during its 1998 outburst, utilizing data from the Rossi X-ray Timing Explorer (RXTE). We analyze a comprehensive dataset spanning the outburst's rise, peak, and decline phases, focusing on the temporal variations of Ecut and its relationship with other spectral parameters, such as photon index (Γ) and disk fraction. Our analysis reveals a significant correlation between Ecut and the source's luminosity, indicating a systematic hardening of the X-ray spectrum as the outburst progresses. Specifically, Ecut is found to increase with luminosity during the rising phase, reaching a maximum near the peak of the outburst, followed by a gradual decrease during the decline.\n\nFurthermore, we examine the connection between Ecut and the photon index, observing an anti-correlation in certain spectral states. This suggests that changes in the Comptonizing plasma's temperature and optical depth play a crucial role in shaping the observed X-ray spectrum. We also investigate the behavior of the disk fraction, finding evidence for a geometrically thin, optically thick accretion disk contributing significantly to the soft X-ray emission. The disk fraction tends to decrease as Ecut increases, implying a shift in the dominant emission component from the disk to the Comptonizing corona.\n\nTo gain further insights into the physical processes governing the spectral evolution, we employ spectral modeling techniques, utilizing models that incorporate both thermal Comptonization and disk blackbody emission. These models allow us to estimate the temperature and optical depth of the Comptonizing plasma, as well as the inner disk radius. Our results suggest that the Comptonizing corona undergoes significant changes in its geometry and physical properties throughout the outburst. The implications of these findings for understanding the accretion flow geometry and the physical mechanisms responsible for X-ray emission in black hole binaries are discussed. We further explore the potential role of magnetic fields in shaping the corona and regulating the accretion process. Finally, we compare our results with those obtained for other black hole binaries, highlighting similarities and differences in their spectral behavior.\n"
  },
  {
    "id": 115,
    "prompt": "Time Step Expansions and the Invariant Manifold Approach to Lattice Boltzmann Models",
    "HWT": "The classical method for deriving the macroscopic dynamics of a lattice Boltzmann system is to use a combination of different approximations and expansions. Usually a Chapman-Enskog analysis is performed, either on the continuous Boltzmann system, or its discrete velocity counterpart. Separately a discrete time approximation is introduced to the discrete velocity Boltzmann system, to achieve a practically useful approximation to the continuous system, for use in computation. Thereafter, with some additional arguments, the dynamics of the Chapman-Enskog expansion are linked to the discrete time system to produce the dynamics of the completely discrete scheme. In this paper we put forward a different route to the macroscopic dynamics. We begin with the system discrete in both velocity space and time. We hypothesize that the alternating steps of advection and relaxation, common to all lattice Boltzmann schemes, give rise to a slow invariant manifold. We perform a time step expansion of the discrete time dynamics using the invariance of the manifold. Finally we calculate the dynamics arising from this system. By choosing the fully discrete scheme as a starting point we avoid mixing approximations and arrive at a general form of the microscopic dynamics up to the second order in the time step. We calculate the macroscopic dynamics of two commonly used lattice schemes up to the first order, and hence find the precise form of the deviation from the Navier-Stokes equations in the dissipative term, arising from the discretization of velocity space. Finally we perform a short wave perturbation on the dynamics of these example systems, to find the necessary conditions for their stability.",
    "MGT": "The Lattice Boltzmann Method (LBM) has emerged as a powerful computational tool for simulating fluid dynamics, offering advantages in handling complex geometries and multiphase flows. However, a deeper understanding of its connection to the underlying macroscopic equations is crucial for designing stable and accurate models. This work investigates the relationship between LBM and the Navier-Stokes equations through a rigorous time-step expansion, focusing on the role of invariant manifolds. We present a systematic derivation of the macroscopic equations from the discrete Boltzmann equation, highlighting the approximations inherent in the LBM formulation.\n\nThe core of our analysis lies in exploiting the Chapman-Enskog expansion to derive the macroscopic equations. We demonstrate how the choice of equilibrium distribution and collision operator influences the resulting macroscopic physics. Specifically, we examine the connection between the relaxation parameters in the collision operator and the transport coefficients in the Navier-Stokes equations. We show that the standard LBM models implicitly assume a specific form of the equilibrium distribution, which can limit their accuracy and stability in certain flow regimes.\n\nFurthermore, we explore the concept of invariant manifolds in the context of LBM. The invariant manifold represents the slow, macroscopic dynamics of the system, while the fast, microscopic modes are assumed to relax rapidly to equilibrium. By projecting the discrete Boltzmann equation onto this invariant manifold, we obtain a reduced-order model that captures the essential physics of the flow. We discuss the implications of truncating the higher-order terms in the Chapman-Enskog expansion and their effect on the accuracy of the LBM.\n\nOur analysis provides a framework for designing improved LBM models with enhanced accuracy and stability. We demonstrate how to systematically derive higher-order LBM schemes that minimize the errors associated with the time-step expansion. The insights gained from this study can be applied to develop LBM models for a wider range of fluid dynamics problems, including turbulent flows and multiphase systems. The presented methodology offers a pathway for constructing more accurate and efficient LBM schemes by carefully considering the underlying macroscopic equations and the role of invariant manifolds.\n"
  },
  {
    "id": 116,
    "prompt": "Azimuthal Anisotropy: Ridges, Recombination and Breaking of Quark Number Scaling",
    "HWT": "Azimuthal anisotropy is studied by taking into account the ridges created by semi-hard scattering, which is sensitive to the initial spatial configuration in non-central heavy-ion collisions. No rapid thermalization is required. Although hydrodynamics is not used in this study, the validity of hydrodynamical expansion is not excluded at later time after equilibration is achieved. Phenomenological properties of the bulk and ridge behaviors are used as inputs to determine the elliptic flow of pion and proton at low p_T. At intermediate p_T the recombination of shower partons with thermal partons becomes more important. The phi dependence arises from the variation of the in-medium path length of the hard parton that generates the shower. The p_T dependence of v_2 is therefore very different at intermediate p_T compared to that at low p_T. Quark number scaling of v_2 is shown to be only approximately valid at low p_T, but is broken at intermediate p_T, even though recombination is the mechanism of hadronization in all p_T regions considered.",
    "MGT": "Heavy-ion collisions at relativistic energies create a hot, dense medium known as the quark-gluon plasma (QGP). Azimuthal anisotropy, quantified by the Fourier coefficients $v_n$ of the produced particles' momentum distribution, is a key observable for characterizing the QGP's properties. This work investigates the system size and energy dependence of the second-order ($v_2$) and third-order ($v_3$) anisotropic flow coefficients in Au+Au, Cu+Au, and U+U collisions at $\\sqrt{s_{NN}}$ = 200 GeV and Pb+Pb collisions at $\\sqrt{s_{NN}}$ = 2.76 TeV and 5.02 TeV using a multi-phase transport (AMPT) model. We analyze the centrality dependence of $v_2$ and $v_3$ for identified hadrons, including pions, kaons, protons, lambdas, and omegas. The study also examines the phenomenon of \"ridge\" formation and its connection to the initial geometry. Furthermore, we explore the recombination mechanism's role in hadron formation and its impact on the observed $v_n$. Finally, we investigate the breaking of quark number scaling (NCQ) in $v_n$ at different collision energies and centralities, providing insights into the degree of thermalization and partonic collectivity achieved in these collisions. Our findings contribute to a comprehensive understanding of the QGP's dynamics and properties, offering valuable information for future experimental measurements and theoretical developments.\n"
  },
  {
    "id": 117,
    "prompt": "Accurate structure factors from pseudopotential methods",
    "HWT": "Highly accurate experimental structure factors of silicon are available in the literature, and these provide the ideal test for any \\emph{ab initio} method for the construction of the all-electron charge density. In a recent paper [J. R. Trail and D. M. Bird, Phys. Rev. B {\\bf 60}, 7863 (1999)] a method has been developed for obtaining an accurate all-electron charge density from a first principles pseudopotential calculation by reconstructing the core region of an atom of choice. Here this method is applied to bulk silicon, and structure factors are derived and compared with experimental and Full-potential Linear Augmented Plane Wave results (FLAPW). We also compare with the result of assuming the core region is spherically symmetric, and with the result of constructing a charge density from the pseudo-valence density + frozen core electrons. Neither of these approximations provide accurate charge densities. The aspherical reconstruction is found to be as accurate as FLAPW results, and reproduces the residual error between the FLAPW and experimental results.",
    "MGT": "Accurate structure factors are crucial for understanding and predicting material properties using first-principles calculations. Pseudopotential methods, while computationally efficient, often introduce approximations that can affect the accuracy of calculated structure factors, particularly at high momentum transfer. This work investigates the accuracy of structure factors obtained from various pseudopotential approaches, focusing on the impact of core-valence interactions and the choice of exchange-correlation functional. We present a systematic comparison of structure factors calculated using all-electron methods and different pseudopotentials for a range of materials, including simple metals, semiconductors, and transition metal oxides. Our analysis reveals that norm-conserving pseudopotentials generally provide good agreement with all-electron calculations at low momentum transfer, but deviations become significant at higher values. We demonstrate that the inclusion of nonlinear core corrections and the use of more sophisticated exchange-correlation functionals, such as hybrid functionals, can substantially improve the accuracy of structure factors. Furthermore, we explore the influence of pseudopotential generation parameters, such as the core radius, on the resulting structure factors. Our findings provide valuable insights into the limitations of pseudopotential methods and offer guidelines for selecting appropriate pseudopotentials and computational parameters to obtain accurate structure factors for diverse materials.\n"
  },
  {
    "id": 118,
    "prompt": "Examining the crossover from hadronic to partonic phase in QCD",
    "HWT": "It is argued that, due to the existence of two vacua -- perturbative and physical -- in QCD, the mechanism for the crossover from hadronic to partonic phase is hard to construct. The challenge is: how to realize the transition between the two vacua during the gradual crossover of the two phases. A possible solution of this problem is proposed and a mechanism for crossover, consistent with the principle of QCD, is constructed. The essence of this mechanism is the appearance and growing up of a kind of grape-shape perturbative vacuum inside the physical one. A dynamical percolation model based on a simple dynamics for the delocalization of partons is constructed to exhibit this mechanism. The crossover from hadronic matter to sQGP as well as the transition from sQGP to wQGP in the increasing of temperature is successfully described by using this model with a temperature dependent parameter.",
    "MGT": "The transition from hadronic to partonic degrees of freedom in Quantum Chromodynamics (QCD) is investigated using a variety of theoretical and experimental approaches. This crossover, not a sharp phase transition, is explored through lattice QCD calculations, effective field theories, and heavy-ion collision experiments. Lattice QCD provides crucial insights into the equation of state at finite temperature and chemical potential, revealing a smooth but rapid change in thermodynamic quantities near the pseudocritical temperature. Effective models, such as the Polyakov-loop extended Nambu–Jona-Lasinio (PNJL) model, capture essential features of the deconfinement transition and chiral symmetry restoration. Heavy-ion collisions at RHIC and LHC recreate the extreme conditions necessary to probe the quark-gluon plasma (QGP), the deconfined phase of QCD. Observables such as collective flow, jet quenching, and heavy quark diffusion provide experimental constraints on the properties of the QGP. This article reviews the current understanding of the hadronic-partonic crossover, highlighting the interplay between theoretical predictions and experimental data. Future directions, including the exploration of the QCD phase diagram at high baryon density, are also discussed.\n"
  },
  {
    "id": 119,
    "prompt": "Supersymmetric Leptogenesis with a Light Hidden Sector",
    "HWT": "Supersymmetric scenarios incorporating thermal leptogenesis as the origin of the observed matter-antimatter asymmetry generically predict abundances of the primordial elements which are in conflict with observations. In this paper we propose a simple way to circumvent this tension and accommodate naturally thermal leptogenesis and primordial nucleosynthesis. We postulate the existence of a light hidden sector, coupled very weakly to the Minimal Supersymmetric Standard Model, which opens up new decay channels for the next-to-lightest supersymmetric particle, thus diluting its abundance during nucleosynthesis. We present a general model-independent analysis of this mechanism as well as two concrete realizations, and describe the relevant cosmological and astrophysical bounds and implications for this dark matter scenario. Possible experimental signatures at colliders and in cosmic-ray observations are also discussed.",
    "MGT": "Supersymmetric (SUSY) models with a light hidden sector, accessible through kinetic mixing with the Standard Model (SM) hypercharge, offer a compelling framework for addressing several open questions in particle physics and cosmology. This work explores the impact of such a hidden sector on leptogenesis, a popular mechanism for generating the observed baryon asymmetry of the Universe. We focus on scenarios where the right-handed neutrinos responsible for leptogenesis interact with the hidden sector, potentially altering their decay dynamics and CP violating asymmetries. We demonstrate that the presence of a light hidden sector, consisting of a hidden photon and associated fermions, can significantly enhance the efficiency of leptogenesis, even for relatively small couplings between the right-handed neutrinos and the hidden sector. Furthermore, we investigate the constraints imposed by observations of the effective number of neutrinos, $N_{eff}$, and discuss the implications for the parameter space of the model. Our results highlight the importance of considering hidden sector effects in leptogenesis models and provide a novel avenue for probing SUSY with light hidden sectors.\n"
  },
  {
    "id": 120,
    "prompt": "Switching between Hidden Markov Models using Fixed Share",
    "HWT": "In prediction with expert advice the goal is to design online prediction algorithms that achieve small regret (additional loss on the whole data) compared to a reference scheme. In the simplest such scheme one compares to the loss of the best expert in hindsight. A more ambitious goal is to split the data into segments and compare to the best expert on each segment. This is appropriate if the nature of the data changes between segments. The standard fixed-share algorithm is fast and achieves small regret compared to this scheme. Fixed share treats the experts as black boxes: there are no assumptions about how they generate their predictions. But if the experts are learning, the following question arises: should the experts learn from all data or only from data in their own segment? The original algorithm naturally addresses the first case. Here we consider the second option, which is more appropriate exactly when the nature of the data changes between segments. In general extending fixed share to this second case will slow it down by a factor of T on T outcomes. We show, however, that no such slowdown is necessary if the experts are hidden Markov models.",
    "MGT": "Switching between multiple models is a powerful approach for handling non-stationary time series data. This paper introduces a novel algorithm for switching between Hidden Markov Models (HMMs) using the Fixed Share (FS) online learning framework. Unlike traditional methods that rely on abrupt model switches, the proposed algorithm allows for smooth transitions between HMMs by maintaining a probability distribution over a finite set of candidate models. The Fixed Share algorithm dynamically updates these probabilities based on the observed data, effectively allocating \"shares\" of belief across the different models. This approach allows the algorithm to adapt quickly to changing data patterns while maintaining a degree of robustness against spurious fluctuations.\n\nWe derive the update equations for the FS algorithm in the context of HMMs, explicitly addressing the challenges of parameter estimation and inference within this framework. The algorithm maintains a set of HMMs, each with its own parameters, and uses the observed data to update the weights associated with each model. The update rule incorporates a mixing parameter that controls the rate at which the algorithm adapts to new information. We provide a detailed analysis of the algorithm's computational complexity and demonstrate its scalability to problems with a moderate number of HMMs.\n\nThe performance of the proposed algorithm is evaluated on both synthetic and real-world datasets. Results demonstrate that the FS-HMM algorithm outperforms single HMMs and other switching algorithms in terms of prediction accuracy and adaptation speed. Furthermore, we show that the algorithm is able to effectively capture the underlying dynamics of the data, even in the presence of noise and outliers. The ability to switch smoothly between different HMMs allows the algorithm to adapt to changing data patterns more effectively than methods that rely on abrupt model switches.\n"
  },
  {
    "id": 121,
    "prompt": "Massive gravity and structure formation",
    "HWT": "We study the growth of cosmological perturbations in the model of Lorentz-violating massive gravity. The Friedman equation in this model acquires an unconventional term due to the Lorentz-breaking condensates which has the equation of state w = -1 / (3 gamma) with gamma being a free parameter taking values outside of the range [0,1/3]. Apart from the standard contributions, the perturbations above the Friedmann background contain an extra piece which is proportional to an arbitrary function theta(x) of the space coordinates. This function appears as an integration constant and corresponds to a non-propagating scalar mode which may, however, become dynamical with the account of the higher-derivative corrections. For -1 < gamma < 0 and gamma = 1 the ``anomalous'' perturbations grow slower than the standard ones and thus the model is compatible with observations. Whether the model is experimentally acceptable at other values of \\gamma depends on the value of the function theta(x) at the beginning of the radiation-dominated epoch.",
    "MGT": "Massive gravity offers a compelling alternative to dark energy for explaining the accelerated expansion of the Universe. This article investigates the impact of massive gravity on the formation of large-scale structures, focusing on the Vainshtein mechanism's effectiveness in screening the graviton mass within high-density regions. We explore a specific class of massive gravity models, analyzing the modified Friedmann equations and the linear perturbation theory. The background evolution is constrained by observational data, including the cosmic microwave background, baryon acoustic oscillations, and supernovae Ia.\n\nWe derive the modified growth equation for matter density perturbations, taking into account the effects of the massive graviton. Numerical simulations are performed to solve the growth equation, and the matter power spectrum is computed at various redshifts. Our results indicate that massive gravity can lead to a suppression of power on large scales and an enhancement on small scales, compared to the standard ΛCDM model. We further investigate the implications for the abundance of galaxy clusters and the integrated Sachs-Wolfe effect. The parameter space of the model is constrained by comparing the theoretical predictions with observational data from galaxy surveys. We find that massive gravity can provide a viable explanation for the observed structure formation, while alleviating some of the tensions present in the ΛCDM model.\n"
  },
  {
    "id": 122,
    "prompt": "From high-mass starless cores to high-mass protostellar objects",
    "HWT": "Aims: Our aim is to understand the evolutionary sequence of high-mass star formation from the earliest evolutionary stage of high-mass starless cores, via high-mass cores with embedded low- to intermediate-mass objects, to finally high-mass protostellar objects. Methods: Herschel far-infrared PACS and SPIRE observations are combined with existing data at longer and shorter wavelengths to characterize the spectral and physical evolution of massive star-forming regions. Results: The new Herschel images spectacularly show the evolution of the youngest and cold high-mass star-forming regions from mid-infrared shadows on the Wien-side of the spectral energy distribution (SED), via structures almost lost in the background emission around 100mum, to strong emission sources at the Rayleigh-Jeans tail. Fits of the SEDs for four exemplary regions covering evolutionary stages from high-mass starless cores to high-mass protostellar objects reveal that the youngest regions can be fitted by single-component black-bodies with temperatures on the order of 17K. More evolved regions show mid-infrared excess emission from an additional warmer component, which however barely contributes to the total luminosities for the youngest regions. Exceptionally low values of the ratio between bolometric and submm luminosity additionally support the youth of the infrared-dark sources. Conclusions: The Herschel observations reveal the spectral and physical properties of young high-mass star-forming regions in detail. The data clearly outline the evolutionary sequence in the images and SEDs. Future work on larger samples as well as incorporating full radiative transfer calculations will characterize the physical nature at the onset of massive star formation in even more depth.",
    "MGT": "The formation of high-mass stars (HMSs; >8 M⊙) remains a significant challenge in astrophysics, primarily due to the complexities associated with observing and modeling these rare and distant objects. This study investigates the evolutionary transition from high-mass starless cores (HMSCs) to high-mass protostellar objects (HMPOs), focusing on characterizing the physical and chemical properties of these early stages of HMS formation. We present multi-wavelength observations, spanning from millimeter to near-infrared, targeting a sample of carefully selected HMSCs identified through their cold dust emission and lack of associated mid-infrared emission.\n\nOur analysis combines data from the Atacama Large Millimeter/submillimeter Array (ALMA), the James Clerk Maxwell Telescope (JCMT), and archival data from Spitzer and Herschel. We derived the physical parameters, including gas density, temperature, and velocity structure, using radiative transfer modeling of the dust continuum emission and molecular line observations. Furthermore, we investigated the chemical composition of the cores by analyzing the abundance and distribution of various molecular species, such as CO, HCO+, N2H+, and complex organic molecules (COMs).\n\nThe results reveal a diverse range of physical conditions within the HMSC sample. Some cores exhibit relatively quiescent velocity fields and low temperatures (T < 20 K), indicative of an early stage of collapse. Other cores show evidence of more complex kinematics, including infall motions and velocity gradients, suggesting ongoing accretion and fragmentation. The chemical analysis reveals that CO depletion is common in the denser regions of the cores, while N2H+ is enhanced, consistent with theoretical predictions for cold, dense environments. We also detected tentative evidence for the presence of COMs in some of the cores, suggesting that chemical complexity can develop even in the pre-stellar phase. By comparing the properties of the HMSCs with those of a sample of HMPOs, we aim to provide a comprehensive picture of the evolutionary sequence leading to the formation of HMSs.\n"
  },
  {
    "id": 123,
    "prompt": "Modeling Vacuum Arcs",
    "HWT": "We are developing a model of vacuum arcs. This model assumes that arcs develop as a result of mechanical failure of the surface due to Coulomb explosions, followed by ionization of fragments by field emission and the development of a small, dense plasma that interacts with the surface primarily through self sputtering and terminates as a unipolar arc capable of producing breakdown sites with high enhancement factors. We have attempted to produce a self consistent picture of triggering, arc evolution and surface damage. We are modeling these mechanisms using Molecular Dynamics (mechanical failure, Coulomb explosions, self sputtering), Particle-In-Cell (PIC) codes (plasma evolution), mesoscale surface thermodynamics (surface evolution), and finite element electrostatic modeling (field enhancements). We can present a variety of numerical results. We identify where our model differs from other descriptions of this phenomenon.",
    "MGT": "Vacuum arcs are a complex plasma phenomenon crucial in various applications, including vacuum interrupters and pulsed power devices. This work presents a comprehensive computational model of vacuum arcs, focusing on the dynamic evolution of plasma parameters and electrode erosion. The model employs a hybrid approach, combining a particle-in-cell (PIC) method for simulating the kinetic behavior of charged particles in the plasma sheath regions near the electrodes with a fluid description for the bulk plasma. The model incorporates key physical processes such as thermionic and field emission from the cathode, ionization and excitation of metal vapor, and particle transport within the arc plasma. The simulation results reveal the spatial distribution of plasma density, electron temperature, and ion velocity, providing insights into the arc discharge mechanism. The model is validated against experimental data, demonstrating good agreement in terms of arc voltage and current characteristics. Furthermore, the model is used to investigate the influence of various parameters, such as electrode material and geometry, on the arc behavior and electrode erosion rate. This work contributes to a deeper understanding of vacuum arcs and provides a valuable tool for optimizing the design of vacuum arc-based devices.\n"
  },
  {
    "id": 124,
    "prompt": "Comprehensive Characterization of InGaAs/InP Avalanche Photodiodes at 1550 nm with an Active Quenching ASIC",
    "HWT": "We present an active quenching application specific integrated circuit (ASIC), for use in conjunction with InGaAs/InP avalanche photodiodes (APDs), for 1550 nm single-photon detection. To evaluate its performance, we first compare its operation with that of standard quenching electronics. We then test 4 InGaAs/InP APDs using the ASIC, operating both in the free-running and gated modes, to study more general behavior. We investigate not only the standard parameters under different working conditions but also parameters such as charge persistence and quenching time. We also use the multiple trapping model to account for the afterpulsing behavior in the gated mode, and further propose a model to take account of the afterpulsing effects in the free-running mode. Our results clearly indicate that the performance of APDs with an on-chip quenching circuit significantly surpasses the conventional quenching electronics, and makes them suitable for practical applications, e.g., quantum cryptography.",
    "MGT": "This paper presents a comprehensive characterization of InGaAs/InP avalanche photodiodes (APDs) at 1550 nm, integrated with a custom-designed active quenching application-specific integrated circuit (ASIC). The APDs, crucial components in high-sensitivity optical receivers, are evaluated for their performance metrics including breakdown voltage, dark current, responsivity, gain, and noise characteristics.  The active quenching circuit, implemented in a 0.18 µm CMOS process, is designed to effectively suppress afterpulsing effects and enable high-speed single-photon detection.  Detailed measurements of the APD's dark current are performed across a range of temperatures, revealing the dominant generation-recombination mechanisms. Furthermore, the responsivity and gain are characterized as a function of reverse bias voltage, demonstrating the APD's capability for high signal amplification.  The noise performance is assessed through measurements of the excess noise factor, providing insights into the impact ionization statistics within the APD's avalanche region.  Finally, the integration with the active quenching ASIC is shown to significantly improve the APD's single-photon detection efficiency and reduce the afterpulsing probability, enabling high-performance operation in demanding applications such as quantum key distribution and LiDAR systems. The results demonstrate the potential of this integrated APD-ASIC solution for achieving high sensitivity and low noise performance at 1550 nm.\n"
  },
  {
    "id": 125,
    "prompt": "Coexistence between superconducting and spin density wave states in iron-based superconductors: Ginzburg-Landau analysis",
    "HWT": "We consider the interplay between superconducting (SC) and commensurate spin-density-wave (SDW) orders in iron-pnictides by analyzing a multiple order Ginzburg-Landau free energy. We are particularly interested in whether the doping-induced transition between the two states is first order, or the two pure phases are separated by an intermediate phase with coexisting SC and SDW orders. For perfect nesting, the two orders do not coexist, because SDW order, which comes first, gaps the full Fermi surface leaving no space for SC to develop. When nesting is not perfect due to either ellipticity of electron bands or doping-induced difference in chemical potentials for holes and electrons, SDW order still leaves modified Fermi surfaces for not too strong SDW magnetism and the SC order may develop. We show that the two orders coexist only when certain relations between ellipticity and doping are met. In particular, in a compensated metal, ellipticity alone is not sufficient for coexistence of the two orders.",
    "MGT": "The interplay between superconductivity (SC) and spin density wave (SDW) order in iron-based superconductors remains a central puzzle. We investigate this coexistence using a two-band Ginzburg-Landau (GL) formalism, focusing on the spatial structure of the SC and SDW order parameters. Our model incorporates the competition between SC and SDW order through interband and intraband coupling terms, allowing for various relative phases between the two order parameters. We analyze the free energy landscape to determine the stable ground state configurations, considering both homogeneous and spatially modulated phases. We find that the relative strength of the coupling constants dictates whether the SC and SDW orders coexist uniformly, form spatially separated domains, or exhibit more complex patterns. Specifically, we explore the emergence of a novel \"striped\" phase where SC and SDW orders alternate periodically. Furthermore, we examine the effects of external magnetic fields on the coexistence phase, predicting the evolution of the order parameter profiles and the potential for field-induced transitions. Our results provide insights into the microscopic mechanisms governing the competition and cooperation between SC and SDW order, offering a framework for understanding the diverse phase diagrams observed in iron-based superconductors.\n"
  },
  {
    "id": 126,
    "prompt": "A Constructive Generalization of Nash Equilibrium for Better Payoffs and Stability",
    "HWT": "In a society of completely selfish individuals where everybody is only interested in maximizing his own payoff, does any equilibrium exist for the society? John Nash proved more than 50 years ago that an equilibrium always exists such that nobody would benefit from unilaterally changing his strategy. Nash Equilibrium is a central concept in game theory, which offers a mathematical foundation for social science and economy. However, it is important from both a theoretical and a practical point of view to understand game playing where individuals are less selfish. This paper offers a constructive generalization of Nash equilibrium to study n-person games where the selfishness of individuals can be defined at any level, including the extreme of complete selfishness. The generalization is constructive since it offers a protocol for individuals in a society to reach an equilibrium. Most importantly, this paper presents experimental results and theoretical investigation to show that the individuals in a society can reduce their selfishness level together to reach a new equilibrium where they can have better payoffs and the society is more stable at the same time. This study suggests that, for the benefit of everyone in a society (including the financial market), the pursuit of maximal payoff by each individual should be controlled at some level either by voluntary good citizenship or by imposed regulations.",
    "MGT": "Nash equilibrium, a cornerstone of game theory, often leads to suboptimal outcomes and lacks robustness in dynamic environments. This paper introduces a constructive generalization of Nash equilibrium, termed \"Cooperative Nash Equilibrium (CNE),\" designed to enhance both individual payoffs and overall system stability. CNE leverages the concept of temporary, incentive-compatible agreements among a subset of players to achieve mutually beneficial outcomes. Unlike traditional cooperative game theory, these agreements are not binding and players retain the option to deviate back to their individual strategies if the cooperative arrangement becomes disadvantageous.\n\nWe develop a novel algorithm for constructing CNE by iteratively identifying potential coalitions and evaluating their potential payoff improvements. The algorithm incorporates a stability criterion that ensures the formed coalitions are resistant to unilateral deviations, promoting long-term adherence to the cooperative agreement. We rigorously analyze the theoretical properties of CNE, demonstrating its existence under mild conditions and proving its Pareto improvement over Nash equilibrium in a wide range of game scenarios.\n\nFurthermore, we investigate the dynamics of CNE in repeated games, showing that it fosters a more cooperative and stable environment compared to traditional Nash equilibrium. Through extensive simulations across diverse game settings, including the Prisoner's Dilemma, the Public Goods Game, and Cournot competition, we demonstrate the superior performance of CNE in terms of both individual payoffs and system-wide welfare. The results highlight the potential of CNE to address the limitations of Nash equilibrium in real-world applications, offering a more robust and efficient framework for strategic decision-making. Finally, we discuss the implications of CNE for mechanism design and the development of cooperative AI agents.\n"
  },
  {
    "id": 127,
    "prompt": "High energy emission and polarisation limits for the INTEGRAL burst GRB 061122",
    "HWT": "(Abridged) GRB 061122 is one of the brightest GRBs detected within INTEGRAL's field of view to date. The two gamma-ray detectors on INTEGRAL were used to investigate the spectral characteristics of GRB 061122. A search for linear polarisation in the prompt emission was carried out using the SPI multiple event data in the energy range 100 keV-1 MeV. The prompt spectrum was best fit by a combination of a blackbody and a power--law model (the quasithermal model), with evidence for high energy emission continuing above 8 MeV. A pseudo-redshift value of pz = 0.95 +/- 0.18 was determined using the spectral fit parameters. The isotropic energy at this pseudo-redshift is 8.5 x 10^{52} erg. The jet opening angle was estimated to be smaller than 2.8 deg or larger than 11.9 deg from the X-ray lightcurve. An upper limit of 60% polarisation was determined for the prompt emission of GRB 061122, using the multiple event data. The high energy emission observed in the spectrum may be due to the reverse shock interacting with the GRB ejecta when it is decelerated by the circumburst medium. This behaviour has been observed in a small fraction of GRBs to date, but is expected to be more commonly observed by the Fermi Gamma-ray Space Telescope. The conditions for polarisation are met if the jet opening angle is less than 2.8 deg, but further constraints on the level of polarisation are not possible.",
    "MGT": "Gamma-ray bursts (GRBs) are the most luminous explosions in the universe, exhibiting intense bursts of gamma-ray radiation followed by a longer-lasting afterglow at various wavelengths. GRB 061122, detected by the INTEGRAL satellite, is a particularly interesting event due to its relatively high fluence and complex temporal structure. This study presents a detailed analysis of the high-energy emission from GRB 061122 observed by INTEGRAL/SPI and IBIS instruments, focusing on the prompt emission phase. We perform time-resolved spectral analysis using various spectral models, including power-law, cutoff power-law, and Band functions, to characterize the evolution of the burst's spectral properties. Our analysis reveals significant spectral variability within the burst, with the peak energy (Epeak) evolving over time. Furthermore, we investigate the temporal behavior of the high-energy emission, searching for correlations between the intensity and spectral parameters.\n\nPolarization measurements provide crucial insights into the emission mechanisms and magnetic field configurations within GRBs. We utilize the INTEGRAL/IBIS instrument, which is sensitive to polarization, to place constraints on the polarization fraction of the prompt emission from GRB 061122. Due to the limited statistics, we derive upper limits on the polarization fraction for different time intervals and energy ranges. These upper limits are compared with theoretical predictions from various emission models, such as synchrotron radiation and inverse Compton scattering, to constrain the physical parameters of the emitting region. The implications of these polarization limits for understanding the magnetic field structure and particle acceleration processes in GRB jets are discussed. Finally, we compare our results with those obtained from other GRBs observed by INTEGRAL and other gamma-ray telescopes, providing a broader context for understanding the high-energy emission and polarization properties of these enigmatic events. Our findings contribute to a better understanding of the physical processes governing GRBs and their role in the high-energy universe.\n"
  },
  {
    "id": 128,
    "prompt": "A critical layer model for turbulent pipe flow",
    "HWT": "A model-based description of the scaling and radial location of turbulent fluctuations in turbulent pipe flow is presented and used to illuminate the scaling behaviour of the very large scale motions. The model is derived by treating the nonlinearity in the perturbation equation (involving the Reynolds stress) as an unknown forcing, yielding a linear relationship between the velocity field response and this nonlinearity. We do not assume small perturbations. We examine propagating modes, permitting comparison of our results to experimental data, and identify the steady component of the velocity field that varies only in the wall-normal direction as the turbulent mean profile. The \"optimal\" forcing shape, that gives the largest velocity response, is assumed to lead to modes that will be dominant and hence observed in turbulent pipe flow. An investigation of the most amplified velocity response at a given wavenumber-frequency combination reveals critical layer-like behaviour reminiscent of the neutrally stable solutions of the Orr-Sommerfeld equation in linearly unstable flow. Two distinct regions in the flow where the influence of viscosity becomes important can be identified, namely a wall layer that scales with $R^{+1/2}$ and a critical layer, where the propagation velocity is equal to the local mean velocity, that scales with $R^{+2/3}$ in pipe flow. This framework appears to be consistent with several scaling results in wall turbulence and reveals a mechanism by which the effects of viscosity can extend well beyond the immediate vicinity of the wall.",
    "MGT": "The near-wall region of turbulent pipe flow is investigated using a novel critical layer model. This model focuses on the interaction between coherent structures and the mean shear flow, particularly within the critical layer where the phase speed of the structures matches the local mean flow velocity. Unlike traditional approaches relying on eddy viscosity or mixing length hypotheses, this model explicitly accounts for the dynamics of individual coherent structures, represented as traveling wave solutions to the linearized Navier-Stokes equations. The critical layer acts as a selective amplifier, favoring structures with specific wavelengths and orientations that resonate with the local mean shear.\n\nThe model predicts the Reynolds stress profile by integrating the contributions of these amplified structures, weighted by their respective energies. A key element is the determination of the energy distribution among different modes, which is achieved through a self-consistency condition requiring the total Reynolds stress to be sufficient to sustain the mean shear. This condition leads to a non-linear eigenvalue problem that determines the dominant wavenumbers and their corresponding amplitudes.\n\nThe model is applied to fully developed turbulent pipe flow at various Reynolds numbers. Results show good agreement with direct numerical simulation (DNS) data for the mean velocity profile, Reynolds stress distribution, and turbulence intensities. The model accurately captures the logarithmic region and the near-wall peak in Reynolds stress, demonstrating its ability to represent the essential physics of the turbulent boundary layer. Furthermore, the analysis reveals the dominant coherent structures contributing to the Reynolds stress at different wall-normal distances, providing insights into the mechanisms of turbulence production and transport. The model offers a computationally efficient alternative to DNS and large eddy simulation (LES) for predicting turbulent pipe flow, while providing a physically grounded framework for understanding the role of coherent structures in turbulence dynamics.\n"
  },
  {
    "id": 129,
    "prompt": "Importance of constraining the dense matter Equation of State in pulsar astrophysics",
    "HWT": "We study the dependence of the surface magnetic fields of radio pulsars on the choice of Equations of State, pulsar masses and the values of the angle between the magnetic axis and the spin axis of the pulsars within simple dipole model. We show that the values of the surface magnetic field can be even order of magnitude different from its canonical values. This difference will effect any magnetosphere related model to explain observational features of radio pulsars and magnetars. We find a significant difference of the value of the surface magnetic field from the commonly quoted value for the faster member of the double pulsar system, i.e. PSR J0737-3039A as here both the mass of the pulsar and the angle between the magnetic axis and the spin axis are known. Our study reveals the importance of constraining the dense matter Equations of State in pulsar astrophysics as well as hints an alternative way to constrain these by independent determination of the pulsar magnetic field.",
    "MGT": "Neutron stars, with their extreme densities, provide a unique laboratory for probing the equation of state (EOS) of matter at supranuclear densities. The EOS governs the relationship between pressure and density, dictating crucial neutron star properties such as mass, radius, and tidal deformability. Precisely constraining the EOS is paramount for advancing our understanding of fundamental physics, including the nature of strong interactions and the composition of matter under extreme conditions. Pulsar astrophysics offers several avenues for constraining the dense matter EOS. Precise mass measurements of pulsars in binary systems, particularly the observation of pulsars exceeding two solar masses, provide a lower limit on the maximum mass supported by the EOS. Radius measurements, though challenging, offer complementary constraints. Furthermore, the detection of gravitational waves from neutron star mergers, specifically the tidal deformability extracted from the waveform, offers a novel way to probe the EOS. This article reviews recent progress in constraining the dense matter EOS using pulsar observations and gravitational wave data. We discuss the theoretical uncertainties and observational challenges associated with each method, highlighting the interplay between nuclear physics, astrophysics, and gravitational wave astronomy in unraveling the mysteries of neutron star interiors. Future observations and improved theoretical models promise to further refine our understanding of the dense matter EOS, shedding light on the fundamental nature of matter at its most extreme.\n"
  },
  {
    "id": 130,
    "prompt": "Noise and nonlinearities in high-throughput data",
    "HWT": "High-throughput data analyses are becoming common in biology, communications, economics and sociology. The vast amounts of data are usually represented in the form of matrices and can be considered as knowledge networks. Spectra-based approaches have proved useful in extracting hidden information within such networks and for estimating missing data, but these methods are based essentially on linear assumptions. The physical models of matching, when applicable, often suggest non-linear mechanisms, that may sometimes be identified as noise. The use of non-linear models in data analysis, however, may require the introduction of many parameters, which lowers the statistical weight of the model. According to the quality of data, a simpler linear analysis may be more convenient than more complex approaches. In this paper, we show how a simple non-parametric Bayesian model may be used to explore the role of non-linearities and noise in synthetic and experimental data sets.",
    "MGT": "High-throughput technologies, while revolutionizing scientific discovery, are inherently susceptible to noise and nonlinearities that can obscure underlying biological signals and lead to inaccurate interpretations. This article explores the pervasive influence of these factors on diverse high-throughput datasets, including transcriptomics, proteomics, and metabolomics. We delve into the multifaceted sources of noise, encompassing both technical variations arising from experimental procedures and intrinsic biological variability. Furthermore, we examine the impact of nonlinear relationships between measured signals and underlying biological processes, such as saturation effects and complex regulatory networks. We present a comprehensive overview of statistical and computational methods designed to mitigate the effects of noise and account for nonlinearities in high-throughput data analysis. These methods include normalization techniques, batch effect correction algorithms, and nonlinear regression models. We critically evaluate the strengths and limitations of each approach, providing practical guidance for researchers seeking to enhance the accuracy and reliability of their high-throughput data analyses. Finally, we highlight the importance of rigorous quality control measures and careful experimental design in minimizing the introduction of noise and addressing nonlinearities from the outset.\n"
  },
  {
    "id": 131,
    "prompt": "Dilatons in Hidden Local Symmetry for Hadrons in Dense Matter",
    "HWT": "With the explicit breaking of scale invariance by the trace anomaly of QCD rephrased in terms of spontaneous breaking, low-energy strong interaction dynamics of dense (and also hot) matter can be effectively captured by -- in addition to the Nambu-Goldstone bosons and the vector mesons -- two dilaton fields, the \"soft\" ($\\chi_s$) field that is locked to chiral symmetry and the \"hard\" ($\\chi_h$) field which remains unaffected by chiral symmetry. The interplay of the soft and hard dilatons plays a subtle role in how chiral symmetry is manifested in hot and/or dense matter. The scale anomaly in which the soft component intervenes vanishes at the chiral transition in a way analogous to the restoration of scale symmetry in the Freund-Nambu model, while that of the hard component remains broken throughout the QCD sector. Most remarkable of all is its role in the chiral anomaly sector through a \"homogeneous Wess-Zumino (hWZ) term\" of the form $\\omega_\\mu B^\\mu$ on the structure of a single baryon as well as dense baryonic matter. It figures crucially in predicting a \"Little Bag\" for the nucleon and a \"quarkyonic phase\" in the form of a half-skyrmion matter at high density. We show how the vanishing of the vector-meson mass at the vector manifestation fixed point in hidden local symmetry theory can be related to the property of the \"matter field\" in the Freund-Nambu model that leaves scale symmetry invariant. The emerging structure of dense hadronic matter in the model so constructed suggests what could be amiss in describing dense matter in holographic dual QCD at its large $N_c$ and 't Hooft limit.",
    "MGT": "This study investigates the role of dilatons within the framework of Hidden Local Symmetry (HLS) in describing the properties of hadrons in dense matter, particularly focusing on the implications for chiral symmetry restoration and the equation of state. The HLS Lagrangian, extended to incorporate a dilaton field associated with the spontaneous breaking of scale invariance, provides a powerful tool for analyzing the behavior of vector mesons, nucleons, and scalar fields at finite density. We explore the effective potential for the dilaton field, demonstrating how its minimum shifts with increasing density, signaling the approach to chiral restoration. This shift directly influences the masses of hadrons, leading to a predicted dropping of vector meson masses, consistent with experimental observations in heavy-ion collisions.\n\nThe introduction of the dilaton field modifies the in-medium properties of the vector mesons, affecting their dispersion relations and spectral functions. We calculate the in-medium vector meson masses and decay widths, incorporating the effects of both baryonic density and the dilaton condensate. Our analysis reveals a significant broadening of the vector meson spectral functions at high densities, indicating a substantial modification of their properties due to interactions with the dense medium. This broadening is further enhanced by the presence of the dilaton, which couples to the vector mesons and alters their decay channels.\n\nFurthermore, we examine the impact of the dilaton on the equation of state for dense matter. By incorporating the dilaton contribution to the energy density and pressure, we obtain an equation of state that is consistent with constraints from neutron star observations and chiral effective field theory calculations at lower densities. The dilaton's role in mimicking the effects of scale invariance breaking allows for a more accurate description of the transition to a chirally restored phase at high densities. The model predicts a softening of the equation of state at densities relevant to the cores of neutron stars, potentially impacting the maximum mass and radius of these compact objects. Overall, this study highlights the importance of including the dilaton field in HLS models for a comprehensive understanding of hadron properties and the equation of state in dense matter.\n"
  },
  {
    "id": 132,
    "prompt": "Surface magnetic fields on two accreting T Tauri stars: CV Cha and CR Cha",
    "HWT": "We have produced brightness and magnetic field maps of the surfaces of CV Cha and CR Cha: two actively accreting G and K-type T Tauri stars in the Chamaeleon I star-forming cloud with ages of 3-5 Myr. Our magnetic field maps show evidence for strong, complex multi-polar fields similar to those obtained for young rapidly rotating main sequence stars. Brightness maps indicate the presence of dark polar caps and low latitude spots -- these brightness maps are very similar to those obtained for other pre-main sequence and rapidly rotating main sequence stars. Only two other classical T Tauri stars have been studied using similar techniques so far: V2129 Oph and BP Tau. CV Cha and CR Cha show magnetic field patterns that are significantly more complex than those recovered for BP Tau, a fully convective T Tauri star. We discuss possible reasons for this difference and suggest that the complexity of the stellar magnetic field is related to the convection zone; with more complex fields being found in T Tauri stars with radiative cores (V2129 Oph, CV Cha and CR Cha). However, it is clearly necessary to conduct magnetic field studies of T Tauri star systems, exploring a wide range of stellar parameters in order to establish how they affect magnetic field generation, and thus how these magnetic fields are likely to affect the evolution of T Tauri star systems as they approach the main sequence.",
    "MGT": "We present a detailed study of the surface magnetic fields of two accreting T Tauri stars, CV Cha and CR Cha, based on high-resolution spectropolarimetric observations obtained with HARPSpol at the ESO 3.6m telescope. Our primary goal is to characterise the strength and topology of their magnetic fields and investigate their influence on accretion processes. We employed the Least-Squares Deconvolution (LSD) technique to extract mean Stokes I and V profiles from the observed spectra, allowing us to infer the longitudinal magnetic field ($B_l$).\n\nFor CV Cha, we detected a strong and variable $B_l$ signal, ranging from approximately 0.8 to 1.2 kG. Time-series analysis reveals a clear rotational modulation of the $B_l$ signal, suggesting a complex, non-axisymmetric magnetic field topology. Zeeman Doppler Imaging (ZDI) was applied to reconstruct the surface magnetic field vector. The resulting magnetic maps show a strong polar field with a dominant octupolar component and weaker contributions from dipolar and quadrupolar terms. The field strength reaches up to 3 kG at the poles.\n\nCR Cha also exhibits a strong longitudinal magnetic field, with $B_l$ values varying between 1.5 and 2.0 kG. The rotational modulation of the $B_l$ signal is less pronounced compared to CV Cha, indicating a potentially more axisymmetric field configuration. ZDI analysis reveals a strong, predominantly dipolar field structure with a field strength exceeding 4 kG at the magnetic pole.\n\nAnalysis of accretion signatures, such as Hα and He I λ5876 emission lines, reveals a correlation between the magnetic field strength and accretion activity. The stronger and more organised magnetic field of CR Cha appears to channel accretion flows more efficiently than the more complex field of CV Cha. Our results provide valuable insights into the role of magnetic fields in regulating accretion processes in young stars and highlight the diversity of magnetic field topologies among T Tauri stars.\n"
  },
  {
    "id": 133,
    "prompt": "Positive and negative streamers in ambient air: modeling evolution and velocities",
    "HWT": "We simulate short positive and negative streamers in air at standard temperature and pressure. They evolve in homogeneous electric fields or emerge from needle electrodes with voltages of 10 to 20 kV. The streamer velocity at given streamer length depends only weakly on the initial ionization seed, except in the case of negative streamers in homogeneous fields. We characterize the streamers by length, head radius, head charge and field enhancement. We show that the velocity of positive streamers is mainly determined by their radius and in quantitative agreement with recent experimental results both for radius and velocity. The velocity of negative streamers is dominated by electron drift in the enhanced field; in the low local fields of the present simulations, it is little influenced by photo-ionization. Though negative streamer fronts always move at least with the electron drift velocity in the local field, this drift motion broadens the streamer head, decreases the field enhancement and ultimately leads to slower propagation or even extinction of the negative streamer.",
    "MGT": "Streamer discharges are fundamental to understanding electrical breakdown phenomena in gases, playing a crucial role in diverse applications ranging from lightning initiation to plasma-assisted combustion. This study presents a comprehensive numerical model to investigate the dynamics of both positive and negative streamers propagating in ambient air under atmospheric conditions. The model incorporates a drift-diffusion approximation coupled with Poisson's equation, accounting for key processes such as electron impact ionization, attachment, photoionization, and ion conversion reactions. Special attention is given to the accurate representation of photoionization, a crucial mechanism for streamer propagation in air. We employ a parallelized adaptive mesh refinement scheme to resolve the steep gradients characteristic of streamer channels. Our simulations reveal distinct differences in the morphology and propagation velocities of positive and negative streamers. Positive streamers exhibit a broader, more diffuse structure and propagate faster due to the synergistic effect of electron impact ionization and photoionization ahead of the streamer tip. Negative streamers, conversely, exhibit a narrower channel and slower propagation speeds, primarily driven by electron impact ionization within the high-field region at the streamer head. The model predictions are validated against experimental measurements, demonstrating good agreement in terms of streamer velocity and radius. Furthermore, we investigate the influence of applied voltage and background electric field on streamer characteristics, providing insights into the underlying physical mechanisms governing streamer propagation in air.\n"
  },
  {
    "id": 134,
    "prompt": "Extinction risk and structure of a food web model",
    "HWT": "We investigate in detail the model of a trophic web proposed by Amaral and Meyer [Phys. Rev. Lett. 82, 652 (1999)]. We focused on small-size systems that are relevant for real biological food webs and for which the fluctuations are playing an important role. We show, using Monte Carlo simulations, that such webs can be non-viable, leading to extinction of all species in small and/or weakly coupled systems. Estimations of the extinction times and survival chances are also given. We show that before the extinction the fraction of highly-connected species (\"omnivores\") is increasing. Viable food webs exhibit a pyramidal structure, where the density of occupied niches is higher at lower trophic levels, and moreover the occupations of adjacent levels are closely correlated. We also demonstrate that the distribution of the lengths of food chains has an exponential character and changes weakly with the parameters of the model. On the contrary, the distribution of avalanche sizes of the extinct species depends strongly on the connectedness of the web. For rather loosely connected systems we recover the power-law type of behavior with the same exponent as found in earlier studies, while for densely-connected webs the distribution is not of a power-law type.",
    "MGT": "Understanding how species extinctions cascade through ecological communities is crucial for conservation efforts. We investigate the relationship between initial species loss and subsequent extinctions within a food web context, focusing on the role of food web structure in mediating extinction risk. Using a static food web model, we simulate primary extinctions and track secondary extinctions resulting from trophic interactions. We explore different extinction scenarios, varying the identity and number of initially removed species based on their trophic position (basal, intermediate, or top predators) and centrality within the food web. Network metrics, including connectance, linkage density, and vulnerability, are calculated to characterize food web structure.\n\nOur results demonstrate that the impact of primary extinctions on secondary extinctions is highly dependent on the trophic role of the lost species. Removal of basal species leads to the most pronounced cascading effects, while the loss of top predators has a relatively smaller impact. Furthermore, species with high centrality, indicating numerous connections, are found to be critical for maintaining food web stability; their removal triggers a disproportionately large number of secondary extinctions.  Food webs with higher connectance and linkage density exhibit greater resilience to initial extinctions, suggesting that complex trophic interactions can buffer against cascading effects. However, highly connected food webs may also experience larger extinction cascades once a critical threshold of species loss is reached. Our findings highlight the importance of considering food web structure and species roles when assessing extinction risk and prioritizing conservation strategies.\n"
  },
  {
    "id": 135,
    "prompt": "Dumb-bell swimmers",
    "HWT": "We investigate the way in which oscillating dumb-bells, a simple microscopic model of apolar swimmers, move at low Reynold's number. In accordance with Purcell's Scallop Theorem a single dumb-bell cannot swim because its stroke is reciprocal in time. However the motion of two or more dumb-bells, with mutual phase differences, is not time reversal invariant, and hence swimming is possible. We use analytical and numerical solutions of the Stokes equations to calculate the hydrodynamic interaction between two dumb-bell swimmers and to discuss their relative motion. The cooperative effect of interactions between swimmers is explored by considering first regular, and then random arrays of dumb-bells. We find that a square array acts as a micropump. The long time behaviour of suspensions of dumb-bells is investigated and compared to that of model polar swimmers.",
    "MGT": "This study investigates the biomechanics and physiological effects of using dumbbells during swimming, specifically examining the impact on stroke efficiency, muscle activation, and energy expenditure. Ten competitive swimmers performed a series of swimming trials with and without dumbbells of varying weights (0.5 kg, 1.0 kg). Kinematic data was collected using underwater cameras to analyze stroke length, stroke rate, and body position. Electromyography (EMG) was used to measure muscle activation in key upper body muscle groups, including the latissimus dorsi, pectoralis major, and triceps brachii. Metabolic rate was assessed through indirect calorimetry to determine energy expenditure. Results indicated that dumbbell use significantly reduced stroke length and increased stroke rate, leading to a decrease in overall swimming efficiency. EMG data showed increased activation in the latissimus dorsi and pectoralis major muscles when using dumbbells. Furthermore, energy expenditure was significantly higher during dumbbell swimming. These findings suggest that while dumbbell swimming can increase muscle activation and energy expenditure, it negatively impacts stroke efficiency. Therefore, the use of dumbbells in swimming training should be carefully considered and tailored to specific training goals, such as strength development, rather than overall swimming performance enhancement.\n"
  },
  {
    "id": 136,
    "prompt": "Indirect detection of gravitino dark matter including its three-body decays",
    "HWT": "It was recently pointed out that in supersymmetric scenarios with gravitino dark matter and bilinear R-parity violation, gravitinos with masses below Mw typically decay with a sizable branching ratio into the 3-body final states W^*+lepton and Z^*+neutrino. In this paper we study the indirect detection signatures of gravitino dark matter including such final states. First, we obtain the gamma ray spectrum from gravitino decays, which features a monochromatic contribution from the decay into photon+neutrino and a continuum contribution from the three-body decays. After studying its dependence on supersymmetric parameters, we compute the expected gamma ray fluxes and derive new constraints, from recent FERMI data, on the R-parity breaking parameter and on the gravitino lifetime. Indirect detection via antimatter searches, a new possibility brought about by the three-body final states, is also analyzed. For models compatible with the gamma ray observations, the positron signal is found to be negligible whereas the antiproton one can be significant.",
    "MGT": "Gravitino dark matter, while weakly interacting, can still produce observable signals through its decays, particularly if the R-parity violating couplings are non-negligible. This work investigates the indirect detection prospects of gravitino dark matter, focusing on scenarios where the gravitino undergoes three-body decays into Standard Model particles. We develop a comprehensive framework that incorporates the relevant particle physics parameters, including the gravitino mass, the R-parity violating couplings, and the properties of the intermediate particles in the decay chain. We calculate the resulting fluxes of photons, neutrinos, and charged cosmic rays produced by these decays, considering various decay channels and branching ratios. These fluxes are then compared with current observational constraints from gamma-ray telescopes such as Fermi-LAT and H.E.S.S., neutrino detectors like IceCube, and cosmic-ray experiments such as AMS-02. We explore the parameter space of gravitino mass and R-parity violating couplings, identifying regions that are already excluded by current data and projecting the sensitivity of future experiments. Our analysis highlights the importance of considering three-body decay modes in indirect searches for gravitino dark matter, and provides constraints on the R-parity violating parameter space that complements collider searches. We discuss the implications of our findings for various cosmological models and the nature of dark matter.\n"
  },
  {
    "id": 137,
    "prompt": "Impure Thoughts on Inelastic Dark Matter",
    "HWT": "The inelastic dark matter scenario was proposed to reconcile the DAMA annual modulation with null results from other experiments. In this scenario, WIMPs scatter into an excited state, split from the ground state by an energy delta comparable to the available kinetic energy of a Galactic WIMP. We note that for large splittings delta, the dominant scattering at DAMA can occur off of thallium nuclei, with A~205, which are present as a dopant at the 10^-3 level in NaI(Tl) crystals. For a WIMP mass m~100GeV and delta~200keV, we find a region in delta-m-parameter space which is consistent with all experiments. These parameters in particular can be probed in experiments with thallium in their targets, such as KIMS, but are inaccessible to lighter target experiments. Depending on the tail of the WIMP velocity distribution, a highly modulated signal may or may not appear at CRESST-II.",
    "MGT": "The nature of dark matter remains one of the most compelling mysteries in modern physics. While Weakly Interacting Massive Particles (WIMPs) have long been a leading candidate, null results from direct detection experiments have motivated exploration of alternative models. Inelastic Dark Matter (iDM), where dark matter particles can up-scatter to a slightly heavier state via interactions with Standard Model particles, presents a viable alternative. This work re-examines the theoretical framework of iDM, highlighting subtleties often overlooked in phenomenological studies. We demonstrate that seemingly innocuous assumptions regarding the mass splitting between the two dark matter states and the mediator mass can lead to inconsistencies in the effective field theory description. Specifically, we show how neglecting higher-order terms in the expansion can result in inaccurate predictions for direct detection cross-sections and collider signatures. Furthermore, we explore the implications of these \"impure\" effects on the allowed parameter space for iDM, considering constraints from direct detection, indirect detection, and collider searches. Our findings suggest that a more careful treatment of these effects is crucial for accurately assessing the viability of iDM as a dark matter candidate and for interpreting experimental results.\n"
  },
  {
    "id": 138,
    "prompt": "Opportunistic Scheduling and Beamforming for MIMO-OFDMA Downlink Systems with Reduced Feedback",
    "HWT": "Opportunistic scheduling and beamforming schemes with reduced feedback are proposed for MIMO-OFDMA downlink systems. Unlike the conventional beamforming schemes in which beamforming is implemented solely by the base station (BS) in a per-subcarrier fashion, the proposed schemes take advantages of a novel channel decomposition technique to perform beamforming jointly by the BS and the mobile terminal (MT). The resulting beamforming schemes allow the BS to employ only {\\em one} beamforming matrix (BFM) to form beams for {\\em all} subcarriers while each MT completes the beamforming task for each subcarrier locally. Consequently, for a MIMO-OFDMA system with $Q$ subcarriers, the proposed opportunistic scheduling and beamforming schemes require only one BFM index and $Q$ supportable throughputs to be returned from each MT to the BS, in contrast to $Q$ BFM indices and $Q$ supportable throughputs required by the conventional schemes. The advantage of the proposed schemes becomes more evident when a further feedback reduction is achieved by grouping adjacent subcarriers into exclusive clusters and returning only cluster information from each MT. Theoretical analysis and computer simulation confirm the effectiveness of the proposed reduced-feedback schemes.",
    "MGT": "Opportunistic scheduling and beamforming techniques offer significant throughput gains in multi-user multiple-input multiple-output orthogonal frequency division multiple access (MIMO-OFDMA) downlink systems. However, these techniques typically require substantial channel state information (CSI) feedback from the users to the base station, posing a significant overhead burden. This paper proposes a novel opportunistic scheduling and beamforming algorithm designed to minimize feedback requirements while maintaining near-optimal performance. The core idea involves a two-stage approach: first, users are grouped based on their coarse-grained channel quality indicators (CQIs), requiring minimal feedback. Within each group, users are then selected opportunistically based on a limited number of beamforming vectors, further reducing feedback overhead. The proposed algorithm leverages a codebook-based beamforming scheme, where users only need to report the index of the best beamforming vector for their channel. To enhance performance, we introduce a probabilistic user selection strategy that balances exploitation of strong users with exploration of potentially better users. Simulation results demonstrate that the proposed algorithm achieves a significant reduction in feedback overhead compared to conventional CSI-based approaches, while maintaining a comparable sum-rate performance. The trade-off between feedback reduction and performance is carefully analyzed, highlighting the algorithm's adaptability to various system constraints and user distributions. Specifically, the proposed scheme achieves a 70% reduction in feedback with only a 5% loss in sum-rate compared to full CSI feedback schemes. The results showcase the potential of the proposed algorithm for practical implementation in future wireless communication systems.\n"
  },
  {
    "id": 139,
    "prompt": "Normal galaxies in the XMM-Newton fields. X-rays as a star formation indicator",
    "HWT": "Context: We use the first XMM serendipitous source catalogue (1XMM) to compile a sample of normal X-ray galaxies Aims: We seek to expand the database of X-ray selected normal galaxies at intermediate redshifts and examine the relation between X-ray emission and star formation for late-type systems Methods: The candidates are selected based on their X-ray (soft spectra), X-ray to optical [log(fx/fo)<-2] and optical (extended sources) properties. 44 candidates are found and 35 are spectroscopically observed with the Australian National University's 2.3m telescope to examine their nature. Results: Of the 35 sources observed, 2 are AGN, 11 emission line galaxies, 12 absorption line galaxies, 6 have featureless spectra while 4 are associated with Galactic stars. We combine our emission line sample with earlier works forming the most comprehensive X-ray selected galaxy sample for the study of the X-ray luminosity to the Halpha luminosity - a well-calibrated star-formation indicator - relation. Conclusions: We find that the X-ray luminosity strongly correlates with the Halpha luminosity, suggesting that the X-rays efficiently trace the star-formation.",
    "MGT": "We present a study of normal galaxies detected in XMM-Newton fields, focusing on the relationship between their X-ray emission and star formation rate (SFR). Leveraging a large, multi-wavelength dataset, we investigate the X-ray luminosity (Lx) – SFR correlation for a sample of galaxies selected based on optical and infrared properties, ensuring a robust characterization of their star formation activity. We explore the contributions of high-mass X-ray binaries (HMXBs) and hot gas to the total X-ray luminosity, using spectral fitting and multi-wavelength diagnostics to disentangle these components. Our analysis reveals a strong correlation between Lx and SFR, consistent with previous findings, but with significant scatter attributed to variations in metallicity, age of the stellar population, and AGN contamination. We also examine the evolution of the Lx-SFR relation with redshift, finding evidence for a potential increase in the X-ray luminosity per unit SFR at higher redshifts, possibly indicating a higher HMXB formation efficiency in younger, more metal-poor galaxies. These results highlight the utility of X-ray observations as a valuable tool for probing star formation activity in galaxies, particularly at high redshifts where other SFR indicators may be less reliable. We discuss the implications of our findings for understanding galaxy evolution and the cosmic star formation history.\n"
  },
  {
    "id": 140,
    "prompt": "Effect of chemical substitution and pressure on YbRh2Si2",
    "HWT": "We carried out electrical resistivity experiments on (Yb, La)Rh2Si2 and on Yb(Rh, Ir)2Si2 under pressure and in magnetic fields. YbRh2Si2 exhibits a weak antiferromagnetic transition at atmospheric pressure with a N\\'eel temperature of only T_N = 70 mK. By applying a small magnetic field T_N can be continuously suppressed to T=0 at B_c = 60 mT (B_|_c) driving the system to a quantum critical point (QCP). On applying external pressure the magnetic phase is stabilized and T_N(p) is increasing as usually observed in Yb-based heavy-fermion metals. Substituting Yb by La or Rh by Ir allows to create a negative chemical pressure, La (Ir) being smaller than Yb (Rh), and eventually to drive YbRh2Si2 to a pressure controlled QCP. In this paper we compare the effect of external hydrostatic pressure and chemical substitution on the ground-state properties of YbRh2Si2.",
    "MGT": "YbRh2Si2 is a heavy fermion compound exhibiting unconventional quantum criticality. Understanding the interplay between chemical pressure, external pressure, and quantum fluctuations is crucial for elucidating its complex phase diagram. This study investigates the effects of chemical substitution and hydrostatic pressure on the electronic and magnetic properties of YbRh2Si2. We present a comprehensive investigation of Yb(Rh1-xCox)2Si2 and YbRh2(Si1-xGex)2 single crystals, focusing on how Co and Ge substitution tune the system away from the quantum critical point. Electrical resistivity, magnetic susceptibility, and specific heat measurements reveal that both Co and Ge doping suppress the antiferromagnetic order and induce a Fermi liquid state. Furthermore, we examine the impact of hydrostatic pressure on the parent compound YbRh2Si2, observing a suppression of the antiferromagnetic transition temperature and an enhancement of the Kondo temperature. Comparative analysis of chemical and physical pressure effects suggests that both act to modify the hybridization between the localized Yb 4f electrons and the conduction electrons. Our findings provide valuable insights into the quantum criticality of YbRh2Si2 and the underlying mechanisms governing the heavy fermion behavior in this material, highlighting the subtle interplay between Kondo coupling, Ruderman-Kittel-Kasuya-Yosida (RKKY) interactions, and quantum fluctuations.\n"
  },
  {
    "id": 141,
    "prompt": "Radiation-Hydrodynamics of Hot Jupiter Atmospheres",
    "HWT": "Radiative transfer in planetary atmospheres is usually treated in the static limit, i.e., neglecting atmospheric motions. We argue that hot Jupiter atmospheres, with possibly fast (sonic) wind speeds, may require a more strongly coupled treatment, formally in the regime of radiation-hydrodynamics. To lowest order in v/c, relativistic Doppler shifts distort line profiles along optical paths with finite wind velocity gradients. This leads to flow-dependent deviations in the effective emission and absorption properties of the atmospheric medium. Evaluating the overall impact of these distortions on the radiative structure of a dynamic atmosphere is non-trivial. We present transmissivity and systematic equivalent width excess calculations which suggest possibly important consequences for radiation transport in hot Jupiter atmospheres. If winds are fast and bulk Doppler shifts are indeed important for the global radiative balance, accurate modeling and reliable data interpretation for hot Jupiter atmospheres may prove challenging: it would involve anisotropic and dynamic radiative transfer in a coupled radiation-hydrodynamical flow. On the bright side, it would also imply that the emergent properties of hot Jupiter atmospheres are more direct tracers of their atmospheric flows than is the case for Solar System planets. Radiation-hydrodynamics may also influence radiative transfer in other classes of hot exoplanetary atmospheres with fast winds.",
    "MGT": "Hot Jupiters, exoplanets with masses comparable to Jupiter and orbital periods of only a few days, experience intense stellar irradiation. This extreme environment drives complex atmospheric dynamics and thermal structures. This study investigates the radiation-hydrodynamic (RHD) processes governing the atmospheres of hot Jupiters using a three-dimensional (3D) RHD code. We focus on understanding the interplay between radiative transfer, hydrodynamics, and chemical processes in shaping the atmospheric temperature profiles, wind patterns, and observable features.\n\nOur simulations incorporate a multi-band radiative transfer scheme to accurately model the absorption and emission of stellar radiation by various atmospheric constituents, including TiO, VO, and water vapor. We solve the hydrodynamic equations using a finite-volume method, accounting for advection, pressure gradients, and viscous forces. Furthermore, we include simplified chemical kinetics to model the formation and destruction of key opacity sources.\n\nWe present results from simulations of a representative hot Jupiter, HD 209458b, exploring the impact of different stellar spectral energy distributions and atmospheric compositions on the simulated atmospheric structure and dynamics. We find that the inclusion of TiO and VO significantly alters the temperature profile, leading to the formation of a strong thermal inversion layer in the upper atmosphere. This inversion influences the wind patterns, generating strong equatorial jets and complex circulation patterns. We also examine the effect of varying the planetary albedo on the overall atmospheric energy budget.\n\nFinally, we synthesize phase curves and transmission spectra from our 3D RHD simulations and compare them with observational data. The comparison allows us to constrain the atmospheric composition and dynamics of HD 209458b, providing insights into the physical processes shaping the atmospheres of these highly irradiated exoplanets. These models contribute to a more comprehensive understanding of hot Jupiter atmospheres and provide a framework for interpreting future observations.\n"
  },
  {
    "id": 142,
    "prompt": "Gemini GMOS spectroscopy of HeII nebulae in M33",
    "HWT": "We have carried out a narrow-band survey of the Local Group galaxy, M33, in the HeII4686 emission line, to identify HeII nebulae in this galaxy. With spectroscopic follow-up observations, we confirm three of seven candidate objects, including identification of two new HeII nebulae, BCLMP651, HBW673. We also obtain spectra of associated ionizing stars for all the HII regions, identifying two new WN stars. We demonstrate that the ionizing source for the known HeII nebula, MA 1, is consistent with being the early-type WN star MC8 (M33-WR14), by carrying out a combined stellar and nebular analysis of MC8 and MA1. We were unable to identify the helium ionizing sources for HBW 673 and BCLMP 651, which do not appear to be Wolf-Rayet stars. According to the [OIII]5007/Hbeta vs [NII]6584/Halpha diagnostic diagram, excitation mechanisms apart from hot stellar continuum are needed to account for the nebular emission in HBW 673, which appears to have no stellar source at all.",
    "MGT": "We present Gemini/GMOS integral field spectroscopy of three nebulae in M33, targeting regions exhibiting strong HeII λ4686 emission. These nebulae, known for their high ionization, offer insights into the nature of ionizing sources in metal-poor environments. We analyze the spatially resolved emission line fluxes, including Hα, Hβ, [OIII]λλ4959,5007, and HeII λ4686, to map the physical conditions and ionization structure within each nebula. Electron temperatures and densities are derived using standard diagnostic line ratios. We find elevated electron temperatures in regions coinciding with the strongest HeII emission. Photoionization models, tailored to the observed nebular properties, are employed to constrain the characteristics of the ionizing source. Our models suggest that a single, hot Wolf-Rayet star is unlikely to fully reproduce the observed HeII λ4686/Hβ ratios. Alternative ionizing sources, such as X-ray binaries or shocks, are explored and discussed. The spatial correlation between HeII emission and other emission lines is examined, providing further clues about the ionization mechanisms at play. These findings contribute to a better understanding of the diverse range of ionization sources capable of producing HeII nebulae and their impact on the interstellar medium in low-metallicity galaxies. Future work will involve expanding the sample size and incorporating additional spectral features to refine the photoionization models.\n"
  },
  {
    "id": 143,
    "prompt": "Magnetic Field Properties in High Mass Star Formation from Large to Small Scales - A Statistical Analysis from Polarization Data",
    "HWT": "Polarization data from high mass star formation regions (W51 e2/e8, Orion BN/KL) are used to derive statistical properties of the plane of sky projected magnetic field. Structure function and auto-correlation function are calculated for observations with various resolutions from the BIMA and SMA interferometers, covering a range in physical scales from $\\sim 70$~mpc to $\\sim 2.1$~mpc. Results for the magnetic field turbulent dispersion, its turbulent to mean field strength ratio and the large-scale polarization angle correlation length are presented as a function of the physical scale at the star formation sites. Power law scaling relations emerge for some of these physical quantities. The turbulent to mean field strength ratio is found to be close to constant over the sampled observing range, with a hint of a decrease toward smaller scales, indicating that the role of magnetic field and turbulence is evolving with physical scale. A statistical method is proposed to separate large and small scale correlations from an initial ensemble of polarization segments. This also leads to a definition of a turbulent polarization angle correlation length.",
    "MGT": "Magnetic fields play a crucial role in the process of high-mass star formation, influencing gas dynamics and regulating the collapse of molecular clouds. Understanding their properties across various scales is essential for a comprehensive picture of star formation. This study presents a statistical analysis of magnetic field properties in high-mass star-forming regions, utilizing polarization data obtained from diverse instruments. We examine a sample of regions, spanning from large-scale molecular clouds to smaller-scale dense cores, to investigate the magnetic field morphology, strength, and alignment with respect to density structures. We employ techniques such as the Histogram of Relative Orientations (HRO) and the Velocity Gradient Technique (VGT) to quantify the relative importance of magnetic fields and turbulence. Our results indicate a transition from magnetically dominated regimes on large scales to more turbulent regimes on smaller scales, particularly within dense cores. The alignment between magnetic fields and density structures varies significantly across different regions, suggesting a complex interplay between gravity, magnetic fields, and turbulence. Furthermore, we explore the relationship between magnetic field properties and star formation efficiency in these regions. These findings provide valuable insights into the role of magnetic fields in the regulation of high-mass star formation and contribute to a more complete understanding of the physical processes involved.\n"
  },
  {
    "id": 144,
    "prompt": "Star cluster kinematics with AAOmega",
    "HWT": "The high-resolution setup of the AAOmega spectrograph on the Anglo-Australian Telescope makes it a beautiful radial velocity machine, with which one can measure velocities of up to 350-360 stars per exposure to +/-1--2 km/s in a 2-degree field of view. Here we present three case studies of star cluster kinematics, each based on data obtained on three nights in February 2008. The specific aims included: (i) cluster membership determination for NGC 2451A and B, two nearby open clusters in the same line-of-sight; (ii) a study of possible membership of the planetary nebula NGC 2438 in the open cluster M46; and (iii) the radial velocity dispersion of M4 and NGC 6144, a pair of two globular clusters near Antares. The results which came out of only three nights of AAT time illustrate very nicely the potential of the instrument and, for example, how quickly one can resolve decades of contradiction in less than two hours of net observing time.",
    "MGT": "Open star clusters are fundamental building blocks of the Galactic disk, offering insights into stellar evolution, Galactic structure, and the star formation process. This study presents a comprehensive kinematic analysis of several open clusters using high-resolution spectroscopic data obtained with the AAOmega multi-object spectrograph at the Anglo-Australian Telescope. We analyze radial velocities of member stars to determine cluster systemic velocities and velocity dispersions. Proper motion data from Gaia DR3 are incorporated to derive 3D space motions and kinematic distances. We employ a Bayesian membership analysis combining radial velocity, proper motion, and photometric data to refine cluster membership probabilities, minimizing field star contamination. The results reveal detailed internal kinematics, including evidence for rotation and velocity gradients within some clusters. Furthermore, we investigate the relationship between cluster kinematics and age, mass, and Galactocentric distance. The derived kinematic parameters are compared with theoretical models of cluster dynamics and tidal interactions with the Galactic potential. Our findings contribute to a better understanding of the dynamical evolution of open clusters and their role in shaping the Galactic disk. This work showcases the power of combining spectroscopic and astrometric data for unraveling the complexities of star cluster kinematics.\n"
  },
  {
    "id": 145,
    "prompt": "The Dependence of Type Ia Supernova Luminosities on their Host Galaxies",
    "HWT": "(Abridged) Precision cosmology with Type Ia supernovae (SNe Ia) makes use of the fact that SN Ia luminosities depend on their light-curve shapes and colours. Using Supernova Legacy Survey (SNLS) and other data, we show that there is an additional dependence on the global characteristics of their host galaxies: events of the same light-curve shape and colour are, on average, 0.08mag (~4.0sigma) brighter in massive host galaxies (presumably metal-rich) and galaxies with low specific star-formation rates (sSFR). SNe Ia in galaxies with a low sSFR also have a smaller slope (\"beta\") between their luminosities and colours with ~2.7sigma significance, and a smaller scatter on SN Ia Hubble diagrams (at 95% confidence), though the significance of these effects is dependent on the reddest SNe. SN Ia colours are similar between low-mass and high-mass hosts, leading us to interpret their luminosity differences as an intrinsic property of the SNe and not of some external factor such as dust. If the host stellar mass is interpreted as a metallicity indicator, the luminosity trends are in qualitative agreement with theoretical predictions. We show that the average stellar mass, and therefore the average metallicity, of our SN Ia host galaxies decreases with redshift. The SN Ia luminosity differences consequently introduce a systematic error in cosmological analyses, comparable to the current statistical uncertainties on parameters such as w. We show that the use of two SN Ia absolute magnitudes, one for events in high-mass (metal-rich) galaxies, and one for events in low-mass (metal-poor) galaxies, adequately corrects for the differences. Cosmological fits incorporating these terms give a significant reduction in chi^2 (3.8-4.5sigma). We conclude that future SN Ia cosmological analyses should use a correction of this (or similar) form to control demographic shifts in the galaxy population.",
    "MGT": "Type Ia supernovae (SNe Ia) are crucial tools for measuring cosmological distances and probing the expansion history of the universe. Their utility stems from their relatively uniform peak luminosity after applying empirical corrections based on light-curve shape and color. However, subtle variations in their intrinsic brightness remain, potentially introducing systematic uncertainties in cosmological analyses. Recent studies have revealed a correlation between SN Ia luminosity and host galaxy properties, suggesting that the environment in which a supernova explodes influences its characteristics. This paper investigates the dependence of SN Ia luminosities on a range of host galaxy properties, including stellar mass, star formation rate, metallicity, and age.\n\nWe analyze a large, spectroscopically confirmed sample of SNe Ia from the Pan-STARRS1 Medium Deep Survey and the Sloan Digital Sky Survey, supplemented with archival data. Host galaxy properties are derived from multi-wavelength photometry using spectral energy distribution (SED) fitting techniques. We employ a hierarchical Bayesian model to simultaneously fit SN Ia light curves and model the relationship between SN Ia luminosity and host galaxy properties. This approach allows us to account for intrinsic scatter in SN Ia luminosities and uncertainties in host galaxy property measurements.\n\nOur results confirm a statistically significant correlation between SN Ia luminosity and host galaxy stellar mass, with SNe Ia in more massive galaxies tending to be brighter after light-curve correction. We also find evidence for a weaker correlation with host galaxy star formation rate and metallicity. Specifically, SNe Ia residing in galaxies with higher star formation rates tend to be slightly fainter. Furthermore, we explore potential correlations with stellar population age, finding suggestive evidence that younger stellar populations may host slightly fainter SNe Ia. We investigate potential selection biases that could influence these correlations, including Malmquist bias and host galaxy dust extinction. We find that these biases are unlikely to fully explain the observed relationships. The implications of these findings for cosmological distance measurements are discussed, highlighting the importance of incorporating host galaxy information into SN Ia standardization procedures to mitigate systematic uncertainties. We conclude by suggesting avenues for future research, including expanding the sample size and exploring the physical mechanisms responsible for the observed correlations between SN Ia luminosities and their host galaxies. Understanding these relationships is crucial for refining SN Ia-based cosmological measurements and improving our understanding of the nature of dark energy.\n"
  },
  {
    "id": 146,
    "prompt": "Radiative emission of solar features in the Ca II K line: comparison of measurements and models",
    "HWT": "We study the radiative emission of various types of solar features, such as quiet Sun, enhanced network, plage, and bright plage regions, identified on filtergrams taken in the Ca II K line. We analysed fulldisk images obtained with the PSPT, by using three interference filters that sample the Ca II K line with different bandpasses. We studied the dependence of the radiative emission of disk features on the filter bandpass. We also performed a NLTE spectral synthesis of the Ca II K line integrated over the bandpass of PSPT filters. The synthesis was carried out by utilizing both the PRD and CRD with the most recent set of semi empirical atmosphere models in the literature and some earlier atmosphere models. We measured the CLV of intensity values for various solar features identified on PSPT images and compared the results obtained with those derived from the synthesis. We find that CRD calculations derived using the most recent quiet Sun model, on average, reproduce the measured values of the quiet Sun regions slightly more accurately than PRD computations with the same model. This may reflect that the utilized atmospheric model was computed assuming CRD. Calculations with PRD on earlier quiet Sun model atmospheres reproduce measured quantities with a similar accuracy as to that achieved here by applying CRD to the recent model. We also find that the median contrast values measured for most of the identified bright features, disk positions, and filter widths are, on average, a factor 1.9 lower than those derived from PRD simulations performed using the recent bright feature models. The discrepancy between measured and modeled values decreases by 12% after taking into account straylight effects on PSPT images. PRD computations on either the most recent or the earlier atmosphere models of bright features reproduce measurements from plage and bright plage regions with a similar accuracy.",
    "MGT": "The solar Ca II K line is a crucial diagnostic tool for probing the upper solar chromosphere. Its emission is highly sensitive to temperature and density variations, making it an invaluable proxy for solar activity. This study presents a comparative analysis of observed and modeled Ca II K emission originating from various solar features, aiming to assess the accuracy of current radiative transfer models and to improve our understanding of chromospheric heating mechanisms. We utilize high-resolution spectral data obtained from the Dunn Solar Telescope (DST) and the Swedish Solar Telescope (SST) covering a range of solar features, including quiet Sun regions, plages, network boundaries, and active region filaments. These observations are co-aligned with simultaneous magnetograms from the Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamics Observatory (SDO) to provide contextual magnetic field information. We perform radiative transfer calculations using the non-LTE code RH to synthesize Ca II K line profiles based on three-dimensional magnetohydrodynamic (MHD) models of the solar atmosphere. These models incorporate various heating mechanisms, including Joule heating, ambipolar diffusion, and acoustic wave dissipation. The synthesized profiles are then convolved with the instrumental profiles of the DST and SST to allow for a direct comparison with the observed spectra. Our analysis reveals significant discrepancies between the observed and modeled Ca II K emission, particularly in the plage and active region filament regions. The models generally underestimate the observed intensity in the K2 peaks, suggesting that the chromospheric temperature structure in these features is not adequately captured by the current MHD models. Furthermore, we find that the modeled K2-K1 separation is smaller than observed, indicating a potential underestimation of the chromospheric density. We explore the sensitivity of the synthesized Ca II K profiles to variations in key model parameters, such as the magnetic field strength, the electron density, and the temperature stratification. We discuss the implications of these findings for our understanding of chromospheric heating mechanisms and the limitations of current radiative transfer models. Finally, we propose refinements to the MHD models and the radiative transfer calculations to improve the agreement between observations and simulations, ultimately leading to a more accurate representation of the solar chromosphere.\n"
  },
  {
    "id": 147,
    "prompt": "Possible evidence for a common radial structure in nearby AGN tori",
    "HWT": "We present a quantitative and relatively model-independent way to assess the radial structure of nearby AGN tori. These putative tori have been studied with long-baseline infrared (IR) interferometry, but the spatial scales probed are different for different objects. They are at various distances and also have different physical sizes which apparently scale with the luminosity of the central engine. Here we look at interferometric visibilities as a function of spatial scales normalized by the size of the inner torus radius R_in. This approximately eliminates luminosity and distance dependence and, thus, provides a way to uniformly view the visibilities observed for various objects and at different wavelengths. We can construct a composite visibility curve over a large range of spatial scales if different tori share a common radial structure. The currently available observations do suggest model-independently a common radial surface brightness distribution in the mid-IR that is roughly of a power-law form r^-2 as a function of radius r, and extends to ~100 times R_in. Taking into account the temperature decrease toward outer radii with a simple torus model, this corresponds to the radial surface density distribution of dusty material directly illuminated by the central engine roughly in the range between r^0 and r^-1. This should be tested with further data.",
    "MGT": "Active galactic nuclei (AGN) are characterized by a supermassive black hole accreting material, surrounded by an obscuring torus of gas and dust. The structure and physical properties of this torus remain a subject of active research, with various models proposed to explain the observed infrared emission and obscuration. This study investigates the possibility of a common radial structure within the tori of nearby AGN, focusing on the relationship between the torus radius and the AGN luminosity.\n\nWe compiled a sample of well-studied nearby AGN with available measurements of the inner radius of the torus, primarily derived from near-infrared reverberation mapping and interferometric observations. We compared these radius measurements with the AGN bolometric luminosity, X-ray luminosity, and mid-infrared luminosity. Our analysis reveals a statistically significant correlation between the torus inner radius and the AGN luminosity across different wavelengths, suggesting a consistent scaling relation.\n\nFurthermore, we explored the dispersion around this scaling relation and investigated potential secondary parameters that might influence the torus radius. We found tentative evidence that the Eddington ratio and the black hole mass may play a role in determining the torus size, potentially indicating a more complex interplay of physical processes.\n\nThese findings support the idea of a common radial structure in AGN tori, where the torus size is primarily determined by the AGN luminosity. This provides valuable constraints for theoretical models of the torus and its formation. Future studies with larger samples and improved measurements of torus radii will be crucial to further refine these relationships and gain a deeper understanding of the AGN unification scheme.\n"
  },
  {
    "id": 148,
    "prompt": "Testing Newtonian gravity with distant globular clusters: NGC1851 and NGC1904",
    "HWT": "Globular clusters are useful to test the validity of Newtonian dynamics in the low acceleration regime typical of galaxies, without the complications of non-baryonic dark matter. Specifically, in absence of disturbing effects, e.g. tidal heating, their velocity dispersion is expected to vanish at large radii. If such behaviour is not observed, and in particular if, as observed in elliptical galaxies, the dispersion is found constant at large radii below a certain threshold acceleration, this might indicate a break down of Newtonian dynamics. To minimise the effects of tidal heating in this paper we study the velocity dispersion profile of two distant globular clusters, NGC 1851 and NGC 1904. The velocity dispersion profile is derived from accurate radial velocities measurements, obtained at the ESO 8m VLT telescope. Reliable data for 184 and 146 bona fide cluster star members, respectively for NGC 1851 and NGC 1904, were obtained. These data allow to trace the velocity dispersion profile up to ~2r0, where r0 is the radius at which the cluster internal acceleration of gravity is a0 = 10e-8 cm/s/s. It is found that in both clusters the velocity dispersion becomes constant beyond ~r0. These new results are fully in agreement with those found for other five globular clusters previously investigated as part of this project. Taken all together, these 7 clusters support the claim that the velocity dispersion is constant beyond r0, irrespectively of the specific physical properties of the clusters: mass, size, dynamical history, and distance from the Milky Way. The strong similarly with the constant velocity dispersion observed in elliptical galaxies beyond r0 is suggestive of a common origin for this phenomenon in the two class of objects, and might indicate a breakdown of Newtonian dynamics below a0.",
    "MGT": "Globular clusters (GCs) serve as valuable probes for testing the validity of Newtonian gravity in the weak-field regime, particularly at large galactocentric distances where the influence of dark matter is expected to dominate. This study focuses on two distant GCs, NGC 1851 and NGC 1904, located in the outer halo of the Milky Way, to assess their dynamical properties and constrain the mass-to-light ratio (M/L) under Newtonian gravity. We utilize a combination of high-precision radial velocity measurements obtained from ground-based telescopes and proper motion data from the Gaia astrometric satellite to construct detailed kinematic profiles for both clusters. These profiles extend to large radii, allowing us to probe the outer regions where deviations from Newtonian predictions might be more pronounced.\n\nWe employ a spherical Jeans equation analysis, incorporating realistic density profiles derived from surface brightness measurements, to model the observed kinematics. The Jeans equation relates the velocity dispersion of the stars to the gravitational potential, which is determined by the mass distribution of the cluster. By fitting the model to the observed kinematic data, we can estimate the M/L ratio of each cluster and compare it to expectations based on stellar population synthesis models.\n\nOur results indicate that the observed kinematics of both NGC 1851 and NGC 1904 are generally consistent with Newtonian gravity, with no significant evidence for deviations at large radii. The derived M/L ratios are within the range predicted by stellar population models, suggesting that the clusters are primarily composed of normal stellar populations and do not require substantial amounts of non-baryonic dark matter to explain their dynamical properties. However, we note that the uncertainties in the measurements, particularly in the proper motions at large radii, limit our ability to detect subtle deviations from Newtonian gravity. We perform a detailed error analysis to quantify the impact of these uncertainties on our results. We also explore alternative models, such as modified Newtonian dynamics (MOND), to assess whether they can provide a better fit to the observed kinematics. While MOND can potentially explain the observed kinematics without invoking dark matter, it requires specific assumptions about the external field effect, which are difficult to constrain with the available data. Future observations with improved precision, particularly in proper motion measurements, will be crucial to further test the validity of Newtonian gravity in the outer halo of the Milky Way and to distinguish between different theoretical models.\n"
  },
  {
    "id": 149,
    "prompt": "Jets and environment of microquasars",
    "HWT": "Two relativistic X-ray jets have been detected with the Chandra X-ray observatory from the black hole X-ray transient XTE J1550-564. We report a full analysis of the evolution of the two jets with a gamma-ray burst external shock model. A plausible scenario suggests a cavity outside the central source and the jets first travelled with constant velocity and then are slowed down by the interactions between the jets and the interstellar medium (ISM). The best fitted radius of the cavity is $\\sim$0.36 pc on the eastern side and $\\sim$0.46 pc on the western side, and the densities also show asymmetry, of $\\sim$0.015 cm$^{-3}$ on the east to $\\sim$0.21 cm$^{-3}$ on the west. A large scale low density region is also found in another microquasar system, H 1743-322. These results are consistent with previous suggestions that the environment of microquasars should be rather vacuous, compared to the normal Galactic environment. A generic scenario for microquasar jets is proposed, classifying the observed jets into three main categories, with different jet morphologies (and sizes) corresponding to different scales of vacuous environments surrounding them.",
    "MGT": "Microquasars, stellar-mass black hole or neutron star binaries exhibiting relativistic jets, provide a unique laboratory to study accretion and ejection processes analogous to their supermassive counterparts, quasars, but on much shorter timescales. This review synthesizes current knowledge on the interaction between microquasar jets and their surrounding environments. We explore the diverse range of observed environments, from the interstellar medium to stellar winds and circumstellar disks, and how these environments influence jet propagation, morphology, and radiative output.\n\nWe examine the physical mechanisms driving jet-environment interactions, including hydrodynamic instabilities, magnetic field amplification, and particle acceleration. The resulting emission signatures, spanning radio, optical, X-ray, and gamma-ray wavelengths, are discussed in detail, highlighting their diagnostic potential for probing jet composition, power, and velocity. We review observational evidence for jet feedback effects, such as the heating and ionization of the surrounding medium, and the potential for triggering or suppressing star formation.\n\nFurthermore, we address the challenges in disentangling jet-induced emission from other sources, such as accretion disk corona and stellar activity. We present a critical assessment of theoretical models used to interpret jet-environment interactions, emphasizing their strengths and limitations. Finally, we outline future research directions, including the need for multi-wavelength observations with high spatial and temporal resolution, as well as advanced numerical simulations incorporating relativistic magnetohydrodynamics and radiative transfer. Understanding these interactions is crucial for a complete picture of microquasar physics and their role in the broader astrophysical context.\n"
  },
  {
    "id": 150,
    "prompt": "Radio and mid-infrared identification of BLAST source counterparts in the Chandra Deep Field South",
    "HWT": "We have identified radio and/or mid-infrared counterparts to 198 out of 350 sources detected at >=5 sigma over ~ 9 square degrees centered on the Chandra Deep Field South (CDFS) by the Balloon-borne Large Aperture Submillimeter Telescope (BLAST) at 250, 350 and 500 um. We have matched 114 of these counterparts to optical sources with previously derived photometric redshifts and fitted SEDs to the BLAST fluxes and fluxes at 70 and 160 um acquired with the Spitzer Space Telescope. In this way, we have constrained dust temperatures, total far-infrared/sub-millimeter luminosities and star formation rates for each source. Our findings show that on average, the BLAST sources lie at significantly lower redshifts and have significantly lower rest-frame dust temperatures compared to submm sources detected in surveys conducted at 850 um. We demonstrate that an apparent increase in dust temperature with redshift in our sample arises as a result of selection effects. Finally, we provide the full multi-wavelength catalog of >= 5 sigma BLAST sources contained within the complete ~ 9 square degree survey area.",
    "MGT": "Deep extragalactic surveys at submillimeter wavelengths have revealed a population of dusty, star-forming galaxies (DSFGs) at high redshifts. Identifying their counterparts at other wavelengths is crucial for understanding their physical properties and contribution to cosmic star formation history. This study presents a comprehensive multi-wavelength analysis aimed at identifying radio and mid-infrared (MIR) counterparts of sources detected by the Balloon-borne Large Aperture Submillimeter Telescope (BLAST) in the Chandra Deep Field South (CDFS). We cross-correlated the BLAST 250, 350, and 500 $\\mu$m catalogs with deep 1.4 GHz radio data from the Very Large Array (VLA) and 3.6 and 4.5 $\\mu$m data from the Spitzer Space Telescope. We employed a likelihood ratio method to assess the probability of association between BLAST sources and potential counterparts, considering positional uncertainties and flux densities. We identified robust radio and MIR counterparts for a significant fraction of the BLAST sources. The radio-identified DSFGs exhibit higher star formation rates and tend to reside at higher redshifts compared to those detected solely in the MIR. We also investigate the infrared spectral energy distributions (SEDs) of the counterparts, finding evidence for a range of dust temperatures and starburst ages. Our results provide valuable insights into the nature of DSFGs and their role in galaxy evolution.\n"
  },
  {
    "id": 151,
    "prompt": "Noncommutativity in space-time extended by Liouville field",
    "HWT": "The world-sheet quantum conformal invariance can be realized in the presence of the conformal factor $F$, by inclusion of Liouville term. In the background with linear dilaton field, $\\Phi(x)=\\Phi_0+a_\\mu x^\\mu$, the field $F$ becomes a new noncommutative variable. Therefore, it is natural to extend space-time with a new coordinate, $F$, in order to unify expressions for noncommutativity parameter $\\Theta^{ij}$ of the space-time coordinates $x^i$, with the part $\\Theta^i$ connecting noncommutativity between coordinates $x^i$ and $F$. In this way we solve the problems of Dp-brane noncommutativity in a more elegant way. The technical advantage uses the fact that in the extended space-time the action with dilaton field can be rewritten in dilaton free form. We use canonical method and extend its application to the derivation of boundary conditions. From requirement that Hamiltonian, as the time translation generator, has well defined derivatives in the coordinates and momenta, we obtain boundary conditions directly in the canonical form.",
    "MGT": "This paper investigates the implications of introducing a noncommutativity parameter into spacetime extended by a Liouville field. We explore a modified quantum field theory framework where the usual commutative spacetime coordinates are replaced by noncommutative ones, defined by a Lie algebra with a constant antisymmetric tensor. This deformation is then coupled to a Liouville field, which dynamically generates a curved background geometry. We analyze the impact of this noncommutative deformation on the Liouville field dynamics, focusing on the modified correlation functions and the resulting alterations to the spacetime structure. Specifically, we derive the modified propagator for the Liouville field in this noncommutative background and demonstrate how the noncommutativity parameter affects the scaling dimensions and conformal properties of the theory. Furthermore, we examine the implications of this framework for understanding quantum gravity at short distances, where noncommutative geometry is often invoked as a regularization scheme. Our results indicate that the introduction of noncommutativity leads to a suppression of short-distance singularities and a smoothing of the spacetime geometry. We also discuss the potential connections of this model to string theory and other approaches to quantum gravity, highlighting the role of the Liouville field in mediating the interaction between the noncommutative spacetime and the underlying quantum degrees of freedom. Finally, we propose future directions for research, including the investigation of higher-order corrections and the application of this model to cosmological scenarios.\n"
  },
  {
    "id": 152,
    "prompt": "Concavity for nuclear binding energies, thermodynamical functions and density functionals",
    "HWT": "Sequences of experimental ground-state energies for both odd and even $A$ are mapped onto concave patterns cured from convexities due to pairing and/or shell effects. The same patterns, completed by a list of excitation energies, give numerical estimates of the grand potential $\\Omega(\\beta, \\mu)$ for a mixture of nuclei at low or moderate temperatures $T=\\beta^{-1}$ and at many chemical potentials $\\mu. $ The average nucleon number $<{\\bf A} >(\\beta, \\mu)$ then becomes a continuous variable, allowing extrapolations towards nuclear masses closer to drip lines. We study the possible concavity of several thermodynamical functions, such as the free energy and the average energy, as functions of $<{\\bf A} >. $ Concavity, which always occur for the free energy and is usually present for the average energy, allows easy interpolations and extrapolations providing upper and lower bounds, respectively, to binding energies. Such bounds define an error bar for the prediction of binding energies. Finally we show how concavity and universality are related in the theory of the nuclear density functional.",
    "MGT": "Nuclear binding energies, thermodynamical functions, and density functionals are fundamental concepts in nuclear physics and condensed matter physics. These quantities are inherently linked to the stability and properties of nuclei and materials. However, the mathematical properties of these functions, particularly concavity, are often overlooked, despite their profound implications. This article delves into the concavity properties of nuclear binding energies, thermodynamical functions, and density functionals, exploring their connections and consequences.\n\nWe present a rigorous analysis demonstrating that under specific conditions, nuclear binding energies exhibit concave behavior as a function of nucleon number. This concavity is intrinsically linked to the saturation of nuclear forces and the stability of nuclei against fission and alpha decay. Furthermore, we investigate the concavity of thermodynamical functions, such as entropy and free energy, within the context of statistical mechanics. We show that concavity is a direct consequence of the second law of thermodynamics and ensures the stability of thermodynamic equilibrium.\n\nFinally, we examine the concavity properties of density functionals used in electronic structure calculations. We demonstrate that the concavity of the energy functional with respect to electron density is crucial for ensuring the uniqueness and stability of the ground state solution. We discuss the implications of non-concave density functionals, which can lead to spurious solutions and inaccurate predictions. By establishing the concavity properties of these fundamental quantities, we provide a deeper understanding of their underlying mathematical structure and their physical implications for nuclear stability, thermodynamic behavior, and electronic structure calculations.\n"
  },
  {
    "id": 153,
    "prompt": "Quantum corrections to solitons and BPS saturation",
    "HWT": "We review our work of the past decade on one-loop quantum corrections to the mass M and central charge Z of solitons in supersymmetric field theories: the kink, the vortex, and the monopoles (focussing on the kink and the monopoles here). In each case a new feature was needed to obtain BPS saturation: a new anomaly-like contribution to Z for the kink and the N=2 monopole, the effect of classical winding of the quantum vortex contributing to Z, surface terms contributing to M of the N=4 monopole and to Z of the N=2 and N=4 monopoles, and composite operator renormalization for the currents of the \"finite\" N=4 model. We use dimensional regularization, modified to preserve susy and be applicable to solitons, and suitable renormalization conditions. In the mode expansion of bosonic and fermionic quantum fields, zero modes appear then as massless nonzero modes.",
    "MGT": "This article explores quantum corrections to solitons and their connection to BPS saturation. We investigate how quantum fluctuations alter the classical properties of solitons, specifically focusing on mass and charge. Our analysis employs a variety of techniques, including path integral quantization and background field methods, to compute one-loop corrections. We examine the role of zero modes and their proper treatment in the quantum theory. Furthermore, we delve into the relationship between quantum corrections and BPS bounds. We demonstrate that in certain supersymmetric theories, these corrections preserve the BPS saturation condition, ensuring the stability of solitons against quantum effects. We analyze specific examples, such as kinks in scalar field theories and vortices in gauge theories, to illustrate the general principles. Our results provide insights into the quantum nature of solitons and their role in non-perturbative phenomena. We discuss the implications of our findings for understanding the phase diagrams of quantum field theories and the stability of topological defects. We also comment on the challenges and future directions in this research area.\n"
  },
  {
    "id": 154,
    "prompt": "Generation-by-Generation Dissection of the Response Function in Long Memory Epidemic Processes",
    "HWT": "In a number of natural and social systems, the response to an exogenous shock relaxes back to the average level according to a long-memory kernel $\\sim 1/t^{1+\\theta}$ with $0 \\leq \\theta <1$. In the presence of an epidemic-like process of triggered shocks developing in a cascade of generations at or close to criticality, this \"bare\" kernel is renormalized into an even slower decaying response function $\\sim 1/t^{1-\\theta}$. Surprisingly, this means that the shorter the memory of the bare kernel (the larger $1+\\theta$), the longer the memory of the response function (the smaller $1-\\theta$). Here, we present a detailed investigation of this paradoxical behavior based on a generation-by-generation decomposition of the total response function, the use of Laplace transforms and of \"anomalous\" scaling arguments. The paradox is explained by the fact that the number of triggered generations grows anomalously with time at $\\sim t^\\theta$ so that the contributions of active generations up to time $t$ more than compensate the shorter memory associated with a larger exponent $\\theta$. This anomalous scaling results fundamentally from the property that the expected waiting time is infinite for $0 \\leq \\theta \\leq 1$. The techniques developed here are also applied to the case $\\theta >1$ and we find in this case that the total renormalized response is a {\\bf constant} for $t < 1/(1-n)$ followed by a cross-over to $\\sim 1/t^{1+\\theta}$ for $t \\gg 1/(1-n)$.",
    "MGT": "Long memory epidemic processes, characterized by slowly decaying autocorrelations and prolonged impacts of past events, pose significant challenges for understanding and predicting disease dynamics. Traditional compartmental models, often based on Markovian assumptions, struggle to capture these persistent dependencies. This article introduces a novel framework for dissecting the response function in long memory epidemic models, focusing on the generation-by-generation evolution of the infection process. We develop a stochastic model that explicitly incorporates a memory kernel, representing the influence of past infections on current transmission rates. This memory kernel allows us to capture the long-range dependencies characteristic of long memory processes.\n\nOur approach involves decomposing the overall response function into a series of generation-specific response functions. Each generation-specific response function quantifies the impact of infections occurring in a specific generation on the subsequent spread of the disease. By analyzing the shape and magnitude of these generation-specific response functions, we gain insights into the evolving dynamics of the epidemic and the mechanisms driving long memory behavior. We derive analytical expressions for these response functions under various assumptions about the memory kernel and the underlying transmission dynamics.\n\nFurthermore, we explore the implications of different memory kernel structures on the shape of the generation-specific response functions. We demonstrate that the decay rate of the memory kernel significantly influences the persistence of the epidemic and the shape of the response functions. Specifically, slower decaying memory kernels lead to more prolonged epidemic waves and broader, more persistent generation-specific response functions. Numerical simulations are used to validate our analytical results and to explore the behavior of the model under more complex scenarios, including heterogeneous populations and time-varying transmission rates.\n\nFinally, we discuss the potential applications of our framework for analyzing real-world epidemic data and for developing more effective control strategies for long memory epidemics. By understanding the generation-by-generation evolution of the response function, we can better target interventions and predict the long-term impact of control measures. This approach provides a valuable tool for researchers and policymakers seeking to understand and manage the complex dynamics of long memory epidemic processes.\n"
  },
  {
    "id": 155,
    "prompt": "Diffusivity bounds for 1D Brownian polymers",
    "HWT": "We study the asymptotic behavior of a self-interacting one-dimensional Brownian polymer first introduced by Durrett and Rogers [Probab. Theory Related Fields 92 (1992) 337--349]. The polymer describes a stochastic process with a drift which is a certain average of its local time. We show that a smeared out version of the local time function as viewed from the actual position of the process is a Markov process in a suitably chosen function space, and that this process has a Gaussian stationary measure. As a first consequence, this enables us to partially prove a conjecture about the law of large numbers for the end-to-end displacement of the polymer formulated in Durrett and Rogers [Probab. Theory Related Fields 92 (1992) 337--349]. Next we give upper and lower bounds for the variance of the process under the stationary measure, in terms of the qualitative infrared behavior of the interaction function. In particular, we show that in the locally self-repelling case (when the process is essentially pushed by the negative gradient of its own local time) the process is super-diffusive.",
    "MGT": "We investigate diffusivity bounds for one-dimensional Brownian polymers, focusing on the interplay between topological constraints and chain dynamics. Using a combination of analytical techniques and numerical simulations, we derive upper and lower bounds on the center-of-mass diffusion coefficient, $D_{cm}$, as a function of chain length, $N$, and the strength of the confining potential. Our analysis reveals that entanglements, induced by the uncrossability of polymer strands, significantly impact the scaling behavior of $D_{cm}$. Specifically, we demonstrate that in the presence of strong confinement, $D_{cm}$ scales as $N^{-2}$, indicative of Rouse-like dynamics but with an effective friction coefficient enhanced by topological interactions. Conversely, in the weak confinement regime, deviations from the $N^{-2}$ scaling are observed, suggesting the emergence of more complex dynamical behavior. We further explore the influence of excluded volume interactions on the diffusivity bounds, finding that these interactions can lead to a crossover in the scaling exponent of $D_{cm}$. Our results provide valuable insights into the fundamental limits on polymer diffusion and have implications for understanding the transport properties of confined macromolecules in various biological and synthetic systems. Finally, we compare our theoretical predictions with experimental data for DNA molecules confined in nanochannels, demonstrating good agreement and highlighting the predictive power of our approach.\n"
  },
  {
    "id": 156,
    "prompt": "$H^1$ and dyadic $H^1$",
    "HWT": "In this paper we give a simple proof of the fact that the average over all dyadic lattices of the dyadic $H^1$-norm of a function gives an equivalent $H^1$-norm. The proof we present works for both one-parameter and multi-parameter Hardy spaces. The results of such type are known. The first result (for one-parameter Hardy spces) belongs to Burgess Davis (1980). Also, by duality, such results are equivalent to the \"BMO from dyadic BMO\" statements proved by Garnett-Jones(1982} for one parameter case, and by Pipher-Ward (2008) for two-parameter case. While the paper generalizes these results to the multi-parameter setting, this is not its main goal. The purpose of the paper is to present an approach leading to a simple proof, which works in both one-parameter and multi-parameter cases. The main idea of treating square function as a Calderon--Zygmind operator is a commonplace in harmonic analysis; the main observation, on which the paper is based, is that one can treat the random dyadic square function this way. After that, all is proved by using the standard and well-known results about Calderon--Zygmind operators in the Hilbert-space-valued setting. As an added bonus, we get a simple proof of the (equivalent by duality) inclusion $\\text{BMO}\\subset \\text{BMO}_d$, $H^1_d \\subset H^1$ in the multi-parameter case. Note, that unlike the one-parameter case, the inclusions in the general situation are far from trivial.",
    "MGT": "This paper investigates the relationship between the classical Hardy space $H^1(\\mathbb{R})$ and its dyadic counterpart, $H^1_{dyadic}(\\mathbb{R})$, focusing on their structural similarities and differences. We explore the extent to which results established for one space can be transferred to the other, particularly concerning atomic decompositions, maximal function characterizations, and interpolation properties. The study delves into the atomic decomposition of functions in $H^1_{dyadic}(\\mathbb{R})$, providing a detailed analysis of the conditions under which a function can be represented as a sum of dyadic atoms. We then examine the connection between the dyadic maximal function and the classical Hardy-Littlewood maximal function, establishing sharp inequalities that relate their respective norms. This allows us to characterize $H^1_{dyadic}(\\mathbb{R})$ in terms of the dyadic maximal function, mirroring the well-known maximal function characterization of $H^1(\\mathbb{R})$.\n\nFurthermore, we investigate the interpolation properties of $H^1_{dyadic}(\\mathbb{R})$, considering both real and complex interpolation methods. We demonstrate that while certain interpolation results hold analogously to the classical case, subtle differences arise due to the dyadic structure. Specifically, we examine the behavior of operators that are bounded from $H^1_{dyadic}(\\mathbb{R})$ into $L^1(\\mathbb{R})$ and from $L^\\infty(\\mathbb{R})$ into BMO, and determine the intermediate spaces obtained through interpolation. We also discuss the limitations of transferring classical results directly to the dyadic setting, highlighting the specific features of the dyadic structure that necessitate modified approaches. The analysis includes examples that illustrate the disparities between the two spaces, providing a nuanced understanding of their respective properties. The paper concludes with a discussion of open problems and potential avenues for further research, particularly concerning the application of dyadic harmonic analysis to problems in other areas of mathematics.\n"
  },
  {
    "id": 157,
    "prompt": "Constraints on the Cosmological Constant due to Scale Invariance",
    "HWT": "We consider the standard model with local scale invariance. The theory shows exact scale invariance of dimensionally regulated action. We show that massless gauge fields, which may be abelian or non-abelian, lead to vanishing contribution to the cosmological constant in this theory. This result follows in the quantum theory, to all orders in the gauge couplings. However we have not considered contributions higher orders in the gravitational coupling. Similarly we also find that massless fermion fields yield null contribution to the cosmological constant. The effective cosmological constant in this theory is non-zero due to the phenomenon of cosmological symmetry breaking, which also gives masses to all the massive fields, besides generating the Planck mass. We find a simple relationship between the curvature scalar and the vacuum value of the Higgs field in the limit when we ignore all other contributions to the energy density besides the vacuum energy.",
    "MGT": "The cosmological constant, a cornerstone of the standard cosmological model, presents a significant theoretical challenge due to its unnaturally small value compared to theoretical predictions. This work explores an alternative framework where the cosmological constant emerges as a consequence of imposing scale invariance on the gravitational action. We investigate the implications of this constraint on the allowed values of the cosmological constant and its relationship to other fundamental parameters. Specifically, we analyze a modified Einstein-Hilbert action incorporating a scalar field coupled to gravity, demanding that the resulting equations of motion remain invariant under scale transformations. Our analysis reveals that scale invariance imposes stringent conditions on the potential of the scalar field, potentially leading to a dynamically generated cosmological constant. We derive bounds on the cosmological constant based on observational data, including the cosmic microwave background and supernova observations, within this scale-invariant framework. The results suggest that a scale-invariant theory can accommodate a small, non-zero cosmological constant while alleviating the fine-tuning problem. Furthermore, we discuss the potential for future observations to further refine these constraints and discriminate between scale-invariant models and the standard cosmological model.\n"
  },
  {
    "id": 158,
    "prompt": "Total Variation, Adaptive Total Variation and Nonconvex Smoothly Clipped Absolute Deviation Penalty for Denoising Blocky Images",
    "HWT": "The total variation-based image denoising model has been generalized and extended in numerous ways, improving its performance in different contexts. We propose a new penalty function motivated by the recent progress in the statistical literature on high-dimensional variable selection. Using a particular instantiation of the majorization-minimization algorithm, the optimization problem can be efficiently solved and the computational procedure realized is similar to the spatially adaptive total variation model. Our two-pixel image model shows theoretically that the new penalty function solves the bias problem inherent in the total variation model. The superior performance of the new penalty is demonstrated through several experiments. Our investigation is limited to \"blocky\" images which have small total variation.",
    "MGT": "This article explores image denoising techniques, focusing on Total Variation (TV), Adaptive Total Variation (ATV), and a novel approach using the Smoothly Clipped Absolute Deviation (SCAD) penalty within a nonconvex optimization framework. Blocky images, characterized by piecewise constant regions separated by sharp edges, pose challenges for traditional denoising methods. TV regularization effectively preserves edges but can lead to excessive staircasing artifacts. ATV addresses this by adaptively adjusting the regularization strength based on local image features. We propose incorporating the SCAD penalty, known for its sparsity-promoting properties and ability to reduce bias, to further enhance denoising performance. An efficient algorithm based on alternating direction method of multipliers (ADMM) is developed to solve the resulting nonconvex optimization problem. Experimental results on synthetic and real-world blocky images demonstrate the superiority of the proposed SCAD-based approach over standard TV and ATV methods in terms of peak signal-to-noise ratio (PSNR) and visual quality, effectively reducing both noise and staircasing artifacts while preserving important image details.\n"
  },
  {
    "id": 159,
    "prompt": "Graphical Probabilistic Routing Model for OBS Networks with Realistic Traffic Scenario",
    "HWT": "Burst contention is a well-known challenging problem in Optical Burst Switching (OBS) networks. Contention resolution approaches are always reactive and attempt to minimize the BLR based on local information available at the core node. On the other hand, a proactive approach that avoids burst losses before they occur is desirable. To reduce the probability of burst contention, a more robust routing algorithm than the shortest path is needed. This paper proposes a new routing mechanism for JET-based OBS networks, called Graphical Probabilistic Routing Model (GPRM) that selects less utilized links, on a hop-by-hop basis by using a bayesian network. We assume no wavelength conversion and no buffering to be available at the core nodes of the OBS network. We simulate the proposed approach under dynamic load to demonstrate that it reduces the Burst Loss Ratio (BLR) compared to static approaches by using Network Simulator 2 (ns-2) on NSFnet network topology and with realistic traffic matrix. Simulation results clearly show that the proposed approach outperforms static approaches in terms of BLR.",
    "MGT": "Optical burst switching (OBS) is a promising paradigm for next-generation optical networks. However, the lack of optical buffers and the burst contention problem pose significant challenges. Efficient routing algorithms are crucial for mitigating contention and improving network performance. This paper introduces a novel graphical probabilistic routing model specifically designed for OBS networks operating under realistic traffic scenarios. The model leverages a directed acyclic graph to represent the network topology and employs probabilistic link weights that dynamically adapt based on real-time network conditions and historical traffic patterns. These probabilistic weights reflect the likelihood of successful burst transmission along each link, considering factors such as burst arrival rates, burst lengths, and wavelength availability. A key contribution is the integration of a traffic prediction module that uses time series analysis to forecast future traffic demands, enabling proactive route selection that anticipates potential congestion points. The proposed routing algorithm dynamically updates the probabilistic link weights based on the predicted traffic matrix, guiding bursts along paths with lower contention probabilities. Simulation results, conducted using a detailed OBS network simulator and realistic traffic patterns, demonstrate that the proposed graphical probabilistic routing model significantly outperforms traditional shortest path and fixed alternate routing schemes in terms of burst loss probability, end-to-end delay, and overall network throughput. The results highlight the effectiveness of the model in adapting to dynamic traffic conditions and mitigating the impact of burst contention in OBS networks.\n"
  },
  {
    "id": 160,
    "prompt": "Technicolor and Lattice Gauge Theory",
    "HWT": "Technicolor and other theories of dynamical electroweak symmetry breaking invoke chiral symmetry breaking triggered by strong gauge-dynamics, analogous to that found in QCD, to explain the observed W, Z, and fermion masses. In this talk we describe why a realistic theory of dynamical electroweak symmetry breaking must, relative to QCD, produce an enhanced fermion condensate. We quantify the degree to which the technicolor condensate must be enhanced in order to yield the observed quark masses, and still be consistent with phenomenological constraints on flavor-changing neutral-currents. Lattice studies of technicolor and related theories provide the only way to demonstrate that such enhancements are possible and, hopefully, to discover viable candidate models. We comment briefly on the current status of non-perturbative investigations of dynamical electroweak symmetry breaking, and provide a \"wish-list\" of phenomenologically-relevant properties that are important to calculate in these theories",
    "MGT": "Technicolor, an appealingly economical alternative to the Standard Model Higgs mechanism, posits new strong dynamics to break electroweak symmetry. However, simple Technicolor models face phenomenological challenges, notably predicting excessive flavor-changing neutral currents and large electroweak precision parameters. Walking Technicolor attempts to address these issues by introducing a slowly running (\"walking\") gauge coupling, prolonging the near-conformal regime. Lattice gauge theory provides a non-perturbative framework to investigate the dynamics of such theories. This article reviews recent progress in lattice studies of gauge theories relevant to Technicolor, focusing on SU(N) gauge theories with fermions in various representations. We discuss the determination of the spectrum, the running of the coupling, and the chiral condensate. We explore the challenges in simulating these theories, including finite-size effects and the need for chiral extrapolations. Furthermore, we highlight the implications of lattice results for Technicolor model building, specifically regarding the viability of Walking Technicolor and the constraints imposed by electroweak precision data. Finally, we outline future directions for lattice research in this area, emphasizing the importance of improved algorithms and computational resources.\n"
  },
  {
    "id": 161,
    "prompt": "Characteristics of Anemone Active Regions Appearing in Coronal Holes Observed with {\\it Yohkoh} Soft X-ray Telescope",
    "HWT": "Coronal structure of active regions appearing in coronal holes is studied by using the data obtained with the Soft X-Ray Telescope (SXT) aboard {\\it Yohkoh} from 1991 November to 1993 March. The following characteristics are found; Many of active regions appearing in coronal holes show a structure that looks like a ``sea-anemone''. Such active regions are called {\\it anemone ARs}. About one-forth of all active regions that were observed with SXT from their births showed the anemone structure. For almost all the anemone ARs, the order of magnetic polarities is consistent with the Hale-Nicholson's polarity law. These anemone ARs also showed more or less east-west asymmetry in X-ray intensity distribution, such that the following (eastern) part of the ARs is brighter than its preceding (western) part. This, as well as the anemone shape itself, is consistent with the magnetic polarity distribution around the anemone ARs. These observations also suggest that an active region appearing in coronal holes has simpler (less sheared) and more preceding-spot-dominant magnetic structure than those appearing in other regions.",
    "MGT": "Anemone active regions (AARs) are small, short-lived active regions characterized by a cluster of small loops rooted in a common area, resembling a sea anemone. We present a statistical study of AARs appearing within coronal holes (CHs) using observations from the {\\it Yohkoh} Soft X-ray Telescope (SXT). We analyzed SXT synoptic maps from 1991 to 2001, identifying 136 AARs situated within CHs. We investigated their spatial distribution, lifetime, and soft X-ray flux. The AARs tend to be concentrated near the boundaries of CHs. The average lifetime of these AARs is approximately 12 hours, significantly shorter than typical active regions. The soft X-ray flux emitted by these AARs is considerably weaker than that of standard active regions. We found a positive correlation between the size of the AAR and its lifetime. Furthermore, a weak correlation exists between the AAR's flux and its lifetime. These results suggest that AARs in CHs are manifestations of small-scale magnetic flux emergence and cancellation, possibly driven by local dynamo processes within the CH environment. The ephemeral nature and weak flux of these AARs suggest they play a role in the open magnetic flux budget of the Sun.\n"
  },
  {
    "id": 162,
    "prompt": "The Nearby and Extremely Metal-Poor Galaxy CGCG 269-049",
    "HWT": "We present Hubble Space Telescope (HST) and Spitzer Space Telescope images and photometry of the extremely metal-poor (Z = 0.03 Z_sol) blue dwarf galaxy CGCG 269-049. The HST images reveal a large population of red giant and asymptotic giant branch stars, ruling out the possibility that the galaxy has recently formed. From the magnitude of the tip of the red giant branch, we measure a distance to CGCG 269-049 of only 4.9 +/- 0.4 Mpc. The spectral energy distribution of the galaxy between ~3.6 - 70 microns is also best fitted by emission from predominantly ~10 Gyr old stars, with a component of thermal dust emission having a temperature of 52 +/- 10 K. The HST and Spitzer photometry indicate that more than 60% of CGCG 269-049's stellar mass consists of stars ~10 Gyr old, similar to other local blue dwarf galaxies. Our HST H-alpha image shows no evidence of a supernova-driven outflow that could be removing metals from the galaxy, nor do we find evidence that such outflows occurred in the past. Taken together with CGCG 269-049's large ratio of neutral hydrogen mass to stellar mass (~10), these results are consistent with recent simulations in which the metal deficiency of local dwarf galaxies results mainly from inefficient star formation, rather than youth or the escape of supernova ejecta.",
    "MGT": "We present a comprehensive analysis of the nearby, extremely metal-poor galaxy CGCG 269-049, utilizing deep imaging and spectroscopic data obtained from the Dark Energy Camera (DECam) and the Goodman Spectrograph on the Southern Astrophysical Research (SOAR) telescope. CGCG 269-049, located at a distance of approximately 10 Mpc, exhibits a very low metallicity, estimated at 12+log(O/H) ≈ 7.1, making it one of the most metal-poor galaxies in the local universe. Our photometric analysis of the resolved stellar populations reveals a well-defined red giant branch (RGB) and a prominent blue plume, indicative of ongoing star formation. We derive a star formation history (SFH) showing a dominant old stellar population (>10 Gyr) with intermittent bursts of star formation continuing to the present day. The current star formation rate is estimated to be low, but non-negligible.\n\nSpectroscopic observations of HII regions within CGCG 269-049 allow for a direct measurement of the oxygen abundance and provide insights into the ionization conditions. We observe unusually high [OIII]/Hβ ratios for a galaxy of this metallicity, potentially suggestive of a hard ionizing spectrum or density-bounded nebulae. We analyze the α-element enhancement in the galaxy using multiple methods. We also analyze the kinematics of the ionized gas, finding evidence of a relatively quiescent velocity field.\n\nWe compare the properties of CGCG 269-049 with other extremely metal-poor galaxies (XMPGs) and dwarf irregular galaxies in the local volume, highlighting its similarities and differences. The combination of its proximity, extreme metal-poor nature, and ongoing star formation makes CGCG 269-049 a valuable laboratory for studying the processes of galaxy formation and chemical evolution in an environment analogous to the early universe. Our findings contribute to a better understanding of the formation and evolution of low-mass galaxies and the nature of star formation in metal-poor environments.\n"
  },
  {
    "id": 163,
    "prompt": "Spin-Dynamics of the antiferromagnetic S=1/2-Chain at finite magnetic Fields and intermediate Temperatures",
    "HWT": "We present a study of the dynamic structure factor of the antiferromagnetic spin-1/2 Heisenberg chain at finite temperatures and finite magnetic fields. Using Quantum-Monte-Carlo based on the stochastic series expansion and Maximum-Entropy methods we evaluate the longitudinal and the transverse dynamic structure factor from vanishing magnetic fields up to and above the threshold $B_c$ for ferromagnetic saturation, as well as for high and for intermediate temperatures. We study the field-induced redistribution of spectral weight contrasting longitudinal versus transverse excitations. At finite fields below saturation incommensurate low-energy modes are found consistent with zero temperature Bethe-Ansatz. The crossover between the field induced ferromagnet above $B_c$ and the Luttinger liquid below $B_c$ is analyzed in terms of the transverse spin-dynamics. Evaluating sum-rules we assess the quality of the analytic continuation and demonstrate excellent consistency of the Maximum-Entropy results.",
    "MGT": "We investigate the spin dynamics of the antiferromagnetic S=1/2 chain in the presence of a finite magnetic field at intermediate temperatures, employing a combination of analytical and numerical techniques. Our approach leverages the Bethe ansatz solution to derive exact expressions for the low-energy excitations, which are then used to construct an effective field theory describing the system's dynamics. We calculate the dynamic structure factor, S(q,ω), focusing on the contributions from both spinon and bound-state excitations. Numerical simulations, based on the Density Matrix Renormalization Group (DMRG) method, are performed to validate the theoretical predictions and explore the system's behavior beyond the reach of analytical calculations. The results reveal a complex interplay between the magnetic field, temperature, and quantum fluctuations, leading to a rich spectrum of spin excitations. Furthermore, we analyze the temperature dependence of the spectral weight and linewidth of the dominant modes, providing insights into the thermal broadening and damping mechanisms. The findings offer a comprehensive understanding of the spin dynamics in this paradigmatic quantum magnet and are relevant to the interpretation of experimental data from quasi-one-dimensional antiferromagnets.\n"
  },
  {
    "id": 164,
    "prompt": "Millisecond microwave spikes: statistical study and application for plasma diagnostics",
    "HWT": "We analyze a dense cluster of solar radio spikes registered at ~ 4.5 -- 6 GHz by the Purple Mountain Observatory spectrometer (Nanjing, China) operating in the 4.5 -- 7.5 GHz range with the 5 ms temporal resolution. To handle with the data from the spectrometer we developed a new technique utilizing a nonlinear multi-Gaussian spectral fit based on chi-squared criteria to extract individual spikes from the originally recorded spectra. Applying this method to the experimental raw data we eventually identified about 3000 spikes for this event, which allows for a detailed statistical analysis. Various statistical characteristics of the spikes have been evaluated, including intensity distributions, spectral bandwidth distributions, and distribution of the spike mean frequencies. The most striking finding of this analysis is distributions of the spike bandwidth, which are remarkably asymmetric. To reveal the underlaying microphysics we explore the local trap model with the renormalized theory of spectral profile of the electron cyclotron maser (ECM) emission peak in a source with random magnetic irregularities. The distribution of the solar spikes relative bandwidth calculated within the local trap model represents an excellent fit to the experimental data. Accordingly, the developed technique may offer a new tool of studying very low levels of the magnetic turbulence in the spike sources, when the ECM mechanism of the spike cluster is confirmed.",
    "MGT": "Millisecond microwave spikes (MMS) are frequently observed in various plasma experiments, characterized by their short duration and rapid power fluctuations. This study presents a comprehensive statistical analysis of MMS observed in a low-temperature helicon plasma, focusing on their occurrence rate, amplitude distribution, and temporal correlation. A large dataset of microwave signals, acquired over a range of plasma parameters, including gas pressure and radio-frequency power, was analyzed. The statistical distributions of MMS amplitudes were found to deviate significantly from Gaussian behavior, exhibiting heavy tails indicative of non-linear processes governing their generation. Furthermore, the temporal correlation analysis revealed a tendency for MMS to cluster in time, suggesting the presence of underlying mechanisms that promote their formation.\n\nBased on these statistical properties, a novel diagnostic technique for plasma characterization is proposed. By analyzing the MMS occurrence rate and amplitude distribution, information about the plasma density and electron temperature can be inferred. The rationale behind this approach lies in the hypothesized link between MMS generation and specific plasma instabilities, which are sensitive to these fundamental plasma parameters. Preliminary experimental results demonstrate the feasibility of this diagnostic method, showing a clear correlation between MMS characteristics and independently measured plasma parameters using Langmuir probes. The advantages of this technique include its non-invasive nature, high temporal resolution, and potential for real-time plasma monitoring. The limitations and future directions of this research, including the development of a theoretical model to explain the observed MMS behavior and the exploration of its applicability to other plasma environments, are also discussed.\n"
  },
  {
    "id": 165,
    "prompt": "Antibunching correlations in a strongly coupled exciton - photonic crystal cavity system: Role of off-resonant coupling to multiple excitons",
    "HWT": "We employ a master equation approach to study the second-order quantum autocorrelation functions for up to two independent quantum dot excitons, coupled to an off-resonant cavity in a photonic crystal - single quantum dot system. For a single coupled off-resonant exciton, we observe novel oscillatory behaviour in the early-time dynamics of the cavity autocorrelation function, which leads to decreased antibunching relative to the exciton mode. With a second coupled exciton in the system, we find that the magnitude and the lifetime of these oscillations greatly increases, since the cavity is then able to exchange photons with multiple excitonic resonances. We unambiguously show that this spoils the antibunching characteristics of the cavity quasi-mode, while the autocorrelation of the first exciton is unaffected. We also examine the effects of detector time resolution and make a direct connection to a series of recent experiments.",
    "MGT": "We investigate the second-order correlation function, g(2)(τ), of light emitted from a system comprising a single quantum dot strongly coupled to a photonic crystal cavity, considering the influence of off-resonant coupling to multiple excitons. Utilizing a master equation approach, we model the system's dynamics, incorporating dephasing, cavity decay, and exciton recombination. Our analysis reveals that while strong coupling leads to clear antibunching, the presence of multiple, weakly coupled excitons significantly modifies the g(2)(0) value. Specifically, off-resonant coupling to these additional excitons can degrade the antibunching, increasing g(2)(0) above zero. We explore the parameter space, examining the impact of detuning, coupling strengths, and the number of off-resonant excitons. We find that even weak coupling to several off-resonant excitons can measurably influence the observed photon statistics. This highlights the importance of considering the full exciton landscape when designing and interpreting experiments involving cavity-quantum electrodynamics with semiconductor quantum dots. The results provide insights into achieving optimal single-photon sources and understanding the limitations imposed by multi-exciton effects.\n"
  },
  {
    "id": 166,
    "prompt": "Black hole mass and variability in quasars",
    "HWT": "We report on a study that finds a positive correlation between black hole mass and variability amplitude in quasars. Roughly 100 quasars at z<0.75 were selected by matching objects from the QUEST1 Variability Survey with broad-lined objects from the Sloan Digital Sky Survey. Black hole masses were estimated with the virial method using the broad Hbeta line, and variability was characterized from the QUEST1 light curves. The correlation between black hole mass and variability amplitude is significant at the 99% level or better and does not appear to be caused by obvious selection effects inherent to flux-limited samples. It is most evident for rest frame time lags of the order a few months up to the QUEST1 maximum temporal resolution of about 2 years. The correlation between black hole mass and variability amplitude means that the more massive black holes have larger percentage flux variations. Over 2-3 orders of magnitude in black hole mass, the amplitude increases by approximately 0.2 mag. A likely explanation for the correlation is that the more massive black holes are starving and produce larger flux variations because they do not have a steady inflow of gaseous fuel. Assuming that the variability arises from changes in the accretion rate Li & Cao [8] show that flux variations similar to those observed are expected as a consequence of the more massive black holes having cooler accretion disks.",
    "MGT": "The relationship between black hole mass (M$_{BH}$) and variability amplitude in quasars offers a valuable tool for estimating M$_{BH}$ and probing the accretion disk physics. This study investigates this relationship using a large sample of quasars with well-determined M$_{BH}$ values derived from single-epoch spectra and variability measurements from long-term optical light curves. We analyze a sample of approximately 5,000 quasars from the Sloan Digital Sky Survey (SDSS) with available RM-based black hole mass estimates and multi-epoch photometric data from the Catalina Real-Time Transient Survey (CRTS). We quantify the variability amplitude using the structure function (SF) and excess variance ($\\sigma^2_{rms}$), computed over different timescales. We examine the correlation between M$_{BH}$ and variability amplitude, accounting for redshift and luminosity dependencies. Our results confirm a significant anti-correlation between M$_{BH}$ and variability amplitude, such that more massive black holes exhibit lower variability. We find that the anti-correlation is stronger when using longer timescales for the structure function. We explore the potential influence of Eddington ratio (L/L$_{Edd}$) on the M$_{BH}$ - variability relation. We observe that at a given M$_{BH}$, quasars with higher Eddington ratios tend to show increased variability. We also investigate the impact of host galaxy contamination on the observed variability. Our findings suggest that host galaxy light can dilute the variability signal, particularly for lower-luminosity quasars. The observed anti-correlation between M$_{BH}$ and variability amplitude is consistent with models where the accretion disk fluctuations are driven by variations in the mass accretion rate. The observed dependencies on Eddington ratio and timescale offer insights into the physical processes governing accretion disk dynamics and the generation of optical continuum emission in quasars. These results have implications for estimating black hole masses in large quasar samples and for understanding the connection between black hole growth and galaxy evolution.\n"
  },
  {
    "id": 167,
    "prompt": "Determinant Quantum Monte Carlo Study of the Orbitally Selective Mott Transition",
    "HWT": "We study the conductivity, density of states, and magnetic correlations of a two dimensional, two band fermion Hubbard model using determinant Quantum Monte Carlo (DQMC) simulations. We show that an orbitally selective Mott transition (OSMT) occurs in which the more weakly interacting band can be metallic despite complete localization of the strongly interacting band. The DQMC method allows us to test the validity of the use of a momentum independent self-energy which has been a central approximation in previous OSMT studies. In addition, we show that long range antiferromagnetic order (LRAFO) is established in the insulating phase, similar to the single band, square lattice Hubbard Hamiltonian. Because the critical interaction strengths for the onset of insulating behavior are much less than the bandwidth of the itinerant orbital, we suggest that the development of LRAFO plays a key role in the transitions.",
    "MGT": "The orbitally selective Mott transition (OSMT), a phenomenon where some orbitals in a multi-orbital system become Mott insulating while others remain metallic, is investigated using determinant quantum Monte Carlo (DQMC). Focusing on a two-orbital Hubbard model, we explore the parameter space of interaction strength, Hund's coupling, and orbital splitting to map out the phase diagram. Our DQMC simulations reveal a robust OSMT regime characterized by distinct quasiparticle weights and double occupancies for each orbital. We observe a significant enhancement of the effective mass in the insulating orbital near the transition, indicative of strong correlation effects. Furthermore, we analyze the evolution of the local magnetic moments and charge fluctuations, providing insights into the underlying mechanisms driving the OSMT. The results highlight the crucial role of Hund's coupling in stabilizing the OSMT phase and suppressing inter-orbital coherence. The study demonstrates the capability of DQMC in capturing the complex interplay between orbital differentiation and strong correlations in multi-orbital materials, paving the way for a better understanding of correlated electron physics in real materials.\n"
  },
  {
    "id": 168,
    "prompt": "Storage of Quantum Coherences as Phase Labeled Local Polarization in Solid State NMR",
    "HWT": "Nuclear spins are promising candidates for quantum information processing because their good isolation from the environment precludes the rapid loss of quantum coherence. Many strategies have been developed to further extend their decoherence times. Some of them make use of decoupling techniques based on the Carr-Purcell and Carr-Purcell-Meiboom-Gill pulse sequences. In many cases, when applied to inhomogeneous samples, they yield a magnetization decay much slower than the Hahn echo. However, we have proved that these decays cannot be associated with longer decoherence times as coherences remain frozen. They result from coherences recovered after their storage as local polarization and thus they can be used as memories. We show here how this freezing of the coherent state, which can subsequently be recovered after times longer than the natural decoherence time of the system, can be generated in a controlled way with the use of field gradients. A similar behaviour of homogeneous samples in inhomogeneous fields are demonstrated. It is emphasized that the effects of inhomogeneities in solid state NMR, independently of their origin, should not be disregarded as they play a crucial role in multipulse sequences.",
    "MGT": "Quantum coherences represent a crucial resource for quantum information processing and spectroscopy. However, their inherent fragility due to decoherence poses a significant challenge for practical applications. This work explores a novel strategy for protecting quantum coherences by encoding them as phase-labeled local polarization in solid-state nuclear magnetic resonance (NMR). Specifically, we demonstrate the reversible transfer of quantum coherences to symmetry-protected subspaces, where they are stored as robust local polarization. The phase information associated with the original coherences is preserved through carefully designed radiofrequency pulse sequences, allowing for faithful retrieval upon demand. Using a model system of adamantane, we experimentally validate the storage and retrieval of single- and multi-quantum coherences with extended lifetimes compared to direct observation. We further investigate the influence of pulse imperfections and relaxation mechanisms on the storage fidelity. Our results highlight the potential of this approach for extending coherence lifetimes and enabling more complex quantum experiments in solid-state NMR, offering a promising pathway towards robust quantum information storage and manipulation in condensed matter systems. This method provides a framework for developing advanced spectroscopic techniques and quantum control strategies leveraging the inherent stability of local polarization.\n"
  },
  {
    "id": 169,
    "prompt": "Heat Transfer in Underground Rail Tunnels",
    "HWT": "The transfer of heat between the air and surrounding soil in underground tunnels ins investigated, as part of the analysis of environmental conditions in underground rail systems. Using standard turbulent modelling assumptions, flow profiles are obtained in both open tunnels and in the annulus between a tunnel wall and a moving train, from which the heat transfer coefficient between the air and tunnel wall is computed. The radial conduction of heat through the surrounding soil resulting from changes in the temperature of air in the tunnel are determined. An impulse change and an oscillating tunnel air temperature are considered separately. The correlations between fluctuations in heat transfer coefficient and air temperature are found to increase the mean soil temperature. Finally, a model for the coupled evolution of the air and surrounding soil temperature along a tunnel of finite length is given.",
    "MGT": "Underground rail tunnels represent complex thermal environments influenced by heat generated from train operations, ventilation systems, and the surrounding geological strata. This study investigates heat transfer mechanisms within these tunnels, focusing on convective, conductive, and radiative heat transfer processes. A three-dimensional computational fluid dynamics (CFD) model is developed and validated against experimental data to simulate the thermal behavior in a typical underground rail tunnel. The model incorporates detailed representations of the tunnel geometry, train movement, ventilation airflow, and the thermal properties of the surrounding soil and tunnel lining. Simulations are performed to analyze the impact of train speed, ventilation rate, and soil thermal conductivity on the temperature distribution within the tunnel. Results indicate that convective heat transfer dominates near the train and ventilation inlets, while conductive heat transfer is significant in the tunnel lining and surrounding soil. The study provides insights into the thermal management strategies for underground rail tunnels, contributing to improved energy efficiency, passenger comfort, and structural integrity.\n"
  },
  {
    "id": 170,
    "prompt": "On the fate of vacuum bubbles on matter backgrounds",
    "HWT": "In this letter we discuss cosmological first order phase transitions with de Sitter bubbles nucleating on (inhomogeneous) matter backgrounds. The de Sitter bubble can be a toy model for an inflationary phase of universes like our own. Using the thin wall approximation and the Israel junction method we trace the classical evolution of the formed bubbles within a compound model. We first address homogeneous ambient space (FRW model) and already find that bubbles nucleated in a dust dominated background cannot expand. For an inhomogeneous dust background (LTB model) we describe cases with at least initially expanding bubbles. Yet, an ensuing passage of the bubble wall through ambient curvature inhomogeneities remains unnoticed for observers inside the bubble. Notable effects also for interior observers are found in the case of a rapid background phase transition in a FRW model.",
    "MGT": "The dynamics of vacuum bubbles expanding into a matter-dominated universe are explored, focusing on the interplay between the bubble's energy density and the surrounding matter. We analyze the bubble's expansion rate, considering the effects of both the bubble's surface tension and the gravitational influence of the background matter. Through numerical simulations, we demonstrate that the presence of matter significantly alters the bubble's evolution compared to its behavior in a purely de Sitter or Minkowski spacetime. Specifically, we find that the expansion can be decelerated and even halted by the increasing matter density, leading to the potential formation of stable or metastable bubbles. Furthermore, we investigate the impact of different equation-of-state parameters for the background matter on the bubble's trajectory. The results highlight the importance of accounting for the cosmological background when studying vacuum decay and its potential implications for the early universe and the stability of the Standard Model vacuum. The long-term fate of these bubbles, whether they collapse or reach an equilibrium size, is also discussed.\n"
  },
  {
    "id": 171,
    "prompt": "Steady periodic gravity waves with surface tension",
    "HWT": "In this paper we consider two-dimensional, stratified, steady water waves propagating over an impermeable flat bed and with a free surface. The motion is assumed to be driven by capillarity (that is, surface tension) on the surface and a gravitational force acting on the body of the fluid. We prove the existence of global continua of classical solutions that are periodic and traveling. This is accomplished by first constructing a 1-parameter family of laminar flow solutions, $\\mathcal{T}$, then applying bifurcation theory methods to obtain local curves of small amplitude solutions branching from $\\mathcal{T}$ at an eigenvalue of the linearized problem. Each solution curve is then continued globally by means of a degree theoretic theorem in the spirit of Rabinowitz. Finally, we complement the degree theoretic picture by proving an alternate global bifurcation theorem via the analytic continuation method of Dancer.",
    "MGT": "We investigate the existence of steady periodic gravity waves with surface tension in a two-dimensional, irrotational, incompressible fluid of finite depth. The problem is formulated as a nonlocal equation for the free surface elevation using the conformal mapping technique and the Euler equations. By employing a bifurcation approach within a functional analytic framework, we prove the existence of small-amplitude solutions bifurcating from the trivial flat surface. The analysis focuses on the effect of surface tension on the wave profiles. We demonstrate that the presence of surface tension leads to a richer bifurcation structure compared to the case without surface tension. Specifically, we show that near the critical Bond number, where the dispersion relation exhibits a minimum, the bifurcation is no longer simple and can exhibit more complex behavior. Numerical computations are presented to illustrate the theoretical results and to visualize the wave profiles for different values of the Bond number. These computations reveal the emergence of wave profiles with sharp crests and flat troughs as the Bond number approaches the critical value. The results provide insights into the influence of surface tension on the dynamics of gravity waves and contribute to a better understanding of wave phenomena in various physical contexts.\n"
  },
  {
    "id": 172,
    "prompt": "Massive runaway stars in the Large Magellanic Cloud",
    "HWT": "The origin of massive field stars in the Large Magellanic Cloud (LMC) has long been an enigma. The recent measurements of large offsets (~100 km/s) between the heliocentric radial velocities of some very massive (O2-type) field stars and the systemic LMC velocity provides a possible explanation of this enigma and suggests that the field stars are runaway stars ejected from their birth places at the very beginning of their parent cluster's dynamical evolution. A straightforward way to prove this explanation is to measure the proper motions of the field stars and to show that they are moving away from one of the nearby star clusters or OB associations. This approach however is complicated by the large distance to the LMC, which makes accurate proper motion measurements difficult. We use an alternative approach for solving the problem, based on the search for bow shocks produced by runaway stars. The geometry of detected bow shocks would allow us to infer the direction of stellar motion and thereby to determine their possible parent clusters. In this paper we present the results of a search for bow shocks around six massive field stars which were suggested in the literature as candidate runaway stars. Using archival (Spitzer Space Telescope) data, we found a bow shock associated with one of our program stars, the O2 V((f*)) star BI 237, which is the first-ever detection of bow shocks in the LMC. Orientation of the bow shock suggests that BI 237 was ejected from the OB association LH 82 (located at ~120 pc in projection from the star). A by-product of our search is the detection of bow shocks generated by four OB stars in the field of the LMC and an arc-like structure attached to the candidate luminous blue variable R81 (HD 269128). The geometry of two of these bow shocks is consistent with the possibility that their associated stars were ejected from the 30 Doradus star forming complex.",
    "MGT": "Massive stars, exceeding eight times the mass of the Sun, play a crucial role in galactic evolution through their powerful stellar winds, supernova explosions, and contribution to chemical enrichment. Runaway stars, a subset of these massive stars, are characterized by high peculiar velocities relative to their surrounding interstellar medium. Understanding the formation mechanisms and properties of runaway stars is crucial for refining our models of stellar dynamics and binary evolution in galaxies. This study focuses on identifying and characterizing massive runaway stars within the Large Magellanic Cloud (LMC), a nearby dwarf galaxy offering a unique environment with lower metallicity and different star formation history compared to the Milky Way.\n\nWe utilized a combination of astrometric and spectroscopic data from the Gaia satellite and ground-based telescopes to identify candidate runaway stars in the LMC. Gaia's precise proper motion measurements enabled us to identify stars with significant tangential velocities, while spectroscopic observations provided radial velocities and spectral classifications, allowing us to distinguish massive stars from foreground Milky Way stars and background galaxies. We employed a kinematic analysis to determine the runaway status of our candidates, considering their spatial location and velocity vectors relative to the overall LMC rotation. We established a velocity threshold above which stars were classified as runaways, accounting for the LMC's internal velocity dispersion.\n\nOur analysis revealed a significant population of massive runaway stars in the LMC, distributed across various regions of the galaxy. We found a higher concentration of runaways in the vicinity of known star-forming regions, suggesting a possible link between runaway star formation and dynamical interactions within young stellar clusters. Furthermore, we investigated the spectral properties of the identified runaways, finding a range of spectral types and luminosity classes, indicating a diverse sample of massive stars at different evolutionary stages. We explored potential formation scenarios for these runaways, including the dynamical ejection from multiple star systems and the supernova ejection of a companion star in a binary system. We find evidence supporting both scenarios, with some runaways exhibiting characteristics consistent with dynamical ejection and others showing signatures of having been part of a binary system that experienced a supernova event. The observed spatial distribution and velocity characteristics of the runaway stars provide valuable constraints on the relative importance of these different formation channels in the LMC environment. Our findings contribute to a better understanding of the formation and evolution of massive stars in galaxies and highlight the importance of considering runaway stars in studies of stellar populations and galactic dynamics.\n"
  },
  {
    "id": 173,
    "prompt": "Abundance stratification in Type Ia Supernovae - II: The rapidly declining, spectroscopically normal SN 2004eo",
    "HWT": "The variation of properties of Type Ia supernovae, the thermonuclear explosions of Chandrasekhar-mass carbon-oxygen white dwarfs, is caused by different nucleosynthetic outcomes of these explosions, which can be traced from the distribution of abundances in the ejecta. The composition stratification of the spectroscopically normal but rapidly declining SN2004eo is studied performing spectrum synthesis of a time-series of spectra obtained before and after maximum, and of one nebular spectrum obtained about eight months later. Early-time spectra indicate that the outer ejecta are dominated by oxygen and silicon, and contain other intermediate-mass elements (IME), implying that the outer part of the star was subject only to partial burning. In the inner part, nuclear statistical equilibrium (NSE) material dominates, but the production of 56Ni was limited to ~0.43 \\pm 0.05 Msun. An innermost zone containing ~0.25 Msun of stable Fe-group material is also present. The relatively small amount of NSE material synthesised by SN2004eo explains both the dimness and the rapidly evolving light curve of this SN.",
    "MGT": "Type Ia supernovae (SNe Ia) are crucial cosmological distance indicators, yet their progenitors and explosion mechanisms remain debated. Spectroscopic and photometric analyses of SNe Ia have revealed correlations between light curve shape, luminosity, and spectral features, suggesting underlying physical variations. This study presents a detailed analysis of SN 2004eo, a rapidly declining, spectroscopically normal SN Ia, using high-quality optical spectra and light curves. We aim to constrain the explosion parameters and progenitor system of SN 2004eo by comparing its observed properties to theoretical models. Spectral modeling indicates a high degree of ionization and a relatively low velocity gradient compared to typical SNe Ia. Abundance stratification analysis reveals a distinct layering of elements, with heavier elements concentrated in the inner ejecta. The light curve is consistent with a low-mass 56Ni production, suggesting a sub-luminous explosion. Our findings support a scenario where SN 2004eo originated from a near-Chandrasekhar mass explosion of a white dwarf star, possibly involving a double-detonation mechanism. The observed abundance stratification and low velocity gradient may be indicative of a deflagration-to-detonation transition occurring at a relatively shallow depth within the white dwarf. This study contributes to the understanding of the diversity among SNe Ia and highlights the importance of detailed spectroscopic and photometric investigations in unraveling their complex nature.\n"
  },
  {
    "id": 174,
    "prompt": "The Computational Power of Symmetric Hamiltonians",
    "HWT": "The presence of symmetries, be they discrete or continuous, in a physical system typically leads to a reduction in the problem to be solved. Here we report that neither translational invariance nor rotational invariance reduce the computational complexity of simulating Hamiltonian dynamics; the problem is still BQP complete, and is believed to be hard on a classical computer. This is achieved by designing a system to implement a Universal Quantum Interface, a device which enables control of an entire computation through the control of a fixed number of spins, and using it as a building-block to entirely remove the need for control, except in the system initialisation. Finally, it is shown that cooling such Hamiltonians to their ground states in the presence of random magnetic fields solves a QMA-complete problem.",
    "MGT": "Symmetric Hamiltonians, possessing invariance under certain symmetry transformations, are ubiquitous in physics, simplifying calculations and revealing fundamental properties of quantum systems. This work investigates the computational power afforded by such symmetries, exploring their impact on the complexity of simulating quantum dynamics. We demonstrate that specific symmetry constraints, particularly those arising from group representations, can dramatically alter the landscape of quantum computation. We analyze the computational complexity of simulating time evolution under symmetric Hamiltonians, showing that certain symmetry groups enable efficient simulation even when the general problem is QMA-hard. Conversely, we identify classes of symmetries that do not offer significant computational speedups. Our findings highlight a nuanced relationship between symmetry and computational power, revealing that the type and structure of the symmetry group are crucial determinants of simulability. This research provides a framework for understanding how symmetry can be leveraged to design efficient quantum algorithms and sheds light on the inherent computational limitations imposed by physical symmetries.\n"
  },
  {
    "id": 175,
    "prompt": "The Variation of the Galaxy Luminosity Function with Group Properties",
    "HWT": "We explore the shape of the galaxy luminosity function (LF) in groups of different mass by creating composite LFs over large numbers of groups. Following previous work using total group luminosity as the mass indicator, here we split our groups by multiplicity and by estimated virial (group halo) mass, and consider red (passive) and blue (star forming) galaxies separately. In addition we utilise two different group catalogues (2PIGG and Yang et al. ) in order to ascertain the impact of the specific grouping algorithm and further investigate the environmental effects via variations in the LF with position in groups. Our main results are that LFs show a steepening faint end for early type galaxies as a function of group mass/ multiplicity, with a much suppressed trend (evident only in high mass groups) for late type galaxies. Variations between LFs as a function of group mass are robust irrespective of which grouping catalogue is used, and broadly speaking what method for determining group `mass' is used. We find in particular that there is a significant deficit of low-mass passive galaxies in low multiplicity groups, as seen in high redshift clusters. Further to this, the variation in the LF appears to only occur in the central regions of systems, and in fact seems to be most strongly dependent on the position in the group relative to the virial radius. Finally, distance-rank magnitude relations were considered. Only the Yang groups demonstrated any evidence of a correlation between a galaxy's position relative to the brightest group member and its luminosity. 2PIGG possessed no such gradient, the conclusion being the FOF algorithm suppresses the signal for weak luminosity--position trends and the Yang grouping algorithm naturally enhances it.",
    "MGT": "The galaxy luminosity function (LF), describing the number density of galaxies as a function of their luminosity, is a fundamental tool for understanding galaxy formation and evolution. This study investigates the variation of the galaxy LF with the properties of the groups in which galaxies reside, focusing on a large sample of galaxy groups identified in the Sloan Digital Sky Survey (SDSS). We explore how the LF's shape and normalization change as a function of group mass, richness, and velocity dispersion, aiming to disentangle the environmental effects driving galaxy evolution.\n\nOur analysis employs a carefully constructed sample of galaxy groups, ensuring completeness and minimizing contamination. We estimate the LF for galaxies within groups spanning a wide range of halo masses, from dwarf galaxy groups to massive clusters. The LF is parameterized using a Schechter function, allowing us to quantify the characteristic magnitude (M*), faint-end slope (α), and normalization (Φ*) for each group subsample.\n\nWe find a significant correlation between group mass and the LF parameters. Specifically, we observe a trend towards brighter M* values in more massive groups, indicating that the brightest galaxies in these environments are systematically more luminous. The faint-end slope also exhibits a notable dependence on group mass, becoming flatter in more massive systems. This suggests that the suppression of star formation in lower-mass galaxies is more pronounced in denser environments. Furthermore, the normalization of the LF decreases with increasing group mass, consistent with the expectation that a larger fraction of galaxies have been incorporated into the central galaxy in massive halos.\n\nOur results provide valuable insights into the environmental processes shaping galaxy evolution. The observed trends support the idea that processes such as tidal stripping, ram-pressure stripping, and galaxy mergers play a significant role in altering the properties of galaxies in groups. By quantifying the variation of the LF with group properties, we contribute to a more comprehensive understanding of the complex interplay between galaxies and their environment. We also discuss the implications of our findings for galaxy formation models, highlighting the need to accurately reproduce the observed environmental dependencies of the galaxy LF.\n"
  },
  {
    "id": 176,
    "prompt": "Local unitary transformation, long-range quantum entanglement, wave function renormalization, and topological order",
    "HWT": "Two gapped quantum ground states in the same phase are connected by an adiabatic evolution which gives rise to a local unitary transformation that maps between the states. On the other hand, gapped ground states remain within the same phase under local unitary transformations. Therefore, local unitary transformations define an equivalence relation and the equivalence classes are the universality classes that define the different phases for gapped quantum systems. Since local unitary transformations can remove local entanglement, the above equivalence/universality classes correspond to pattern of long range entanglement, which is the essence of topological order. The local unitary transformation also allows us to define a wave function renormalization scheme, under which a wave function can flow to a simpler one within the same equivalence/universality class. Using such a setup, we find conditions on the possible fixed-point wave functions where the local unitary transformations have \\emph{finite} dimensions. The solutions of the conditions allow us to classify this type of topological orders, which generalize the string-net classification of topological orders. We also describe an algorithm of wave function renormalization induced by local unitary transformations. The algorithm allows us to calculate the flow of tensor-product wave functions which are not at the fixed points. This will allow us to calculate topological orders as well as symmetry breaking orders in a generic tensor-product state.",
    "MGT": "Quantum entanglement, particularly long-range entanglement, plays a central role in characterizing topological order in many-body quantum systems. However, directly detecting and quantifying long-range entanglement remains a significant challenge. This work investigates the relationship between local unitary (LU) transformations, wave function renormalization, and the emergence of long-range entanglement indicative of topological order. We demonstrate that specific LU transformations, applied locally to a trivial product state, can generate wave functions possessing non-trivial topological properties, as revealed through entanglement measures. We introduce a novel renormalization procedure, inspired by tensor network methods, that systematically coarse-grains the wave function while preserving its entanglement structure. This procedure allows us to identify fixed-point wave functions characterized by enhanced long-range entanglement and topological invariants. Specifically, we analyze the behavior of entanglement entropy and entanglement negativity under the renormalization flow, showing how these quantities converge to values characteristic of different topological phases. Furthermore, we establish a connection between the form of the LU transformations and the resulting topological order, demonstrating how different choices of LU operations can lead to distinct topological phases. Our results provide a powerful framework for understanding how local quantum operations can generate and manipulate long-range entanglement, offering new insights into the nature of topological order and paving the way for the development of novel quantum technologies based on topologically protected qubits. We illustrate our findings with concrete examples, including the Kitaev chain and the toric code model, demonstrating the effectiveness of our approach in characterizing topologically ordered states.\n"
  },
  {
    "id": 177,
    "prompt": "Two Models for Noisy Feedback in MIMO Channels",
    "HWT": "Two distinct models of feedback, suited for FDD (Frequency Division Duplex) and TDD (Frequency Division Duplex) systems respectively, have been widely studied in the literature. In this paper, we compare these two models of feedback in terms of the diversity multiplexing tradeoff for varying amount of channel state information at the terminals. We find that, when all imperfections are accounted for, the maximum achievable diversity order in FDD systems matches the diversity order in TDD systems. TDD systems achieve better diversity order at higher multiplexing gains. In FDD systems, the maximum diversity order can be achieved with just a single bit of feedback. Additional bits of feedback (perfect or imperfect) do not affect the diversity order if the receiver does not know the channel state information.",
    "MGT": "This paper investigates the impact of noisy feedback on the achievable rates in Multiple-Input Multiple-Output (MIMO) channels. We consider two distinct models for the feedback link: one where the receiver quantizes channel state information (CSI) and transmits it through a noisy channel, and another where the feedback signal is corrupted by additive noise. For the first model, we derive an achievable rate region using a hybrid coding scheme that combines vector quantization and superposition coding. This scheme allows the transmitter to adapt its precoding strategy based on the imperfect CSI received through the noisy feedback link. For the second model, we analyze the impact of additive noise on the feedback link and derive an achievable rate region using a dirty-paper coding approach. We characterize the rate loss due to the noise in the feedback link and identify conditions under which noisy feedback can still provide significant performance gains compared to no feedback. Finally, we compare the performance of the two models through numerical simulations, highlighting the trade-offs between quantization noise and additive noise in the feedback link.\n"
  },
  {
    "id": 178,
    "prompt": "On the Stability of Elliptical Vortices in Accretion Discs",
    "HWT": "(Abriged) The existence of large-scale and long-lived 2D vortices in accretion discs has been debated for more than a decade. They appear spontaneously in several 2D disc simulations and they are known to accelerate planetesimal formation through a dust trapping process. However, the issue of the stability of these structures to the imposition of 3D disturbances is still not fully understood, and it casts doubts on their long term survival. Aim: We present new results on the 3D stability of elliptical vortices embedded in accretion discs, based on a linear analysis and several non-linear simulations. Methods: We derive the linearised equations governing the 3D perturbations in the core of an elliptical vortex, and we show that they can be reduced to a Floquet problem. We solve this problem numerically in the astrophysical regime and we present several analytical limits for which the mechanism responsible for the instability can be explained. Finally, we compare the results of the linear analysis to some high resolution simulations. Results: We show that most anticyclonic vortices are unstable due to a resonance between the turnover time and the local epicyclic oscillation period. In addition, we demonstrate that a strong vertical stratification does not create any additional stable domain of aspect ratio, but it significantly reduces growth rates for relatively weak (and therefore elongated) vortices. Conclusions: Elliptical vortices are always unstable, whatever the horizontal or vertical aspect-ratio is. The instability can however be weak and is often found at small scales, making it difficult to detect in low-order finite-difference simulations.",
    "MGT": "The stability of elliptical vortices embedded within differentially rotating accretion discs is investigated through a combination of analytical and numerical techniques. These vortices, representing coherent structures within the disc, are examined for their susceptibility to various instability mechanisms that could lead to their disruption or dissipation. A primary focus is placed on the elliptical instability, a three-dimensional instability that arises from the periodic deformation of the vortex shape in a rotating frame. We derive analytical criteria, based on Floquet theory, for the onset of this instability, considering the effects of background shear flow and stratification within the disc. These criteria are then validated and extended through three-dimensional hydrodynamic simulations using a high-resolution, finite-volume code.\n\nThe simulations explore a range of vortex parameters, including ellipticity, size relative to the disc scale height, and vertical stratification. Particular attention is paid to the role of compressibility and the presence of density gradients in modifying the instability dynamics. We find that strong background shear can suppress the elliptical instability, particularly for vortices with high ellipticity. However, weaker shear can instead amplify the instability, leading to rapid vortex disruption and the generation of small-scale turbulence. Vertical stratification, modeled through a polytropic equation of state, is found to have a stabilizing effect, particularly for vertically extended vortices. The simulations also reveal the presence of secondary instabilities, such as Kelvin-Helmholtz instabilities along the vortex edges, which can contribute to the overall vortex decay.\n\nFurthermore, the long-term evolution of elliptical vortices is examined, revealing their tendency to undergo merging events with other vortices or to be stretched and sheared by the background flow. The implications of these findings for angular momentum transport and particle trapping within accretion discs are discussed. The results suggest that elliptical vortices, while potentially long-lived under certain conditions, are generally susceptible to a variety of instabilities that limit their lifespan and impact their role in disc dynamics.\n"
  },
  {
    "id": 179,
    "prompt": "Astrophysics with the AMS-02 experiment",
    "HWT": "The Alpha Magnetic Spectrometer (AMS), whose final version AMS-02 is to be installed on the International Space Station (ISS) for at least 3 years, is a detector designed to measure charged cosmic ray spectra with energies up to the TeV region and with high energy photon detection capability up to a few hundred GeV, using state-of-the-art particle identification techniques. Following the successful flight of the detector prototype (AMS-01) aboard the space shuttle, AMS-02 is expected to provide a significant improvement on the current knowledge of the elemental and isotopic composition of hadronic cosmic rays due to its long exposure time (minimum of 3 years) and large acceptance (0.5 m^2 sr) which will enable it to collect a total statistics of more than 10^10 nuclei. Detector capabilities for charge, velocity and mass identification, estimated from ion beam tests and detailed Monte Carlo simulations, are presented. Relevant issues in cosmic ray astrophysics addressed by AMS-02, including the test of cosmic ray propagation models, galactic confinement times and the influence of solar cycles on the local cosmic ray flux, are briefly discussed.",
    "MGT": "The Alpha Magnetic Spectrometer (AMS-02), a high-energy particle detector operating on the International Space Station since 2011, provides a unique platform for studying cosmic rays and searching for new physics. This article reviews key astrophysical results from AMS-02, focusing on precision measurements of cosmic-ray fluxes, including protons, helium, electrons, positrons, and light nuclei like lithium, beryllium, and boron. These measurements, spanning kinetic energies from below 1 GeV to several TeV, have revealed unexpected spectral features, such as hardening in the proton and helium spectra above a few hundred GeV and distinct spectral indices for different species. The positron fraction, the ratio of positron flux to the combined electron and positron flux, exhibits a rise above approximately 10 GeV, indicating a potential contribution from dark matter annihilation or nearby astrophysical sources like pulsars. Furthermore, the antiproton-to-proton ratio provides complementary information on dark matter and propagation models. AMS-02's observations of light nuclei, specifically their flux ratios, offer valuable insights into cosmic-ray propagation mechanisms, including spallation processes in the interstellar medium. The precise data from AMS-02 have significantly constrained theoretical models of cosmic-ray origin, acceleration, and transport, pushing the boundaries of our understanding of high-energy phenomena in the Galaxy and the search for signatures of new physics beyond the Standard Model. These results highlight the crucial role of space-based experiments in advancing astroparticle physics.\n"
  },
  {
    "id": 180,
    "prompt": "Supercurrent and multiple singlet-doublet phase transitions of a quantum dot Josephson junction inside an Aharonov-Bohm ring",
    "HWT": "We study a quantum dot Josephson junction inside an Aharonov-Bohm environment. The geometry is modeled by an Anderson impurity coupled to two directly-linked BCS leads. We illustrate that the well-established picture of the low-energy physics being governed by an interplay of two distinct (singlet and doublet) phases is still valid for this interferometric setup. The phase boundary depends, however, non-monotonically on the coupling strength between the superconductors, causing the system to exhibit re-entrance behavior and multiple phase transitions. We compute the zero-temperature Josephson current and demonstrate that it can become negative in the singlet phase by virtue of the Coulomb interaction U. As a starting point, the limit of large superconducting energy gaps \\Delta=\\infty is solved analytically. In order to tackle arbitrary \\Delta<\\infty and U>0, we employ a truncated functional renormalization group scheme which was previously demonstrated to give quantitatively reliable results for the quantum dot Josephson problem.",
    "MGT": "We investigate the interplay of supercurrent and quantum dot (QD) physics in a Josephson junction (JJ) embedded within an Aharonov-Bohm (AB) ring. The system comprises a spinful QD with an on-site Coulomb interaction coupled to two superconducting leads and threaded by a magnetic flux. Employing a numerical renormalization group (NRG) approach, we explore the ground-state properties and the supercurrent characteristics as a function of QD parameters, superconducting phase difference, and magnetic flux. Our results reveal a rich phase diagram exhibiting multiple singlet-doublet transitions as the QD level position is varied. These transitions are manifested in the supercurrent-phase relation, leading to unconventional current-phase relations, including $\\pi$ junctions and fractional Josephson effects. The AB flux modulates the effective coupling between the QD and the superconducting leads, influencing the parity of the ground state and shifting the critical values of the QD level position at which the singlet-doublet transitions occur. We demonstrate that the magnetic flux can be used to control the supercurrent amplitude and the type of Josephson coupling, offering a pathway for manipulating the superconducting properties of the system. Furthermore, we analyze the local density of states on the QD, providing insights into the formation of Yu-Shiba-Rusinov bound states and their evolution with varying parameters.\n"
  },
  {
    "id": 181,
    "prompt": "Tests of analytical hadronisation models using event shape moments in {\\epem} annihilation",
    "HWT": "Predictions of analytical models for hadronisation, namely the dispersive model, the shape function and the single dressed gluon approximation, are compared with moments of hadronic event shape distributions measured in \\epem annihilation at centre-of-mass energies between 14 and 209 GeV. In contrast to Monte Carlo models for hadronisation, analytical models require to adjust only two universal parameters, the strong coupling and a second quantity parametrising nonperturbative corrections. The extracted values of as are consistent with the world average and competitive with previous measurements. The variance of event shape distributions is compared with predictions given by some of these models. Limitations of the models, probably due to unknown higher order corrections, are demonstrated and discussed.",
    "MGT": "Event shape moments, calculated from distributions of event shape variables, provide valuable insights into the hadronisation process in high-energy collisions. This article presents a comprehensive analysis of event shape moments in \\epem annihilation data collected by the ALEPH detector at LEP. We compare experimental measurements of event shape moments with predictions from several analytical hadronisation models, including the Modified Leading Logarithmic Approximation (MLLA) and its extensions incorporating various assumptions about the non-perturbative regime. Particular attention is paid to the energy evolution of these moments and their sensitivity to different parameters within the models. We investigate the ability of these models to accurately describe the transition from perturbative to non-perturbative QCD. Our results demonstrate that while the MLLA provides a reasonable description at higher energies, deviations are observed at lower energies, highlighting the importance of non-perturbative effects. Further refinements to the models are needed to achieve a more accurate understanding of hadronisation.\n"
  },
  {
    "id": 182,
    "prompt": "Using a priori knowledge to construct copulas",
    "HWT": "Our purpose is to model the dependence between two random variables, taking into account a priori knowledge on these variables. For example, in many applications (oceanography, finance.. . ), there exists an order relation between the two variables; when one takes high values, the other cannot take low values, but the contrary is possible. The dependence for the high values of the two variables is, therefore, not symmetric. However a minimal dependence also exists: low values of one variable are associated with low values of the other variable. The dependence can also be extreme for the maxima or the minima of the two variables. In this paper, we construct step by step asymmetric copulas with asymptotic minimal dependence, and with or without asymptotic maximal dependence, using mixture variables to get at first asymmetric dependence and then minimal dependence. We fit these models to a real dataset of sea states and compare them using Likelihood Ratio Tests when they are nested, and BIC- criterion (Bayesian Information criterion) otherwise.",
    "MGT": "Copulas are powerful tools for modeling dependence between random variables, offering flexibility beyond traditional correlation measures. However, selecting an appropriate copula family can be challenging, particularly in high-dimensional settings or when data is limited. This article introduces a novel framework for constructing copulas by incorporating a priori knowledge about the dependence structure. Our approach leverages expert opinions, theoretical considerations, or previously observed relationships to guide the selection and parameterization of copulas. We demonstrate how this knowledge can be formalized as constraints on the copula's parameters or its functional form. By systematically incorporating these constraints, we can construct copulas that are more aligned with the underlying data generating process and less susceptible to overfitting. We present a general methodology for constructing constrained copulas and illustrate its application through several examples, including modeling financial asset returns and environmental data. Our results show that incorporating a priori knowledge can significantly improve the accuracy and robustness of copula-based models, leading to better predictions and more insightful understanding of dependence relationships. Furthermore, we explore the sensitivity of our approach to the quality and reliability of the a priori knowledge, providing guidance on how to effectively elicit and validate this information. This work contributes to the growing literature on copula construction by providing a practical and theoretically sound method for incorporating domain expertise into the modeling process.\n"
  },
  {
    "id": 183,
    "prompt": "Wave and ray analysis of a type of cloak exhibiting magnified and shifted scattering effect",
    "HWT": "Ray-tracing exercise and full-wave analysis were performed to validate the performance of a new type of cloak composed of isotropic metamaterials. It is shown that objects inside the folded region of this cloak appear invisible to the incoming light from a ray tracing exercise, but exhibit magnified and shifted scattering under a plane wave illumination from a full wave analysis. Gaussian beams are introduced to resolve this interesting paradox resulted from these two methods. We show that at the time-harmonic state, small energy can be diffracted into the folded region and contribute to the resonant state even when the Gaussian beam is steered away from the cloak with an object inside. A scattering pattern identical to that scattered from the image of the object will be formed, which agrees well with the phenomenon in the plane wave incidence case.",
    "MGT": "This study investigates a unique cloaking mechanism that, instead of suppressing scattering, magnifies and shifts it to a different spatial location. We analyze the behavior of electromagnetic waves interacting with a specifically designed metamaterial structure using both full-wave simulations and ray tracing techniques. The cloak is designed to operate at microwave frequencies and consists of concentric layers with spatially varying permittivity and permeability. Our full-wave simulations demonstrate that the cloak effectively redirects incident waves, causing them to scatter from a region significantly larger than the physical dimensions of the cloak itself, and displaced from the original object's location. We further explore the underlying principles by developing a ray-tracing model based on the effective medium properties of the metamaterial. The ray-tracing results corroborate the full-wave simulations, providing a clear visualization of the wave trajectories and highlighting the mechanism responsible for the magnified and shifted scattering effect. This type of cloak could have potential applications in deception and camouflage technologies, as well as in the development of novel scattering-based imaging techniques.\n"
  },
  {
    "id": 184,
    "prompt": "Position Dependent Mass Schroedinger Equation and Isospectral Potentials : Intertwining Operator approach",
    "HWT": "Here we have studied first and second-order intertwining approach to generate isospectral partner potentials of position-dependent (effective) mass Schroedinger equation. The second-order intertwiner is constructed directly by taking it as second order linear differential operator with position depndent coefficients and the system of equations arising from the intertwining relationship is solved for the coefficients by taking an ansatz. A complete scheme for obtaining general solution is obtained which is valid for any arbitrary potential and mass function. The proposed technique allows us to generate isospectral potentials with the following spectral modifications: (i) to add new bound state(s), (ii) to remove bound state(s) and (iii) to leave the spectrum unaffected. To explain our findings with the help of an illustration, we have used point canonical transformation (PCT) to obtain the general solution of the position dependent mass Schrodinger equation corresponding to a potential and mass function. It is shown that our results are consistent with the formulation of type A N-fold supersymmetry [14,18] for the particular case N = 1 and N = 2 respectively.",
    "MGT": "This paper explores the construction of isospectral potentials for the position-dependent mass Schrödinger equation using the intertwining operator approach. We extend the standard supersymmetric quantum mechanics formalism to scenarios where the mass is not constant but varies spatially. Specifically, we derive the generalized intertwining relations and demonstrate how they can be employed to generate new potentials that share the same energy spectrum as the original potential, except possibly for the ground state. We present a detailed analysis of the conditions under which the intertwining relations are well-defined and lead to physically acceptable solutions. Furthermore, we provide several illustrative examples involving different mass functions and potentials, showcasing the versatility of the method. These examples include cases where the mass function is chosen to be exponentially decaying or follows a power-law dependence on position. The resulting isospectral potentials exhibit interesting features and can be used to model a variety of physical systems where the effective mass of a particle is not constant. Our findings contribute to a deeper understanding of the relationship between position-dependent mass Schrödinger equations and supersymmetric quantum mechanics, offering a powerful tool for generating new solvable models in quantum mechanics.\n"
  },
  {
    "id": 185,
    "prompt": "PQCD Formulations with Heavy Quark Masses and Global Analysis",
    "HWT": "We critically review heavy quark mass effects in DIS and their impact on global analyses. We lay out all elements of a properly defined general mass variable flavor number scheme (GM VFNS) that are shared by all modern formulations of the problem. We then explain the freedom in choosing specific implementations and spell out, in particular, the current formulations of the CTEQ and MSTW groups. We clarify the approximations in the still widely-used zero mass variable flavor scheme (ZM VFNS), mention the inherent flaws in its conventional implementation, and consider the possibility of mending some of these flaws. We discuss practical issues concerning the use of parton distributions in various physical applications, in view of the different schemes. And we comment on the possible presence of intrinsic heavy flavors.",
    "MGT": "This study investigates perturbative Quantum Chromodynamics (pQCD) formulations incorporating heavy quark masses, focusing on bottom and charm quarks, and their impact on global analyses of parton distribution functions (PDFs).  We explore various mass schemes, including fixed-flavor number schemes (FFNS) and variable-flavor number schemes (VFNS), examining their theoretical consistency and phenomenological implications.  A detailed comparison of different VFNS implementations is presented, highlighting discrepancies in the treatment of heavy quark thresholds and decoupling effects. We perform a global QCD analysis, fitting PDFs to a wide range of experimental data from deep-inelastic scattering, Drell-Yan processes, and heavy-quark production at hadron colliders. The analysis incorporates data from HERA, Tevatron, and the LHC. The impact of heavy quark mass variations on the extracted PDFs, particularly the gluon and strange quark distributions, is assessed. We quantify the uncertainties associated with heavy quark mass choices and their propagation to predictions for key Standard Model processes. Our results provide valuable insights into the interplay between heavy quark masses and PDFs, contributing to more precise predictions for high-energy physics experiments.\n"
  },
  {
    "id": 186,
    "prompt": "Minimum feature size preserving decompositions",
    "HWT": "The minimum feature size of a crossing-free straight line drawing is the minimum distance between a vertex and a non-incident edge. This quantity measures the resolution needed to display a figure or the tool size needed to mill the figure. The spread is the ratio of the diameter to the minimum feature size. While many algorithms (particularly in meshing) depend on the spread of the input, none explicitly consider finding a mesh whose spread is similar to the input. When a polygon is partitioned into smaller regions, such as triangles or quadrangles, the degradation is the ratio of original to final spread (the final spread is always greater). Here we present an algorithm to quadrangulate a simple n-gon, while achieving constant degradation. Note that although all faces have a quadrangular shape, the number of edges bounding each face may be larger. This method uses Theta(n) Steiner points and produces Theta(n) quadrangles. In fact to obtain constant degradation, Omega(n) Steiner points are required by any algorithm. We also show that, for some polygons, a constant factor cannot be achieved by any triangulation, even with an unbounded number of Steiner points. The specific lower bounds depend on whether Steiner vertices are used or not.",
    "MGT": "Decomposing complex polygons into simpler shapes is a fundamental problem in various fields, including VLSI design, CAD/CAM, and computer graphics. A critical challenge arises when the decomposed shapes must adhere to specific size constraints, particularly a minimum feature size (MFS). Violating the MFS can lead to manufacturing defects, performance degradation, or aesthetic issues. This paper introduces novel algorithms for decomposing polygons while strictly preserving a user-defined MFS. Our approach builds upon constrained Delaunay triangulation, incorporating iterative refinement steps to ensure that all resulting triangles meet the MFS requirement. We present two distinct decomposition strategies: a greedy approach that focuses on maximizing the area covered by MFS-compliant triangles in each iteration, and a divide-and-conquer method that recursively partitions the polygon until all sub-polygons satisfy the MFS constraint.\n\nWe rigorously analyze the theoretical properties of our algorithms, providing bounds on the number of triangles generated and demonstrating their convergence. Furthermore, we conduct extensive experimental evaluations on a diverse set of polygon datasets, comparing the performance of our algorithms with existing decomposition techniques that do not explicitly consider MFS preservation. The results demonstrate that our algorithms achieve significant improvements in MFS compliance, with minimal increase in the number of resulting shapes. We also investigate the trade-off between decomposition complexity and MFS adherence, providing insights into the optimal parameter settings for different application scenarios. Finally, we present a case study illustrating the application of our algorithms in the layout of microfluidic devices, where MFS preservation is crucial for ensuring proper device functionality. Our findings contribute to the development of robust and reliable polygon decomposition methods for applications requiring strict size constraints.\n"
  },
  {
    "id": 187,
    "prompt": "Mapping the Geography of Science: Distribution Patterns and Networks of Relations among Cities and Institutes",
    "HWT": "Using Google Earth, Google Maps and/or network visualization programs such as Pajek, one can overlay the network of relations among addresses in scientific publications on the geographic map. We discuss the pros en cons of the various options, and provide software (freeware) for bridging existing gaps between the Science Citation Indices and Scopus, on the one side, and these various visualization tools, on the other. At the level of city names, the global map can be drawn reliably on the basis of the available address information. At the level of the names of organizations and institutes, there are problems of unification both in the ISI-databases and Scopus. Pajek enables us to combine the visualization with statistical analysis, whereas the Google Maps and its derivates provide superior tools at the Internet.",
    "MGT": "This study investigates the geographical distribution of scientific knowledge production, focusing on the interplay between cities and research institutions. Employing bibliometric data from Scopus and Web of Science, we analyze publication patterns across various scientific disciplines, mapping the concentration of research output in specific urban centers and identifying the key institutions driving this production. The research utilizes network analysis techniques to reveal collaborative relationships between cities and institutions, examining the flow of knowledge and the formation of scientific communities. Our findings demonstrate a significant unevenness in the geographical landscape of science, with a concentration of high-impact research in a limited number of global cities. Furthermore, the analysis highlights the crucial role of inter-institutional collaborations in fostering scientific advancement, revealing complex networks that extend beyond national borders. The study contributes to a better understanding of the spatial dynamics of science, providing insights for policymakers and research managers interested in promoting scientific excellence and regional development. We discuss implications for funding strategies and urban planning.\n"
  },
  {
    "id": 188,
    "prompt": "The ALICE TPC, a large 3-dimensional tracking device with fast readout for ultra-high multiplicity events",
    "HWT": "The design, construction, and commissioning of the ALICE Time-Projection Chamber (TPC) is described. It is the main device for pattern recognition, tracking, and identification of charged particles in the ALICE experiment at the CERN LHC. The TPC is cylindrical in shape with a volume close to 90 m^3 and is operated in a 0.5 T solenoidal magnetic field parallel to its axis. In this paper we describe in detail the design considerations for this detector for operation in the extreme multiplicity environment of central Pb--Pb collisions at LHC energy. The implementation of the resulting requirements into hardware (field cage, read-out chambers, electronics), infrastructure (gas and cooling system, laser-calibration system), and software led to many technical innovations which are described along with a presentation of all the major components of the detector, as currently realized. We also report on the performance achieved after completion of the first round of stand-alone calibration runs and demonstrate results close to those specified in the TPC Technical Design Report.",
    "MGT": "The ALICE experiment at the CERN Large Hadron Collider is designed to study the Quark-Gluon Plasma (QGP) created in ultra-relativistic heavy-ion collisions. The Time Projection Chamber (TPC) is the main tracking detector of ALICE, providing charged particle tracking, momentum measurement, and particle identification via dE/dx. It is designed to operate in a high multiplicity environment, up to dNch/dy ≈ 8000 in central Pb-Pb collisions. The TPC is a large cylindrical detector with a volume of 90 m3, filled with a gas mixture. A central cathode divides the chamber into two drift regions. Ionization electrons produced by charged particles traversing the gas drift towards the endplates where they are detected by Multi-Wire Proportional Chambers (MWPCs).\n\nTo cope with the high track density and interaction rate, the TPC is equipped with a fast readout system. This system is based on custom-designed front-end electronics and a multi-level data acquisition system. The TPC performance has been extensively studied using data from pp, p-Pb, and Pb-Pb collisions. The results show that the TPC meets the design requirements in terms of tracking efficiency, momentum resolution, and particle identification capability. The detector's performance and its crucial role in the ALICE physics program are presented, emphasizing its capability to handle the extreme conditions of heavy-ion collisions and contribute to our understanding of the QGP.\n"
  },
  {
    "id": 189,
    "prompt": "The UV-optical colours of brightest cluster galaxies in optically and X-ray selected clusters",
    "HWT": "Many brightest cluster galaxies (BCGs) at the centers of X-ray selected clusters exhibit clear evidence for recent star formation. However, studies of BCGs in optically-selected clusters show that star formation is not enhanced when compared to control samples of non-BCGs of similar stellar mass. Here we analyze a sample of 113 BCGs in low redshift (z<0.1), optically-selected clusters, a matched control sample of non-BCGs, and a smaller sample of BCGs in X-ray selected clusters. We convolve the SDSS images of the BCGs to match the resolution of the GALEX data and we measure UV-optical colours in their inner and outer regions. We find that optically-selected BCGs exhibit smaller scatter in optical colours and redder inner NUV-r colours than the control galaxies, indicating that they are a homogenous population with very little ongoing star formation. The BCGs in the X-ray selected cluster sample span a similar range in optical colours, but have bluer NUV-r colours. Among X-ray selected BCGs, those located in clusters with central cooling times of less than 1 Gyr are significantly bluer than those located in clusters where the central gas cooling times are long. Our main conclusion is that the location of a galaxy at the centre of its halo is not sufficient to determine whether or not it is currently forming stars. One must also have information about the thermodynamic state of the gas in the core of the halo.",
    "MGT": "Brightest cluster galaxies (BCGs) reside at the centres of galaxy clusters, marking the deepest potential well and representing the culmination of hierarchical structure formation. Their stellar populations encode valuable information about cluster assembly history and feedback processes. We present a comprehensive analysis of the UV-optical colours of BCGs, exploring their dependence on cluster selection method (optical richness vs. X-ray luminosity) and cluster properties. Using a large sample of BCGs drawn from the Sloan Digital Sky Survey (SDSS) matched with both optically selected (redMaPPer) and X-ray selected (ROSAT) galaxy clusters, we investigate the NUV-r and u-r colour distributions as a function of redshift, cluster richness, and X-ray luminosity. We find systematic differences in the UV-optical colours of BCGs in optically and X-ray selected clusters, particularly at lower redshifts. BCGs in optically selected clusters tend to exhibit redder NUV-r colours compared to those in X-ray selected clusters, suggesting a higher incidence of recent star formation in the latter. Furthermore, we observe a weak correlation between BCG UV-optical colours and cluster richness for optically selected clusters, while a stronger correlation exists between BCG colours and X-ray luminosity for X-ray selected clusters. These findings suggest that different physical mechanisms might be driving the evolution of BCGs in clusters identified through different methods. We discuss the implications of these results for our understanding of BCG formation pathways, the role of active galactic nuclei (AGN) feedback in quenching star formation, and the interplay between the intracluster medium (ICM) and the central galaxy. This study highlights the importance of considering cluster selection biases when studying BCG properties and their connection to cluster evolution.\n"
  },
  {
    "id": 190,
    "prompt": "Shells, jets, and internal working surfaces in the molecular outflow from IRAS 04166+2706",
    "HWT": "Context: IRAS 04166+2706 in Taurus is one of the most nearby young stellar objects whose molecular outflow contains a highly collimated fast component. Methods: We have observed the IRAS 04166+2706 outflow with the IRAM Plateau de Bure interferometer in CO(J=2-1) and SiO(J=2-1) achieving angular resolutions between 2'' and 4''. To improve the quality of the CO(2-1) images, we have added single dish data to the interferometer visibilities. Results: The outflow consists of two distinct components. At velocities <10 km/s, the gas forms two opposed, approximately conical shells that have the YSO at their vertex. These shells coincide with the walls of evacuated cavities and seem to result from the acceleration of the ambient gas by a wide-angle wind. At velocities >30 km/s, the gas forms two opposed jets that travel along the center of the cavities and whose emission is dominated by a symmetric collection of at least 7 pairs of peaks. The velocity field of this component presents a sawtooth pattern with the gas in the tail of each peak moving faster than the gas in the head. This pattern, together with a systematic widening of the peaks with distance to the central source, is consistent with the emission arising from internal working surfaces traveling along the jet and resulting from variations in the velocity field of ejection. We interpret this component as the true protostellar wind, and we find its composition consistent with a chemical model of such type of wind. Conclusions: Our results support outflow wind models that have simultaneously wide-angle and narrow components, and suggest that the EHV peaks seen in a number of outflows consist of internally-shocked wind material.",
    "MGT": "We present high-resolution CO (J=2-1) and SiO (J=5-4) observations of the molecular outflow associated with the Class I protostar IRAS 04166+2706, obtained with the Submillimeter Array. These observations reveal a complex outflow morphology characterized by multiple shell-like structures, high-velocity jet-like features, and distinct internal working surfaces (IWS). The CO emission traces the overall outflow structure, revealing a series of nested, expanding shells that appear to be driven by episodic ejection events. The outermost shell exhibits a relatively low velocity, while the inner shells show progressively higher velocities, suggesting an acceleration of the outflow with time. The SiO emission, a known shock tracer, is concentrated in the jet-like features and the IWS, indicating the presence of strong shocks within the outflow.\n\nThe jet-like features are highly collimated and extend over several arcseconds, exhibiting a clear velocity gradient along their axes. These jets appear to be the primary drivers of the observed shell structures, with the IWS marking the locations where the jets interact with the surrounding ambient material. We identify several distinct IWS, characterized by enhanced SiO emission and abrupt changes in velocity. These IWS likely represent locations of enhanced density and temperature, where the kinetic energy of the jet is being dissipated through shocks.\n\nA detailed analysis of the kinematics of the outflow reveals a complex velocity structure, with evidence for both expansion and rotation. The expanding shells exhibit a Hubble-like velocity profile, with velocity increasing linearly with distance from the protostar. The rotation is evident in the velocity gradients observed across the outflow lobes, suggesting that the outflow is launched from a rotating disk. Furthermore, we estimate the mass, momentum, and energy of the outflow components, finding that the jet-like features contribute significantly to the overall outflow momentum and energy budget. These findings support a scenario in which the outflow from IRAS 04166+2706 is driven by a combination of episodic ejection events and continuous jet activity, with the IWS playing a crucial role in the transfer of energy and momentum to the surrounding molecular cloud. The complex morphology and kinematics of this outflow provide valuable insights into the dynamics of protostellar outflows and their impact on the surrounding environment.\n"
  },
  {
    "id": 191,
    "prompt": "Parameter Degeneracy in Flavor-Dependent Reconstruction of Supernova Neutrino Fluxes",
    "HWT": "We reexamine the possibility of reconstructing the initial fluxes of supernova neutrinos emitted in a future core-collapse galactic supernova explosion and detected in a Megaton-sized water Cherenkov detector. A novel key element in our method is the inclusion, in addition to the total and the average energies of each neutrino species, of a \"pinching\" parameter characterizing the width of the distribution as a fit parameter. We uncover in this case a continuous degeneracy in the reconstructed parameters of supernova neutrino fluxes at the neutrinosphere. We analyze in detail the features of this degeneracy and show how it occurs irrespective of the parametrization used for the distribution function. Given that this degeneracy is real we briefly comment on possible steps towards resolving it, which necessarily requires going beyond the setting presented here.",
    "MGT": "Core-collapse supernovae are copious sources of neutrinos, carrying away 99% of the gravitational binding energy. Precise reconstruction of the emitted neutrino fluxes provides crucial insights into the explosion mechanism and neutrino properties. However, flavor-dependent neutrino interactions in the supernova environment introduce significant complexities. This work investigates the impact of parameter degeneracy on the reconstruction of supernova neutrino fluxes, particularly focusing on the effects of neutrino oscillation parameters and the spectral shape parameters of the emitted fluxes. We demonstrate that uncertainties in these parameters can lead to significant ambiguities in determining the true neutrino fluxes. A detailed analysis of the correlation between different parameters is performed, highlighting the dominant degeneracies. We explore how different detection channels at terrestrial neutrino detectors can help to lift these degeneracies and improve the accuracy of flux reconstruction. Furthermore, we discuss the implications of these findings for future supernova neutrino observations and the potential for using such observations to constrain fundamental neutrino parameters.\n"
  },
  {
    "id": 192,
    "prompt": "One-dimensional quantum cellular automata over finite, unbounded configurations",
    "HWT": "One-dimensional quantum cellular automata (QCA) consist in a line of identical, finite dimensional quantum systems. These evolve in discrete time steps according to a local, shift-invariant unitary evolution. By local we mean that no instantaneous long-range communication can occur. In order to define these over a Hilbert space we must restrict to a base of finite, yet unbounded configurations. We show that QCA always admit a two-layered block representation, and hence the inverse QCA is again a QCA. This is a striking result since the property does not hold for classical one-dimensional cellular automata as defined over such finite configurations. As an example we discuss a bijective cellular automata which becomes non-local as a QCA, in a rare case of reversible computation which does not admit a straightforward quantization. We argue that a whole class of bijective cellular automata should no longer be considered to be reversible in a physical sense. Note that the same two-layered block representation result applies also over infinite configurations, as was previously shown for one-dimensional systems in the more elaborate formalism of operators algebras [9]. Here the proof is made simpler and self-contained, moreover we discuss a counterexample QCA in higher dimensions.",
    "MGT": "One-dimensional quantum cellular automata (QCA) are quantum analogs of classical cellular automata, offering a framework for studying quantum computation and complex systems. This paper explores the behavior of one-dimensional QCA acting on finite configurations embedded within an unbounded background. While previous research has largely focused on QCA acting on infinite, periodic, or finite configurations with periodic boundary conditions, our work addresses the more realistic scenario of finite, non-periodic configurations evolving within an infinite, quiescent environment.\n\nWe present a rigorous analysis of the conditions under which QCA preserve the finiteness of configurations. We establish necessary and sufficient criteria, based on the structure of the QCA's local transition rule, for ensuring that an initial finite configuration evolves into another finite configuration at each time step. These criteria involve constraints on the propagation of quantum information beyond the initial support of the configuration.\n\nFurthermore, we investigate the long-term dynamics of QCA acting on finite configurations. We demonstrate that, under certain conditions, the evolution of the QCA can lead to the stabilization of the configuration, where the system eventually reaches a fixed point or a periodic orbit. We also explore scenarios where the QCA exhibits more complex, non-stabilizing behavior, potentially leading to the unbounded spreading of quantum information.\n\nOur results provide valuable insights into the computational power and dynamical properties of QCA acting on finite, unbounded configurations. These findings have implications for the design of quantum algorithms and the modeling of quantum systems in realistic physical settings.\n"
  },
  {
    "id": 193,
    "prompt": "Detection of a Thermal Spectral Component in the Prompt Emission of GRB 100724B",
    "HWT": "Observations of GRB 100724B with the Fermi Gamma-Ray Burst Monitor (GBM) find that the spectrum is dominated by the typical Band functional form, which is usually taken to represent a non-thermal emission component, but also includes a statistically highly significant thermal spectral contribution. The simultaneous observation of the thermal and non-thermal components allows us to confidently identify the two emission components. The fact that these seem to vary independently favors the idea that the thermal component is of photospheric origin while the dominant non-thermal emission occurs at larger radii. Our results imply either a very high efficiency for the non-thermal process, or a very small size of the region at the base of the flow, both quite challenging for the standard fireball model. These problems are resolved if the jet is initially highly magnetized and has a substantial Poynting flux.",
    "MGT": "Gamma-ray bursts (GRBs) are the most luminous explosions in the universe. While their prompt emission is typically described by non-thermal synchrotron radiation, evidence for a thermal component remains elusive. We present a detailed spectral analysis of the bright GRB 100724B observed by Fermi-GBM, focusing on the presence of a possible thermal component. Our analysis reveals a significant improvement in the spectral fit when including a blackbody component in addition to the standard Band function. The blackbody temperature is found to be approximately 15 keV, and its bolometric flux contributes to a substantial fraction of the total prompt emission flux. We explore the possible origins of this thermal component, considering scenarios such as photospheric emission from the jet or thermal emission from a cocoon surrounding the central engine. Our findings suggest that thermal emission may be more common in GRBs than previously thought, potentially providing valuable insights into the physical conditions and emission mechanisms operating during these extreme events. The detection of a thermal component in GRB 100724B offers a valuable opportunity to constrain theoretical models of GRB prompt emission and provides further evidence for the complex nature of these powerful cosmic explosions.\n"
  },
  {
    "id": 194,
    "prompt": "Emittance Measurement in MICE",
    "HWT": "Muon ionization cooling provides the only practical solution to prepare high-brilliance beams necessary for a neutrino factory or muon collider. The Muon Ionization Cooling Experiment (MICE), under development at the Rutherford Appleton Laboratory, is installing the first set of particle detectors ever built to measure to 0.1% the emittance of a 200 MeV/c or so muon beam in and out of a cooling cell, and thus measure the cooling cell's performance. Two identical \"emittometers\" (a precise scintillating-fiber tracker in solenoidal magnetic field and a 50 ps time-of-flight station) measure the six phase-space coordinates of each muon. Another TOF plane and two Cherenkov counters assure the purity of the incoming muon beam. A downstream electron/muon calorimeter eliminates contamination from decay electrons.",
    "MGT": "The Muon Ionization Cooling Experiment (MICE) aims to demonstrate ionization cooling, a technique crucial for future muon colliders. Precise emittance measurement is paramount to quantify the cooling performance. This paper details the emittance measurement techniques employed in MICE, focusing on the tracker-based reconstruction method. The MICE tracker, composed of scintillating fiber detectors, provides high-resolution track reconstruction of individual muons traversing the cooling channel. Emittance is then calculated from the reconstructed phase-space coordinates of these muons. We discuss the calibration procedures necessary to achieve accurate track reconstruction and minimize systematic uncertainties in the emittance determination. Furthermore, we present results from experimental data, illustrating the emittance measurement capabilities of MICE. These measurements are compared with simulations to validate the understanding of the detector performance and the muon beam dynamics within the cooling channel. The challenges and limitations associated with emittance measurement in a high-background environment are also addressed, along with potential strategies for mitigation.\n"
  },
  {
    "id": 195,
    "prompt": "U(1)-Symmetry breaking and violation of axial symmetry in TlCuCl3 and other insulating spin systems",
    "HWT": "We describe the Bose-Einstein condensate of magnetic bosonic quasiparticles in insulating spin systems using a phenomenological standard functional method for T = 0. We show that results that are already known from advanced computational techniques immediately follow. The inclusion of a perturbative anisotropy term that violates the axial symmetry allows us to remarkably well explain a number of experimental features of the dimerized spin-1/2 system TlCuCl3. Based on an energetic argument we predict a general intrinsic instability of an axially symmetric magnetic condensate towards a violation of this symmetry, which leads to the spontaneous formation of an anisotropy gap in the energy spectrum above the critical field. We, therefore, expect that a true Goldstone mode in insulating spin systems, i.e., a strictly linear energy-dispersion relation down to arbitrarily small excitations energies, cannot be observed in any real material.",
    "MGT": "The insulating dimerized spin system TlCuCl3 exhibits a field-induced quantum phase transition from a gapped spin-liquid ground state to a Bose-Einstein condensate of triplons. This transition is characterized by the breaking of a global U(1) symmetry associated with the conservation of the z-component of the total spin. We investigate the effects of perturbations that explicitly break this U(1) symmetry, focusing on the emergence of previously forbidden magnetic Bragg peaks. Through a combination of theoretical analysis using bond operator theory and experimental neutron scattering data, we demonstrate that these peaks are directly related to the violation of axial symmetry in the spin Hamiltonian. Furthermore, we explore the implications of these findings for understanding similar phenomena in other insulating spin systems, including those with anisotropic exchange interactions. Our results highlight the sensitivity of the triplon condensate to symmetry-breaking terms and provide valuable insights into the interplay between symmetry, quantum fluctuations, and magnetic order in these materials.\n"
  },
  {
    "id": 196,
    "prompt": "Identification of the Lithium Depletion Boundary and Age of the Southern Open Cluster Blanco 1",
    "HWT": "We present results from a spectroscopic study of the very low mass members of the Southern open cluster Blanco 1 using the Gemini-N telescope. We obtained intermediate resolution (R~4400) GMOS spectra for 15 cluster candidate members with I~14-20 mag, and employed a series of membership criteria - proximity to the cluster's sequence in an I/I-Ks color-magnitude diagram (CMD), kinematics agreeing with the cluster systemic motion, magnetic activity as a youth indicator - to classify 10 of these objects as probable cluster members. For these objects, we searched for the presence of the Li I 6708 A feature to identify the lithium depletion boundary (LDB) in Blanco 1. The I/I-Ks CMD shows a clear mass segregation in the Li distribution along the cluster sequence; namely, all higher mass stars are found to be Li-poor, while lower mass stars are found to be Li-rich. The division between Li-poor and Li-rich (i.e., the LDB) in Blanco 1 is found at I=$18.78 \\pm 0.24$ and I-Ks=$3.05 \\pm 0.10$. Using current pre-main-sequence evolutionary models we determine an LDB age of $132 \\pm 24$ Myr. Comparing our derived LDB age to upper-main-sequence isochrone ages for Blanco 1, as well as for other open clusters with identified LDBs, we find good chronometric consistency when using stellar evolution models that incorporate a moderate degree of convective core overshoot.",
    "MGT": "This study presents a determination of the lithium depletion boundary (LDB) for the southern open cluster Blanco 1, utilizing deep optical photometry from the Dark Energy Camera (DECam) and follow-up spectroscopic observations. Blanco 1, with an estimated age between 80 and 120 Myr, provides a valuable benchmark for stellar evolution models in the intermediate-age regime. Our photometric analysis identifies cluster members based on their position in color-magnitude diagrams, proper motions, and parallax measurements from Gaia DR3. We carefully account for field star contamination and differential reddening to obtain a clean sample of low-mass stars.\n\nSpectroscopic observations were conducted using the Goodman Spectrograph on the Southern Astrophysical Research (SOAR) telescope to measure equivalent widths of the Li I λ6708 Å absorption line. We targeted a sample of faint, low-mass candidate members spanning a range of effective temperatures near the expected LDB. These observations enabled us to distinguish between lithium-rich and lithium-poor stars, effectively bracketing the LDB.\n\nBy plotting lithium abundance as a function of effective temperature, we identify the LDB at an effective temperature of approximately 3750 ± 50 K, corresponding to a stellar mass of roughly 0.18 M⊙. Comparing our observed LDB with predictions from various stellar evolution models allows us to constrain the age of Blanco 1. We find that models incorporating updated opacities and non-standard mixing processes, such as rotationally induced mixing, provide the best agreement with our observed LDB. Our derived age estimate for Blanco 1 is 105 ± 15 Myr, consistent with previous isochrone fitting results but with improved precision. This refined age determination contributes to a more accurate calibration of age-dating techniques for young stellar populations and provides crucial observational constraints for refining stellar evolution models. Furthermore, the high quality of our data and analysis underscores the utility of combining wide-field photometric surveys with targeted spectroscopic follow-up for studies of open clusters.\n"
  },
  {
    "id": 197,
    "prompt": "Solar Gamma Rays Powered by Secluded Dark Matter",
    "HWT": "Secluded dark matter models, in which WIMPs annihilate first into metastable mediators, can present novel indirect detection signatures in the form of gamma rays and fluxes of charged particles arriving from directions correlated with the centers of large astrophysical bodies within the solar system, such as the Sun and larger planets. This naturally occurs if the mean free path of the mediator is in excess of the solar (or planetary) radius. We show that existing constraints from water Cerenkov detectors already provide a novel probe of the parameter space of these models, complementary to other sources, with significant scope for future improvement from high angular resolution gamma-ray telescopes such as Fermi-LAT. Fluxes of charged particles produced in mediator decays are also capable of contributing a significant solar system component to the spectrum of energetic electrons and positrons, a possibility which can be tested with the directional and timing information of PAMELA and Fermi.",
    "MGT": "The Sun, a seemingly well-understood astrophysical object, may harbor secrets related to dark matter. This work proposes a novel mechanism for producing observable gamma rays from the Sun, powered by the annihilation of secluded dark matter particles. We hypothesize that dark matter, interacting weakly with the Standard Model, becomes gravitationally trapped within the Sun's core. These particles, belonging to a secluded sector, then annihilate into lighter mediators. These mediators, experiencing suppressed interactions with baryonic matter, escape the dense solar core and subsequently decay into Standard Model particles, including photons. We calculate the expected gamma-ray flux resulting from this process, considering various dark matter masses, interaction strengths, and mediator lifetimes. Our results indicate that for specific parameter ranges, the predicted gamma-ray flux could be detectable by current and future gamma-ray telescopes, such as Fermi-LAT and future Cherenkov Telescope Array (CTA). This offers a unique probe of dark matter properties, complementary to direct and indirect detection experiments. The detection of such a signal would not only provide evidence for dark matter but also shed light on the nature of the secluded sector and its interaction with the Standard Model.\n"
  },
  {
    "id": 198,
    "prompt": "P values, confidence intervals, or confidence levels for hypotheses?",
    "HWT": "Null hypothesis significance tests and p values are widely used despite very strong arguments against their use in many contexts. Confidence intervals are often recommended as an alternative, but these do not achieve the objective of assessing the credibility of a hypothesis, and the distinction between confidence and probability is an unnecessary confusion. This paper proposes a more straightforward (probabilistic) definition of confidence, and suggests how the idea can be applied to whatever hypotheses are of interest to researchers. The relative merits of the different approaches are discussed using a series of illustrative examples: usually confidence based approaches seem more transparent and useful, but there are some contexts in which p values may be appropriate. I also suggest some methods for converting results from one format to another. (The attractiveness of the idea of confidence is demonstrated by the widespread persistence of the completely incorrect idea that p=5% is equivalent to 95% confidence in the alternative hypothesis. In this paper I show how p values can be used to derive meaningful confidence statements, and the assumptions underlying the derivation. ) Key words: Confidence interval, Confidence level, Hypothesis testing, Null hypothesis significance tests, P value, User friendliness.",
    "MGT": "The ongoing debate regarding the appropriate statistical reporting practices in scientific research necessitates a re-evaluation of the roles and interpretations of p-values, confidence intervals, and confidence levels. This article delves into the theoretical underpinnings and practical implications of these three commonly employed statistical tools, arguing that a balanced approach considering the strengths and limitations of each is crucial for fostering robust and transparent scientific inference. While p-values offer a measure of the evidence against a null hypothesis, their susceptibility to misinterpretation and overemphasis on statistical significance can lead to flawed conclusions. Confidence intervals, on the other hand, provide a range of plausible values for a population parameter, offering a more informative perspective on the magnitude and precision of an effect. However, the interpretation of confidence intervals is not without its challenges, particularly in complex models or when dealing with multiple comparisons. Furthermore, the arbitrary nature of confidence levels, typically set at 95%, raises questions about the potential for bias and the need for more nuanced approaches. This article advocates for a shift towards a more holistic approach to statistical reporting, emphasizing the importance of effect sizes, contextual understanding, and the careful consideration of both statistical and practical significance. By integrating p-values, confidence intervals, and confidence levels judiciously, researchers can enhance the rigor and reproducibility of their findings, ultimately contributing to a more reliable and informative scientific landscape.\n"
  },
  {
    "id": 199,
    "prompt": "A Non-Cooperative Method for Path Loss Estimation in Femtocell Networks",
    "HWT": "A macrocell superposed by indoor deployed femtocells forms a geography-overlapped and spectrum-shared two tier network, which can efficiently improve coverage and enhance system capacity. It is important for reducing inter-tier co-channel interference that any femtocell user (FU) can select suitable access channel according to the path losses between itself and the macrocell users (MUs). Path loss should be estimated non-cooperatively since information exchange is difficult between macrocell and femtocells. In this paper, a novel method is proposed for FU to estimate the path loss between itself and any MU independently. According to the adaptive modulation and coding (AMC) mode information broadcasted by the macrocell base station (BS), FU first estimates the path loss between BS and a MU by using Maximum a Posteriori (MAP) method. The probability distribution function (PDF) and statistics of the transmission power of the MU is then derived. According to the sequence of received powers from the MU, FU estimates the path loss between itself and the MU by using minimum mean square error (MMSE) method. Simulation results show that the proposed method can efficiently estimate the path loss between any FU and any MU in all kinds of conditions.",
    "MGT": "Path loss estimation is crucial for efficient resource allocation and interference management in femtocell networks. Traditional methods often rely on centralized control or require significant signaling overhead, making them less suitable for the decentralized nature of femtocell deployments. This paper proposes a novel non-cooperative method for path loss estimation in femtocell networks, leveraging received signal strength (RSS) measurements and a game-theoretic framework. Each femtocell autonomously estimates its path loss exponent and shadowing standard deviation by iteratively minimizing a cost function based on the difference between observed RSS values and predicted signal strength. The cost function incorporates a regularization term to prevent overfitting and ensure convergence to a stable solution.\n\nThe proposed method does not require any explicit cooperation or information exchange between femtocells, minimizing signaling overhead and preserving privacy. We model the path loss estimation process as a non-cooperative game, where each femtocell acts as a rational player aiming to minimize its individual cost. We demonstrate that the game possesses a Nash equilibrium, guaranteeing the existence of a stable operating point. Simulation results in a realistic femtocell deployment scenario demonstrate that the proposed method achieves accurate path loss estimation with minimal overhead and computational complexity. Furthermore, the method exhibits robustness to variations in femtocell density and user mobility. The performance of the proposed method is compared to existing techniques, highlighting its advantages in terms of accuracy, scalability, and implementation complexity.\n"
  }
]